[
  {
    "objectID": "people.html",
    "href": "people.html",
    "title": "People-related information",
    "section": "",
    "text": "project funding opportunities\n\n\n\nFunding\n\n\n\nOverview of programs to fund your projects\n\n\n\nCY, MG\n\n\nJul 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvice on preparing and giving scientific talks\n\n\nPractical recommendations for preparing and delivering sicentific talks\n\n\n\nNZ\n\n\nMay 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParticipant compensation\n\n\n\nCompensation\n\n\n\nHow to provide your participants with money or course credit for experiment participation\n\n\n\nNZ\n\n\nSep 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThings to do if you are leaving the lab\n\n\n\nLab\n\n\n\nHow to provide your participants with money or course credit for experiment participation\n\n\n\nNZ\n\n\nSep 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScientific writing in practice\n\n\nPractical recommendations for preparing scientific manuscripts, thesis papers, reports, etc.\n\n\n\nNZ\n\n\nSep 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoffee time\n\n\n\nCoffee\n\n\nLab\n\n\n\nCoffee information\n\n\n\nNZ\n\n\nAug 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nStipends and prizes for master students\n\n\nThis is a list of resources with stipends and prizes you can apply for\n\n\n\nNZ\n\n\nAug 26, 2024\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top",
    "crumbs": [
      "Home",
      "People"
    ]
  },
  {
    "objectID": "people/talks/talks.html",
    "href": "people/talks/talks.html",
    "title": "Advice on preparing and giving scientific talks",
    "section": "",
    "text": "Similar to a manuscript, the talk should have the following parts:\n\nIntroduction\n\nIdeally, the introduction should demonstrate the importance of the topic in a catchy manner. Use a demonstration, an example or anything else that engages/intrigues the audience\n\n\n\n\nMain body\nEnding\n\nIt is good to have a summary slide that reiterates your main points. However, the danger here is that it becomes a slide with lots of borig text. If possible, try to provide a graphical summary instead of text/bullet points.\nScientific talks typically end with an acknowledgement slide, where you thank co-authors and other contributors, as well as the funding sources. Only then you can thank the audience for their attention."
  },
  {
    "objectID": "people/talks/talks.html#structure",
    "href": "people/talks/talks.html#structure",
    "title": "Advice on preparing and giving scientific talks",
    "section": "",
    "text": "Similar to a manuscript, the talk should have the following parts:\n\nIntroduction\n\nIdeally, the introduction should demonstrate the importance of the topic in a catchy manner. Use a demonstration, an example or anything else that engages/intrigues the audience\n\n\n\n\nMain body\nEnding\n\nIt is good to have a summary slide that reiterates your main points. However, the danger here is that it becomes a slide with lots of borig text. If possible, try to provide a graphical summary instead of text/bullet points.\nScientific talks typically end with an acknowledgement slide, where you thank co-authors and other contributors, as well as the funding sources. Only then you can thank the audience for their attention."
  },
  {
    "objectID": "people/talks/talks.html#slides",
    "href": "people/talks/talks.html#slides",
    "title": "Advice on preparing and giving scientific talks",
    "section": "Slides",
    "text": "Slides\nSlides should help you demonstrate the graphical content that is not easy to convey with words: visual illusions, brain activation, graphs, etc. Slides are not meant for you to remind yourself what you wanted to say, and should contain minimum (ideally zero) text.\nSlide titles should be informative. There should be no slide that says e.g. “Introduction”, “Results” or “Conclusion”.\nSlide content should be simple (one, maximum two graphs or images per slide)\n\nCitations\nIt is good scientific practice to indicate sources for each image or figure you use. Scientific articles are cited at the bottom of each slide where you mention them. There should be no reference list at the end."
  },
  {
    "objectID": "people/talks/talks.html#talk-text",
    "href": "people/talks/talks.html#talk-text",
    "title": "Advice on preparing and giving scientific talks",
    "section": "Talk text",
    "text": "Talk text\nYou should deliver you talk in a simple spoken language. Avoid bulky sentences and complicated grammatical structures. Avoid using fancy scientific style. Imagine you are explaining the content to your friend or, even better, to your grandmother.\nPrepare the text you are planning to say for each slide as slide notes. Make sure there are no grammatical mistakes."
  },
  {
    "objectID": "people/talks/talks.html#practice",
    "href": "people/talks/talks.html#practice",
    "title": "Advice on preparing and giving scientific talks",
    "section": "Practice",
    "text": "Practice\nPractice your talk many times, memorizing your text by heart. This is important, because oftentimes it will not be possible to deliver your talk in dual-screen mode, where you see the notes on the main monitor. You should be prepared for this.\nWhile practicing, you will notice that you may want to change the wording/adapt the text. Do this and practice again. This is an iterative process.\nAn example of an excellent speaker in neuroscience is Leslie Ungerleider: https://www.youtube.com/watch?v=mzCl7zMOPYo"
  },
  {
    "objectID": "people/leaving/index.html",
    "href": "people/leaving/index.html",
    "title": "Things to do if you are leaving the lab",
    "section": "",
    "text": "Leaving can be sad. In addition, there is a ton of boring admin work to be done.\n\n\n\nMake sure all published papers are entered into the Forschungsportal\nMake sure all revised manuscript versions and published papers are updated on biorXiv, psyarXiv, medarXiv preprint servers\n\n\n\n\n\nReturn your key to Chrissy once you don’t need a desk\nExtend your UGOnline account if needed\n\n\n\n\n\nMake sure that all subject’s private data that is irrelevant for research (e.g. bank account information for reimbursement) is deleted from the server, from local institute’s computers and from your private computer if relevant\nMake sure that all incidental findings have been clarified, and once this is the case, delete subjects’ contact information if you kept it\nAt the top level of your directory on /storage, create a text/markdown file named “projects” containing server paths (/storage/, J:, or external storage in the case of MW) as follows in the example below\nIt would be nice if you could put a contact email for after your uni-graz email address expires (optional, of course) at the top of this file\n\n\nproject nickname\nFinal version (from the MRI/behavioral lab) of the stimulus script\nRaw data (whichever applies)\n\nbehavioral\neyetracking\nphysiology\n(f)MRI\npdf of the MRI protocol\n\nSubject-level analysis scripts (including preprocessing)\nSubject-level results\nGroup analysis scripts\nGroup results\nLatest version of the submitted manuscript with rebuttal, if applicable. For master students: your UGOnline submitted thesis, preferably in word format\nPosters and submitted conference abstracts related to the study\nLocation of the consent forms (either electronic or paper)\n\n\n/storage/natalia/projects.md\nFor neurodesk users, the projects file can be in your home directory:\n/home/jovyan/\n\n\n\nproject nickname: hexcite\nstimulus script: #todo\nRaw data\n\nbahavior: /storage/nv_shared/projects/excitex/matfiles/\nMRI: /storage/nv_shared/projects/excitex/bids/\nPDF of the MRI protocol: /storage/nv_shared/projects/excitex/111_NZ03_excite_autoalign.pdf\n\nFirst level analysis scripts (including preprocessing): /storage/natalia/projects/hexcite/analyze_hexcite_struc_R1.sh, /storage/natalia/projects/hexcite/analyze_hexcite_fmri.sh\nSubject-level results /storage/natalia/projects/hexcite/bids/derivatives/\nGroup analysis scripts /storage/natalia/projects/hexcite/bids/code/analyze_hexcite_final.m\nGroup results: /storage/natalia/projects/hexcite/group_lowres/\nLatest version of the submitted manuscript with rebuttal: /storage/natalia/projects/hexcite/manuscript/hexcite_CerCor_R1\nPosters and submitted conference abstracts related to the study /storage/natalia/projects/hexcite/presentations\nPaper - Corner office, rightmost drawer",
    "crumbs": [
      "Home",
      "People",
      "Things to do if you are leaving the lab"
    ]
  },
  {
    "objectID": "people/leaving/index.html#papers",
    "href": "people/leaving/index.html#papers",
    "title": "Things to do if you are leaving the lab",
    "section": "",
    "text": "Make sure all published papers are entered into the Forschungsportal\nMake sure all revised manuscript versions and published papers are updated on biorXiv, psyarXiv, medarXiv preprint servers",
    "crumbs": [
      "Home",
      "People",
      "Things to do if you are leaving the lab"
    ]
  },
  {
    "objectID": "people/leaving/index.html#admin",
    "href": "people/leaving/index.html#admin",
    "title": "Things to do if you are leaving the lab",
    "section": "",
    "text": "Return your key to Chrissy once you don’t need a desk\nExtend your UGOnline account if needed",
    "crumbs": [
      "Home",
      "People",
      "Things to do if you are leaving the lab"
    ]
  },
  {
    "objectID": "people/leaving/index.html#data",
    "href": "people/leaving/index.html#data",
    "title": "Things to do if you are leaving the lab",
    "section": "",
    "text": "Make sure that all subject’s private data that is irrelevant for research (e.g. bank account information for reimbursement) is deleted from the server, from local institute’s computers and from your private computer if relevant\nMake sure that all incidental findings have been clarified, and once this is the case, delete subjects’ contact information if you kept it\nAt the top level of your directory on /storage, create a text/markdown file named “projects” containing server paths (/storage/, J:, or external storage in the case of MW) as follows in the example below\nIt would be nice if you could put a contact email for after your uni-graz email address expires (optional, of course) at the top of this file\n\n\nproject nickname\nFinal version (from the MRI/behavioral lab) of the stimulus script\nRaw data (whichever applies)\n\nbehavioral\neyetracking\nphysiology\n(f)MRI\npdf of the MRI protocol\n\nSubject-level analysis scripts (including preprocessing)\nSubject-level results\nGroup analysis scripts\nGroup results\nLatest version of the submitted manuscript with rebuttal, if applicable. For master students: your UGOnline submitted thesis, preferably in word format\nPosters and submitted conference abstracts related to the study\nLocation of the consent forms (either electronic or paper)\n\n\n/storage/natalia/projects.md\nFor neurodesk users, the projects file can be in your home directory:\n/home/jovyan/\n\n\n\nproject nickname: hexcite\nstimulus script: #todo\nRaw data\n\nbahavior: /storage/nv_shared/projects/excitex/matfiles/\nMRI: /storage/nv_shared/projects/excitex/bids/\nPDF of the MRI protocol: /storage/nv_shared/projects/excitex/111_NZ03_excite_autoalign.pdf\n\nFirst level analysis scripts (including preprocessing): /storage/natalia/projects/hexcite/analyze_hexcite_struc_R1.sh, /storage/natalia/projects/hexcite/analyze_hexcite_fmri.sh\nSubject-level results /storage/natalia/projects/hexcite/bids/derivatives/\nGroup analysis scripts /storage/natalia/projects/hexcite/bids/code/analyze_hexcite_final.m\nGroup results: /storage/natalia/projects/hexcite/group_lowres/\nLatest version of the submitted manuscript with rebuttal: /storage/natalia/projects/hexcite/manuscript/hexcite_CerCor_R1\nPosters and submitted conference abstracts related to the study /storage/natalia/projects/hexcite/presentations\nPaper - Corner office, rightmost drawer",
    "crumbs": [
      "Home",
      "People",
      "Things to do if you are leaving the lab"
    ]
  },
  {
    "objectID": "people/compensation/compensation.html",
    "href": "people/compensation/compensation.html",
    "title": "Participant compensation",
    "section": "",
    "text": "Participants can always be compensated with course credit (=Versuchsscheine) if they are psychology students. They can choose between course credit or money only if the project is funded through a grant (please discuss with Natalia in due time if this is the case).\n\nCourse credit\n\nDownload the form available on the Insitute’s sharepoint and fill it out with your study’s data (study name, your name, supervisor’s name).\nPrint some forms (number equal to the number of participants you will test)\nPut a round stamp on each form. Round stamps are available at the secretary’s office or in the Testothek (Universitätsplatz 2, top floor)\nOnce a subject finished the experiment, fill out the rest of the data, put a date, sign, and give it to the subject.\n\n\n\n\nReimbursement\nReimbursement is done via the bank transfer by the University to participants’ bank accounts. Therefore, it is useful to warn them that it will take some weeks until they receive the money. To start the reimbursement process, follow these steps.\n\n\n\n\n\n\nDataprotection!\n\n\n\nWhen you reimburse participants with money and get their information (see below), make sure that participants don´t see the information of others!! In excel this can be achieved by hiding the rows. Additionally, you should save the file where you gather the information on a university server/cloud (e.g. your nextcloud)\n\n\n\nDownload the example excel sheet\nFill out one row per participant, entering information into EVERY field that is nonempty in the excel sheet (you can create your own dedicated template):\n\nBetrag: amount in euro\nText (study name): The study name can not exceed a certain number of symbols, so please keep it short (fMRI studies can use their MRI lab abbreviations: pacmond, clav, deka, etc.)\nName CPD: first and last name of the participant\nStrasse CPD: Street address of the participant\nOrt CPD: City where participant lived\nPLZ CPD: postal code of participant’s address\nLand CPD: Country where participant lived\nBankland CPD: country of the participant’s bank (can be inferred from IBAN)\nBIC CPD: Participant’s bank BIC-code. Note: Although it may seem meaningless, you need to get both IBAN and BIC from you subject (even if they try to convince you that BIC is not needed). This is important for the university.\nIBAN CPD: Participant’s IBAN of the account number where the money should be transferred\n\nNote: Although it may seem meaningless, you need to get both IBAN and BIC from you subject (even if they try to convince you that BIC is not needed).\nAttention!!! The bank information should be 100% correct, otherwise there is an administrative disaster. Therefore, triple-check with participants.\n\nInnenauftrag: Ask Natalia for the correct value of the Innenauftrag field, since this varies from study to study.\nLeave the fields “Zahlweg” and “Kostenstelle” empty\n\nOnce you collect a sufficient number of participants, or once every 1-2 weeks, send this sheet to the secretary’s office of the general psychology section (NOT the institute’s secretary!), and they will take care of the rest.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "People",
      "Participant compensation"
    ]
  },
  {
    "objectID": "other_topics.html",
    "href": "other_topics.html",
    "title": "Other Topics",
    "section": "",
    "text": "Contribute to the website\n\n\n\nWebsite\n\n\n\nInformation about how to contribute and edit the website\n\n\n\nNZ, MG\n\n\nApr 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCalculating a priori sample size using G Power\n\n\n\nPower\n\n\nAnalysis\n\n\n\nInformation and descriptions of how to calculate a priori sample size using G Power\n\n\n\nCY\n\n\nMar 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLab Members Only!!!\n\n\nThis site is dedicated only for members of the lab!\n\n\n\nNZ\n\n\nJan 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGamma correction\n\n\nHow to measure monitor gamma and apply it in visual experiments\n\n\n\nNZ\n\n\nAug 26, 2024\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top",
    "crumbs": [
      "Home",
      "Other Topics"
    ]
  },
  {
    "objectID": "OtherTopics/displays/displays.html",
    "href": "OtherTopics/displays/displays.html",
    "title": "Gamma correction",
    "section": "",
    "text": "Typical screens have nonlinear outputs. This means that if your psychtoolbox or python script gives a command to show luminance values in liner steps, e.g. 0, 0.25, 0.5, 0.75, 1.0, the measured monitor light emittance will not be linearly spaced. For details, see e.g. https://www.eizo.com/library/basics/lcd_display_gamma/\nIn visual psychophysics, we want to have a linear mapping between the input and output, and therefore we “linearize” monitors. This page describes how we do it in the lab.",
    "crumbs": [
      "Home",
      "Other Topics",
      "Gamma correction"
    ]
  },
  {
    "objectID": "OtherTopics/displays/displays.html#footnotes",
    "href": "OtherTopics/displays/displays.html#footnotes",
    "title": "Gamma correction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere are also fancy spectrometers ranging from 200 to 20000 Euros in price, which can be connected to the computer via a USB. This allows to store the color spectra automatically. For simple gamma correction a lux meter is enough though.↩︎",
    "crumbs": [
      "Home",
      "Other Topics",
      "Gamma correction"
    ]
  },
  {
    "objectID": "OtherTopics/CalculatingAPrioriPower.html",
    "href": "OtherTopics/CalculatingAPrioriPower.html",
    "title": "Calculating a priori sample size using G Power",
    "section": "",
    "text": "Power analysis is a statistical technique that helps scientists determine the sample size required to detect an effect of a given size with a desired degree of certainty. It helps researchers understand the likelihood that their study will detect a meaningful difference or relationship (if one exists) between groups or variables being studied. By performing a power analysis before conducting an experiment, scientists can ensure that the study is neither too small (which would result in a lack of statistical power to detect an effect, leading to a possible Type II error) nor too large (which could waste resources and potentially expose participants to unnecessary procedures). It’s a crucial step in the research design process that helps in making informed decisions and in enhancing the credibility and replicability of scientific findings.\nYou need to calculate a priori sample size according to the statistical test you want to make an argument. For example, if you are interested in only the results of ANOVA comparisons then, you should make your calculation for your ANOVA test. However, if you desire to analyze further with multiple comparisons then, you should calculate required sample size based on your post-hoc method which is commonly paired t test.",
    "crumbs": [
      "Home",
      "Other Topics",
      "Calculating a priori sample size using G Power"
    ]
  },
  {
    "objectID": "OtherTopics/CalculatingAPrioriPower.html#how-to-calculate-a-priori-sample-size-on-gpower",
    "href": "OtherTopics/CalculatingAPrioriPower.html#how-to-calculate-a-priori-sample-size-on-gpower",
    "title": "Calculating a priori sample size using G Power",
    "section": "2.1 How to calculate a priori sample size on G*Power",
    "text": "2.1 How to calculate a priori sample size on G*Power\nTo calculate an a priori sample size using G*Power, you need to follow several steps. The specific steps might change slightly depending on the statistical test you plan to use, but the overall process is similar:\n\nDownload and Open G*Power\nChoose the Test\nIn the G*Power interface, select the statistical test that you intend to use from the ‘Test family’ (e.g., t tests, F tests, etc.) and the ‘Statistical test’ (e.g., ANOVA, linear multiple regression, etc.) menus according to the hypothesis of your study.\nDetermine the Type of Power Analysis\nSince you want to calculate the sample size, you should choose ‘A priori: Compute required sample size - given α, power, and effect size’.\nInput Parameters\n\nEffect Size: Estimate the effect size for your desired test, which indicates the magnitude of the relationship or difference that you expect to find. You can use previous literature or pilot studies to estimate this.\nα Error Probability (Significance Level): Typically, this is set at 0.05.\nPower (1-β Error Probability): Commonly set at 0.80, indicating an 80% chance of detecting an effect if there really is one.\nAllocation Ratio (for comparative studies): This is the ratio of the number of participants in one group to the number of participants in the other group(s). For equal group sizes, this is set to 1.\n\nCalculate\nOnce all parameters are entered, click on the ‘Calculate’ button. G*Power will display the required sample size for your study based on the input values.\nReview Output\nG*Power will provide the calculated sample size along with other details related to the power analysis. Carefully review this output to determine if it is suitable for the needs of your study.\n\nKeep in mind that determining these parameters, especially the effect size, can sometimes be the most challenging part of a power analysis. Consult with a statistician, your team members or relevant literature if you are unsure about these inputs. Finally, remember that power analysis is an important part of designing your study, but it should be combined with practical considerations like resource availability, ethical constraints, and methodological limitations.",
    "crumbs": [
      "Home",
      "Other Topics",
      "Calculating a priori sample size using G Power"
    ]
  },
  {
    "objectID": "OtherTopics/CalculatingAPrioriPower.html#repeated-measures-anova-on-gpower",
    "href": "OtherTopics/CalculatingAPrioriPower.html#repeated-measures-anova-on-gpower",
    "title": "Calculating a priori sample size using G Power",
    "section": "2.2 Repeated measures ANOVA on G*Power",
    "text": "2.2 Repeated measures ANOVA on G*Power\nAs an example, let’s say we have 3 different stimulus types. In each session, we will present one type of stimuli at 4 different contrast levels. The task will be the same throughout the experiment. Our aim is to determine the effect of contrast.\nIn such a case, we need using a repeated measures ANOVA (or 2-way mixed ANOVA or split-plot ANOVA). Calculating an a priori sample size for a mixed-design ANOVA in GPower can be a bit tricky, as G*Power does not directly provide a pre-built module for mixed ANOVAs with both within- (repeated measures) and between-subjects factors. However, you can still approximate the necessary calculations by considering the most powerful test within your design, which is typically the repeated measures part if the effect sizes are similar. Here is a general guide on how to do this:\n\nOpen G*Power\nSelect the Statistical Test\n\nChoose ‘F tests’ as the test family.\nChoose ‘ANOVA: Repeated measures, within factors’.\n(You can also choose ‘ANOVA: Repeated measures, between factors’ depending on which effect you expect to be larger or more important to your hypothesis testing. However, a conservative approach is often to consider the within-effects when uncertain.)\n\nChoose the Type of Power Analysis\n\nSelect ‘A priori: Compute required sample size - given α, power, and effect size’.\n\nInput the Parameters\n\nEffect Size: We need to determine the effect size with direct method to count the effect of repeated measure. To do that, click on ‘Determine =)&gt;’. On the pop-up window, select ‘Direct’. Enter eta squared value. Click ‘Calculate and transfer to main window’.\nEta squared (\\(\\eta^{2}\\)) is the effect size that indicates the total variance in testing explained by the within-subject variable (in our case, different contrast levels). Approximate partial eta squared conventions are small = 0.02, medium = 0.06, large = 0.14.\nα Error Probability: The significance level is typically set at 0.05.\nPower: Set this commonly to 0.80, indicating an 80% probability of correctly rejecting the null hypothesis.\nNumber of Groups: For the between-subjects factor, input the number of levels or conditions. If you compare, for example, healthy vs patient group, the number of groups is 2. For within-subject designs, the number of groups should be 1.\nNumber of Measurements: For the within-subjects factor, input the number of levels or conditions. If you have more than one independent variable, you can compute the number of measurements by multiplying the number of conditions for each factor. For example, if you have three stimulus types (k = 3) each of that is presented at four contrast levels (m = 4), you have 12 measurements (k x m = 3 x 4).\nCorrelation among Repeated Measures: This is determined by the expected correlation between the measures at different levels of contrast. If you don’t have an estimate, using a default value like 0.5 can be a starting point.\nNonsphericity Correction ε: If you have an estimate for this, input it here; otherwise, you can keep it at 1 or use the default provided by G*Power.\n\nCalculate\nReview Results\n\n\n\n\nPlease note that this approach is an approximation, as it does not fully represent the complexity of a mixed-design ANOVA. It essentially treats the between-subjects factor as another within-subjects level, which may not be totally accurate but can give you a ballpark figure for your sample size. If possible, consult with a statistician to ensure that the calculation is correctly tailored to your study’s specific needs and to take into account other complexities that G*Power may not directly address for mixed designs.\nSee another example on YouTube",
    "crumbs": [
      "Home",
      "Other Topics",
      "Calculating a priori sample size using G Power"
    ]
  },
  {
    "objectID": "OtherTopics/CalculatingAPrioriPower.html#pwr",
    "href": "OtherTopics/CalculatingAPrioriPower.html#pwr",
    "title": "Calculating a priori sample size using G Power",
    "section": "3.1 pwr",
    "text": "3.1 pwr\nThis is the most common and basic package for power analysis. However, it is not suitable for complex statistical tests such as repeated measures ANOVA.\n\npwr:::pwr.t.test(n = NULL,  # NULL to calculate a priori sample size\n  d = 0.4,  # Cohen's d, effect size\n  sig.level = 0.05,  # alpha value\n  power = 0.80,  # desired power level\n  type = \"paired\",  # type of t test\n  alternative = \"two.sided\"  # alternative hypothesis\n)\n\n\n     Paired t test power calculation \n\n              n = 51.00945\n              d = 0.4\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*",
    "crumbs": [
      "Home",
      "Other Topics",
      "Calculating a priori sample size using G Power"
    ]
  },
  {
    "objectID": "OtherTopics/CalculatingAPrioriPower.html#webpower",
    "href": "OtherTopics/CalculatingAPrioriPower.html#webpower",
    "title": "Calculating a priori sample size using G Power",
    "section": "3.2 WebPower",
    "text": "3.2 WebPower\nThis is a strong tool for different types of ANOVA.\n\nWebPower::wp.rmanova(\n  n = NULL,  # Compute required sample size\n  ng = 1,   # Only one group if it's a within-subjects design\n  nm = 3*3,  # Number of measurements (n*m for n x m design)\n  f = 0.4,  # Effect size\n  nscor = 1,  # Nonsphericity correction (assumed sphericity for now)\n  alpha = 0.05,  # significance level, aka alpha value\n  power = 0.80,  # desired power \n  type = 1 # 0: between-effect; 1: within-effect; 2: interaction effect\n)\n\nRepeated-measures ANOVA analysis\n\n           n   f ng nm nscor alpha power\n    94.86041 0.4  1  9     1  0.05   0.8\n\nNOTE: Power analysis for within-effect test\nURL: http://psychstat.org/rmanova",
    "crumbs": [
      "Home",
      "Other Topics",
      "Calculating a priori sample size using G Power"
    ]
  },
  {
    "objectID": "OtherTopics/CalculatingAPrioriPower.html#superpower",
    "href": "OtherTopics/CalculatingAPrioriPower.html#superpower",
    "title": "Calculating a priori sample size using G Power",
    "section": "3.3 Superpower",
<<<<<<< HEAD
    "text": "3.3 Superpower\nAs the name suggests, it is super for power analysis :) However, you need to simulate your data to calculate required sample size. It might be useful to calculate actual power level. However, it is hard to create simulations in general for our studies in Visual Neuroscience Lab.\n\n# Experimental design\ndesign_result &lt;- Superpower::ANOVA_design(\n  design = \"3w*3w\",  # 3x2 within-subjects \n  n = 30,  # as a start point to find the required sample size\n  mu = c(500, 520, 540, 490, 510, 530, 480, 500, 520),  # average values for dependent variable\n  sd = 50,  # estimated standard deviation\n  r = 0.5,  # correlation between repeated measures\n  labelnames = c(\"Stimulus\", \"A\", \"B\", \"C\", \"Condition\", \"1\", \"2\", \"3\")\n)\n\n\n\n\n\n\n\n# simulation for power analysis\nSuperpower::ANOVA_power(design_result, nsims = 1000)\n\nPower and Effect sizes for ANOVA tests\n                         power effect_size\nanova_Stimulus            92.5     0.22331\nanova_Condition          100.0     0.49542\nanova_Stimulus:Condition   5.6     0.03374\n\nPower and Effect sizes for pairwise comparisons (t-tests)\n                                                power effect_size\np_Stimulus_A_Condition_1_Stimulus_A_Condition_2  55.3     0.41331\np_Stimulus_A_Condition_1_Stimulus_A_Condition_3  98.8     0.81461\np_Stimulus_A_Condition_1_Stimulus_B_Condition_1  18.2    -0.20341\np_Stimulus_A_Condition_1_Stimulus_B_Condition_2  18.4     0.19723\np_Stimulus_A_Condition_1_Stimulus_B_Condition_3  89.7     0.60782\np_Stimulus_A_Condition_1_Stimulus_C_Condition_1  56.3    -0.41280\np_Stimulus_A_Condition_1_Stimulus_C_Condition_2   3.8    -0.01476\np_Stimulus_A_Condition_1_Stimulus_C_Condition_3  53.3     0.40025\np_Stimulus_A_Condition_2_Stimulus_A_Condition_3  55.7     0.40034\np_Stimulus_A_Condition_2_Stimulus_B_Condition_1  89.1    -0.62076\np_Stimulus_A_Condition_2_Stimulus_B_Condition_2  22.1    -0.21623\np_Stimulus_A_Condition_2_Stimulus_B_Condition_3  17.0     0.19548\np_Stimulus_A_Condition_2_Stimulus_C_Condition_1  98.9    -0.82790\np_Stimulus_A_Condition_2_Stimulus_C_Condition_2  59.0    -0.42917\np_Stimulus_A_Condition_2_Stimulus_C_Condition_3   4.5    -0.01177\np_Stimulus_A_Condition_3_Stimulus_B_Condition_1  99.9    -1.02037\np_Stimulus_A_Condition_3_Stimulus_B_Condition_2  87.4    -0.61804\np_Stimulus_A_Condition_3_Stimulus_B_Condition_3  19.2    -0.20640\np_Stimulus_A_Condition_3_Stimulus_C_Condition_1 100.0    -1.22792\np_Stimulus_A_Condition_3_Stimulus_C_Condition_2  98.5    -0.83239\np_Stimulus_A_Condition_3_Stimulus_C_Condition_3  57.1    -0.41424\np_Stimulus_B_Condition_1_Stimulus_B_Condition_2  53.4     0.40224\np_Stimulus_B_Condition_1_Stimulus_B_Condition_3  98.8     0.81094\np_Stimulus_B_Condition_1_Stimulus_C_Condition_1  19.4    -0.20903\np_Stimulus_B_Condition_1_Stimulus_C_Condition_2  17.2     0.19042\np_Stimulus_B_Condition_1_Stimulus_C_Condition_3  88.2     0.60435\np_Stimulus_B_Condition_2_Stimulus_B_Condition_3  54.1     0.40909\np_Stimulus_B_Condition_2_Stimulus_C_Condition_1  88.0    -0.61233\np_Stimulus_B_Condition_2_Stimulus_C_Condition_2  19.8    -0.21169\np_Stimulus_B_Condition_2_Stimulus_C_Condition_3  18.6     0.20341\np_Stimulus_B_Condition_3_Stimulus_C_Condition_1  99.9    -1.02117\np_Stimulus_B_Condition_3_Stimulus_C_Condition_2  89.6    -0.62154\np_Stimulus_B_Condition_3_Stimulus_C_Condition_3  17.4    -0.20711\np_Stimulus_C_Condition_1_Stimulus_C_Condition_2  54.8     0.39687\np_Stimulus_C_Condition_1_Stimulus_C_Condition_3  98.4     0.81640\np_Stimulus_C_Condition_2_Stimulus_C_Condition_3  56.5     0.41409\n\n\nWithin-Subject Factors Included: Check MANOVA Results\n\n\nPower and Effect sizes for ANOVA tests\n                         power effect_size\nanova_Stimulus            92.5     0.22331\nanova_Condition          100.0     0.49542\nanova_Stimulus:Condition   5.6     0.03374\n\nPower and Effect sizes for pairwise comparisons (t-tests)\n                                                power effect_size\np_Stimulus_A_Condition_1_Stimulus_A_Condition_2  55.3     0.41331\np_Stimulus_A_Condition_1_Stimulus_A_Condition_3  98.8     0.81461\np_Stimulus_A_Condition_1_Stimulus_B_Condition_1  18.2    -0.20341\np_Stimulus_A_Condition_1_Stimulus_B_Condition_2  18.4     0.19723\np_Stimulus_A_Condition_1_Stimulus_B_Condition_3  89.7     0.60782\np_Stimulus_A_Condition_1_Stimulus_C_Condition_1  56.3    -0.41280\np_Stimulus_A_Condition_1_Stimulus_C_Condition_2   3.8    -0.01476\np_Stimulus_A_Condition_1_Stimulus_C_Condition_3  53.3     0.40025\np_Stimulus_A_Condition_2_Stimulus_A_Condition_3  55.7     0.40034\np_Stimulus_A_Condition_2_Stimulus_B_Condition_1  89.1    -0.62076\np_Stimulus_A_Condition_2_Stimulus_B_Condition_2  22.1    -0.21623\np_Stimulus_A_Condition_2_Stimulus_B_Condition_3  17.0     0.19548\np_Stimulus_A_Condition_2_Stimulus_C_Condition_1  98.9    -0.82790\np_Stimulus_A_Condition_2_Stimulus_C_Condition_2  59.0    -0.42917\np_Stimulus_A_Condition_2_Stimulus_C_Condition_3   4.5    -0.01177\np_Stimulus_A_Condition_3_Stimulus_B_Condition_1  99.9    -1.02037\np_Stimulus_A_Condition_3_Stimulus_B_Condition_2  87.4    -0.61804\np_Stimulus_A_Condition_3_Stimulus_B_Condition_3  19.2    -0.20640\np_Stimulus_A_Condition_3_Stimulus_C_Condition_1 100.0    -1.22792\np_Stimulus_A_Condition_3_Stimulus_C_Condition_2  98.5    -0.83239\np_Stimulus_A_Condition_3_Stimulus_C_Condition_3  57.1    -0.41424\np_Stimulus_B_Condition_1_Stimulus_B_Condition_2  53.4     0.40224\np_Stimulus_B_Condition_1_Stimulus_B_Condition_3  98.8     0.81094\np_Stimulus_B_Condition_1_Stimulus_C_Condition_1  19.4    -0.20903\np_Stimulus_B_Condition_1_Stimulus_C_Condition_2  17.2     0.19042\np_Stimulus_B_Condition_1_Stimulus_C_Condition_3  88.2     0.60435\np_Stimulus_B_Condition_2_Stimulus_B_Condition_3  54.1     0.40909\np_Stimulus_B_Condition_2_Stimulus_C_Condition_1  88.0    -0.61233\np_Stimulus_B_Condition_2_Stimulus_C_Condition_2  19.8    -0.21169\np_Stimulus_B_Condition_2_Stimulus_C_Condition_3  18.6     0.20341\np_Stimulus_B_Condition_3_Stimulus_C_Condition_1  99.9    -1.02117\np_Stimulus_B_Condition_3_Stimulus_C_Condition_2  89.6    -0.62154\np_Stimulus_B_Condition_3_Stimulus_C_Condition_3  17.4    -0.20711\np_Stimulus_C_Condition_1_Stimulus_C_Condition_2  54.8     0.39687\np_Stimulus_C_Condition_1_Stimulus_C_Condition_3  98.4     0.81640\np_Stimulus_C_Condition_2_Stimulus_C_Condition_3  56.5     0.41409",
=======
    "text": "3.3 Superpower\nAs the name suggests, it is super for power analysis :) However, you need to simulate your data to calculate required sample size. It might be useful to calculate actual power level. However, it is hard to create simulations in general for our studies in Visual Neuroscience Lab.\n\n# Experimental design\ndesign_result &lt;- Superpower::ANOVA_design(\n  design = \"3w*3w\",  # 3x2 within-subjects \n  n = 30,  # as a start point to find the required sample size\n  mu = c(500, 520, 540, 490, 510, 530, 480, 500, 520),  # average values for dependent variable\n  sd = 50,  # estimated standard deviation\n  r = 0.5,  # correlation between repeated measures\n  labelnames = c(\"Stimulus\", \"A\", \"B\", \"C\", \"Condition\", \"1\", \"2\", \"3\")\n)\n\n\n\n\n\n\n\n# simulation for power analysis\nSuperpower::ANOVA_power(design_result, nsims = 1000)\n\nPower and Effect sizes for ANOVA tests\n                         power effect_size\nanova_Stimulus            93.2     0.22125\nanova_Condition          100.0     0.50644\nanova_Stimulus:Condition   4.3     0.03182\n\nPower and Effect sizes for pairwise comparisons (t-tests)\n                                                power effect_size\np_Stimulus_A_Condition_1_Stimulus_A_Condition_2  57.5    0.417292\np_Stimulus_A_Condition_1_Stimulus_A_Condition_3  98.6    0.824605\np_Stimulus_A_Condition_1_Stimulus_B_Condition_1  19.7   -0.213573\np_Stimulus_A_Condition_1_Stimulus_B_Condition_2  17.2    0.200082\np_Stimulus_A_Condition_1_Stimulus_B_Condition_3  89.2    0.621440\np_Stimulus_A_Condition_1_Stimulus_C_Condition_1  57.9   -0.420581\np_Stimulus_A_Condition_1_Stimulus_C_Condition_2   4.4    0.000699\np_Stimulus_A_Condition_1_Stimulus_C_Condition_3  54.1    0.402694\np_Stimulus_A_Condition_2_Stimulus_A_Condition_3  57.2    0.409822\np_Stimulus_A_Condition_2_Stimulus_B_Condition_1  90.3   -0.630003\np_Stimulus_A_Condition_2_Stimulus_B_Condition_2  19.6   -0.213057\np_Stimulus_A_Condition_2_Stimulus_B_Condition_3  18.6    0.204021\np_Stimulus_A_Condition_2_Stimulus_C_Condition_1  99.3   -0.834164\np_Stimulus_A_Condition_2_Stimulus_C_Condition_2  56.2   -0.411286\np_Stimulus_A_Condition_2_Stimulus_C_Condition_3   5.1   -0.011273\np_Stimulus_A_Condition_3_Stimulus_B_Condition_1 100.0   -1.035730\np_Stimulus_A_Condition_3_Stimulus_B_Condition_2  89.7   -0.623096\np_Stimulus_A_Condition_3_Stimulus_B_Condition_3  19.8   -0.205734\np_Stimulus_A_Condition_3_Stimulus_C_Condition_1 100.0   -1.241737\np_Stimulus_A_Condition_3_Stimulus_C_Condition_2  99.0   -0.815930\np_Stimulus_A_Condition_3_Stimulus_C_Condition_3  56.5   -0.412845\np_Stimulus_B_Condition_1_Stimulus_B_Condition_2  58.3    0.414046\np_Stimulus_B_Condition_1_Stimulus_B_Condition_3  99.1    0.827329\np_Stimulus_B_Condition_1_Stimulus_C_Condition_1  17.2   -0.205089\np_Stimulus_B_Condition_1_Stimulus_C_Condition_2  20.2    0.212547\np_Stimulus_B_Condition_1_Stimulus_C_Condition_3  88.3    0.616291\np_Stimulus_B_Condition_2_Stimulus_B_Condition_3  59.6    0.412726\np_Stimulus_B_Condition_2_Stimulus_C_Condition_1  90.4   -0.617694\np_Stimulus_B_Condition_2_Stimulus_C_Condition_2  16.3   -0.198320\np_Stimulus_B_Condition_2_Stimulus_C_Condition_3  17.5    0.203218\np_Stimulus_B_Condition_3_Stimulus_C_Condition_1  99.9   -1.028665\np_Stimulus_B_Condition_3_Stimulus_C_Condition_2  89.9   -0.613790\np_Stimulus_B_Condition_3_Stimulus_C_Condition_3  16.2   -0.210917\np_Stimulus_C_Condition_1_Stimulus_C_Condition_2  58.3    0.417612\np_Stimulus_C_Condition_1_Stimulus_C_Condition_3  99.1    0.819129\np_Stimulus_C_Condition_2_Stimulus_C_Condition_3  53.9    0.399724\n\n\nWithin-Subject Factors Included: Check MANOVA Results\n\n\nPower and Effect sizes for ANOVA tests\n                         power effect_size\nanova_Stimulus            93.2     0.22125\nanova_Condition          100.0     0.50644\nanova_Stimulus:Condition   4.3     0.03182\n\nPower and Effect sizes for pairwise comparisons (t-tests)\n                                                power effect_size\np_Stimulus_A_Condition_1_Stimulus_A_Condition_2  57.5    0.417292\np_Stimulus_A_Condition_1_Stimulus_A_Condition_3  98.6    0.824605\np_Stimulus_A_Condition_1_Stimulus_B_Condition_1  19.7   -0.213573\np_Stimulus_A_Condition_1_Stimulus_B_Condition_2  17.2    0.200082\np_Stimulus_A_Condition_1_Stimulus_B_Condition_3  89.2    0.621440\np_Stimulus_A_Condition_1_Stimulus_C_Condition_1  57.9   -0.420581\np_Stimulus_A_Condition_1_Stimulus_C_Condition_2   4.4    0.000699\np_Stimulus_A_Condition_1_Stimulus_C_Condition_3  54.1    0.402694\np_Stimulus_A_Condition_2_Stimulus_A_Condition_3  57.2    0.409822\np_Stimulus_A_Condition_2_Stimulus_B_Condition_1  90.3   -0.630003\np_Stimulus_A_Condition_2_Stimulus_B_Condition_2  19.6   -0.213057\np_Stimulus_A_Condition_2_Stimulus_B_Condition_3  18.6    0.204021\np_Stimulus_A_Condition_2_Stimulus_C_Condition_1  99.3   -0.834164\np_Stimulus_A_Condition_2_Stimulus_C_Condition_2  56.2   -0.411286\np_Stimulus_A_Condition_2_Stimulus_C_Condition_3   5.1   -0.011273\np_Stimulus_A_Condition_3_Stimulus_B_Condition_1 100.0   -1.035730\np_Stimulus_A_Condition_3_Stimulus_B_Condition_2  89.7   -0.623096\np_Stimulus_A_Condition_3_Stimulus_B_Condition_3  19.8   -0.205734\np_Stimulus_A_Condition_3_Stimulus_C_Condition_1 100.0   -1.241737\np_Stimulus_A_Condition_3_Stimulus_C_Condition_2  99.0   -0.815930\np_Stimulus_A_Condition_3_Stimulus_C_Condition_3  56.5   -0.412845\np_Stimulus_B_Condition_1_Stimulus_B_Condition_2  58.3    0.414046\np_Stimulus_B_Condition_1_Stimulus_B_Condition_3  99.1    0.827329\np_Stimulus_B_Condition_1_Stimulus_C_Condition_1  17.2   -0.205089\np_Stimulus_B_Condition_1_Stimulus_C_Condition_2  20.2    0.212547\np_Stimulus_B_Condition_1_Stimulus_C_Condition_3  88.3    0.616291\np_Stimulus_B_Condition_2_Stimulus_B_Condition_3  59.6    0.412726\np_Stimulus_B_Condition_2_Stimulus_C_Condition_1  90.4   -0.617694\np_Stimulus_B_Condition_2_Stimulus_C_Condition_2  16.3   -0.198320\np_Stimulus_B_Condition_2_Stimulus_C_Condition_3  17.5    0.203218\np_Stimulus_B_Condition_3_Stimulus_C_Condition_1  99.9   -1.028665\np_Stimulus_B_Condition_3_Stimulus_C_Condition_2  89.9   -0.613790\np_Stimulus_B_Condition_3_Stimulus_C_Condition_3  16.2   -0.210917\np_Stimulus_C_Condition_1_Stimulus_C_Condition_2  58.3    0.417612\np_Stimulus_C_Condition_1_Stimulus_C_Condition_3  99.1    0.819129\np_Stimulus_C_Condition_2_Stimulus_C_Condition_3  53.9    0.399724",
>>>>>>> 4f1eb82d34caf3aafd572958364906347c3d1654
    "crumbs": [
      "Home",
      "Other Topics",
      "Calculating a priori sample size using G Power"
    ]
  },
  {
    "objectID": "neurodesk/neurodesktop.html",
    "href": "neurodesk/neurodesktop.html",
    "title": "Neurodesktop access",
    "section": "",
    "text": "Neurodesktop is a Linux environment specifically developed for open and reproducible neuroimaging. For details, see https://www.neurodesk.org\nThis page provides information and instructions on how to use neurodesktop on our lab’s server.\n\n\n\n\nOffice computers and VPN with an employee account\n\n\n\n\nOffice computers\nStudent PC room\n\n\n\n\n\nEach neurodesktop user is limited to 256GB RAM out of 504GB and to 20 out of 32 CPU cores.",
    "crumbs": [
      "Home",
      "Neurodesktop"
    ]
  },
  {
    "objectID": "neurodesk/neurodesktop.html#access",
    "href": "neurodesk/neurodesktop.html#access",
    "title": "Neurodesktop access",
    "section": "",
    "text": "Office computers and VPN with an employee account\n\n\n\n\nOffice computers\nStudent PC room",
    "crumbs": [
      "Home",
      "Neurodesktop"
    ]
  },
  {
    "objectID": "neurodesk/neurodesktop.html#resources",
    "href": "neurodesk/neurodesktop.html#resources",
    "title": "Neurodesktop access",
    "section": "",
    "text": "Each neurodesktop user is limited to 256GB RAM out of 504GB and to 20 out of 32 CPU cores.",
    "crumbs": [
      "Home",
      "Neurodesktop"
    ]
  },
  {
    "objectID": "neurodesk/neurodesktop.html#first-usage",
    "href": "neurodesk/neurodesktop.html#first-usage",
    "title": "Neurodesktop access",
    "section": "First usage",
    "text": "First usage\nThe first time you start the neurodesk or a specific neurodesktop application, it may take a while until it opens. Be patient! This is because the first time everything gets downloaded in the background. It will be much faster when you perform the same operations repeatedly (until the next server reboot).",
    "crumbs": [
      "Home",
      "Neurodesktop"
    ]
  },
  {
    "objectID": "neurodesk/neurodesktop.html#storing-data",
    "href": "neurodesk/neurodesktop.html#storing-data",
    "title": "Neurodesktop access",
    "section": "Storing data",
    "text": "Storing data\nIf working via neurodesk, you SHOULD store data in your home directory. This data is kept after server restart and is backed up.",
    "crumbs": [
      "Home",
      "Neurodesktop"
    ]
  },
  {
    "objectID": "neurodesk/neurodesktop.html#sharing-data",
    "href": "neurodesk/neurodesktop.html#sharing-data",
    "title": "Neurodesktop access",
    "section": "Sharing data",
    "text": "Sharing data\nAll neurodesk users have read and write permissions to /shared neurodesktop folder, which should be used to store things used by everyone (like atlases, software, shared data)",
    "crumbs": [
      "Home",
      "Neurodesktop"
    ]
  },
  {
    "objectID": "neurodesk/neurodesktop.html#data-transfer",
    "href": "neurodesk/neurodesktop.html#data-transfer",
    "title": "Neurodesktop access",
    "section": "Data transfer",
    "text": "Data transfer\nThere are currently two alternatives\n\nYou can transfer the data via nextcloud client, which is included in neurodsktop\n\n“file upload” button at the top of the jupyter lab menu (up-pointing arrow) or download by right-clicking on the relevant files. This option does not currently support multiple files, so if you want to transfer many files at once this way, it is advisable to zip them.",
    "crumbs": [
      "Home",
      "Neurodesktop"
    ]
  },
  {
    "objectID": "neurodesk/neurodesktop.html#using-freesurfer",
    "href": "neurodesk/neurodesktop.html#using-freesurfer",
    "title": "Neurodesktop access",
    "section": "Using FreeSurfer",
    "text": "Using FreeSurfer\nTo force freesurfer/freeview to see the /storage folder, source freesurfer from the terminal as follows\nml freesurfer/7.4.1\ncd /storage\nfreeview\nTo set up custom subject directory in a container, use the following (instead of export SUBJECTS_DIR=)\nexport SINGULARITYENV_SUBJECTS_DIR=[yourpath] export APPTAINERENV_SUBJECTS_DIR=[yourpath]",
    "crumbs": [
      "Home",
      "Neurodesktop"
    ]
  },
  {
    "objectID": "neurodesk/neurodesktop.html#using-fitlins",
    "href": "neurodesk/neurodesktop.html#using-fitlins",
    "title": "Neurodesktop access",
    "section": "Using fitlins",
    "text": "Using fitlins\nUsing FitLins (not a part of neurodesktop basic set of tools) with Singularity:\n\nsingularity run /shared/poldracklab_fitlins_latest-2022-10-17-32670dc66bf7.simg [your fitlins commands]\nPlease note that we are abandoning fitlins usage and are switching to nilearn for GLM analysis",
    "crumbs": [
      "Home",
      "Neurodesktop"
    ]
  },
  {
    "objectID": "neurodesk/neurodesktop.html#resource-esp.-ram-usage",
    "href": "neurodesk/neurodesktop.html#resource-esp.-ram-usage",
    "title": "Neurodesktop access",
    "section": "Resource (esp. RAM) usage",
    "text": "Resource (esp. RAM) usage\nPlease keep an eye on your resource usage, especially RAM. If too much RAM is used up by users, the server will be slow and eventually unusable.\nThe overall resource usage on the server (all users) is displayed in the bottom left of the neurodesktop interface as blue and red bars:\n\n\n\nExample of about 50% RAM usage\n\n\nIf you want to check how much resources is used by you, open the terminal and type “htop” (you can use F6 to sort processes by “percentage memory”):\n\n\n\nExample of “normal” memory usage for running fMRIprep.",
    "crumbs": [
      "Home",
      "Neurodesktop"
    ]
  },
  {
    "objectID": "neurodesk/neurodesktop.html#using-notebooks-and-scripts-in-jupyterlab",
    "href": "neurodesk/neurodesktop.html#using-notebooks-and-scripts-in-jupyterlab",
    "title": "Neurodesktop access",
    "section": "Using notebooks and scripts in JupyterLab",
    "text": "Using notebooks and scripts in JupyterLab\nWhen you want to use python scrips or notebooks, you´ll probably set them up in JupyterLab. There are a few “nice-to-have” (e.g. changing the Theme at the section Settings at the menu bar) but also potentially crucial settings that are worth taking a look at:\n\nSettings &gt; Settings Editor\n\nat the sidebar, go to Notebook. Here you have the option to individualize different aspects about your notebooks. Two potentially important/helpful adaptations:\n\nEnable “Auto Closing Brackets” (should be right at the top). When handling multiple brackets, this might be helpful to keep track of where you are.\nIf you scroll down a bit, there should be “Shut down kernel”. Even though unused kernels are shut down after 24h automatically, it might be a good practice to enable this option. This results in an automatic kernel-shutdown whenever you close a notebook. This helps to keep the RAM clean by avoiding active kernels that are not used.\n\nat the sidebar, go to Code Completion. Depending what you are used to when coding, you can activate the option Enable autocompletion here\n\nThe ⏩ sign: \\(\\to\\) by clicking this button at the top of notebooks, it will restart the kernel and run the whole notebook sequentially (so all cells after each other)\nThe ⏵ sign: run the current cell (Alternative: Strg + Enter)\nThe ⏹ sign: interrups all current processes of the kernel\nThe ⟳ sign: restarts the kernel (all variales are lost, everything is cleared from RAM)\nEnsuring limited strain on RAM for notebooks with high RAM demands: Sometimes you need to run notebooks that take some time and that require much RAM (e.g. analysis/processing with surface images of multiple subjects). First, you should figure out how many subjects are even possible at once (due to limited RAM). If you run a notebook that takes so much time that you potentially can´t look at the data immediately (as it may take an hour an you are not in the office then), it can be useful to add cell at the very end of your notebook with os._exit(0) (if you dodn´t use os in your notebook, you have to include import os before that line as well). Including this line and with using ⏩, your notebook runs through completely and automatically aborts the kernel afterwards. Thus, you free up the required RAM immediately after your notebook finished (obviously, you must make sure that the data you need is saved in the process)",
    "crumbs": [
      "Home",
      "Neurodesktop"
    ]
  },
  {
    "objectID": "neurodesk/neurodesktop.html#folder-mappings",
    "href": "neurodesk/neurodesktop.html#folder-mappings",
    "title": "Neurodesktop access",
    "section": "Folder mappings:",
    "text": "Folder mappings:\n(left is server right is neurodesk)\n/storage -&gt; /storage\n/storage/neurodesk/shared -&gt; /shared\n/storage/neurodesk/users/{username} -&gt; /home/jovyan\nOnly these three folders are persistent.",
    "crumbs": [
      "Home",
      "Neurodesktop"
    ]
  },
  {
    "objectID": "neurodesk/neurodesktop.html#for-interfacing-with-the-folders-on-storage",
    "href": "neurodesk/neurodesktop.html#for-interfacing-with-the-folders-on-storage",
    "title": "Neurodesktop access",
    "section": "For interfacing with the folders on storage",
    "text": "For interfacing with the folders on storage\nYou can create symbolic links in your home directory:\nln -s /storage/ /home/jovyan/storage",
    "crumbs": [
      "Home",
      "Neurodesktop"
    ]
  },
  {
    "objectID": "neurodesk/neurodesktop.html#mount-network-shares",
    "href": "neurodesk/neurodesktop.html#mount-network-shares",
    "title": "Neurodesktop access",
    "section": "Mount network shares",
    "text": "Mount network shares\nYou can mount network shares with Lukas’ helper script at /shared/mount_network_share.sh (i.e. you´ll see a directory through which you can access the network from within neurodesk)\n\nUsage: /shared/mount_network_share.sh &lt;mount_point&gt; &lt;share_choice|full_share_path&gt;\nShare choices\n\nJ \\(\\to\\) mounts //pslg067044.pers.ad.uni-graz.at/Allgemeine\nY \\(\\to\\) mounts //pers.ad.uni-graz.at/fs/ou/602\nAny other value \\(\\to\\) treated as a full UNC path (e.g. //server/share/path)\n\nExamples:\n\n/shared/mount_network_share.sh /home/jovyan/allg_j J\\(\\to\\) mounts the J network to /home/jovyan/allg_j (i.e. there will be a directory allg_j within your /home/jovyan/ directory that accesses the J network)\nshared/mount_network_share.sh /home/jovyan/allg_y Y\\(\\to\\) mounts the Y network to /home/jovyan/allg_y (i.e. there will be a directory allg_y within your /home/jovyan/ directory that accesses the Y network)",
    "crumbs": [
      "Home",
      "Neurodesktop"
    ]
  },
  {
    "objectID": "neurodesk/neurodesktop.html#checking-resource-usage-directly-on-the-server",
    "href": "neurodesk/neurodesktop.html#checking-resource-usage-directly-on-the-server",
    "title": "Neurodesktop access",
    "section": "Checking resource usage directly on the server",
    "text": "Checking resource usage directly on the server\n\nresource usage per neurodesk container\n\ndocker stats\n\nper user and process\n\nhtop\n(all student containers are assigned to one user “allg_user”)",
    "crumbs": [
      "Home",
      "Neurodesktop"
    ]
  },
  {
    "objectID": "mri/physiology/index.html",
    "href": "mri/physiology/index.html",
    "title": "Physiological noise correction",
    "section": "",
    "text": "October 2023; additional info and further read: https://www.sciencedirect.com/science/article/pii/S016502701630259X?via%3Dihub",
    "crumbs": [
      "Home",
      "MRI",
      "Physiological noise correction"
    ]
  },
  {
    "objectID": "mri/physiology/index.html#acquistion-of-log-files",
    "href": "mri/physiology/index.html#acquistion-of-log-files",
    "title": "Physiological noise correction",
    "section": "Acquistion of log-files:",
    "text": "Acquistion of log-files:\n\nPhysiology logging:\n\nPress tab + [-&gt; + Entf to enter admin mode\nPMU control\ncontrol-esc\nrun -&gt; cmd\nin the terminal, type ideacmdtool\nDON’T use startLogAll, start puls and ECG separately (if both are started simultaneously, recording will terminate after 5 min.)\nAfter a while there should be a message with “ok” at the end\nData is stored in C:\\ProgramFiles\\Siemens\\Numaris\\ Mars\\log\\ (there is a shortcut “log” in the Explorer )\nStop recording after experiment with stopLogAll, move data to C:\\_Natalia\\STUDYNAME",
    "crumbs": [
      "Home",
      "MRI",
      "Physiological noise correction"
    ]
  },
  {
    "objectID": "mri/physiology/index.html#required",
    "href": "mri/physiology/index.html#required",
    "title": "Physiological noise correction",
    "section": "Required:",
    "text": "Required:\n\nJson- files of functional data for each run\nPhysiology log-files, one for all runs (.resp and .puls files)\n\nFigure 1. Puls and resp-file.\n\n\nScript: /storage/nv_shared/PhysIO_excitex/PhysIO_get_regressors.m",
    "crumbs": [
      "Home",
      "MRI",
      "Physiological noise correction"
    ]
  },
  {
    "objectID": "mri/physiology/index.html#before-running-the-script",
    "href": "mri/physiology/index.html#before-running-the-script",
    "title": "Physiological noise correction",
    "section": "Before running the script:",
    "text": "Before running the script:\n\nAlways check if on path: SPM: /usr/local/spm12/ Toolbox-code modified for our scanner model: /storage/nv_shared/software/PhysIO/tapas-master/PhysIO/code\nIndividual parameter settings in the script:\n\nphysio.log_files.vendor = ‘Siemens_xa20’; % specific for our scanner model.\nphysio.log_files.resp_type = ‘spine’; % set to spine or belt, depending on type of physiology correction.\nphysio.model.orthogonalise = ‘RETROICOR’; % select depending on physiology model used, e.g. ‘RETROICOR’, or c, r; to avoid correlations between regressors.\nphysio.model.retroicor.include = true; % select physiology model here: RETROICOR, RVT or HVT.\nphysio.model.retroicor.order.c = 3;\nphysio.model.retroicor.order.r = 4;\nphysio.model.retroicor.order.cr = 1; % select orders for RETROICOR model; here the default option based on Glover et al., 2000 is set.\nphysio.model.rvt.include = false; % select physiology model here.\nphysio.model.hrv.include = false; % select physiology model here.\nphysio.model.other.include = false; % select physiology model here.\nphysio.verbose.level = 3; % 2 or 3, will produce control plots – useful to check quality of data and if synchronization has worked; when analyzing several participants it’s best to set to 0.",
    "crumbs": [
      "Home",
      "MRI",
      "Physiological noise correction"
    ]
  },
  {
    "objectID": "mri/physiology/index.html#after-running-the-script",
    "href": "mri/physiology/index.html#after-running-the-script",
    "title": "Physiological noise correction",
    "section": "After running the script:",
    "text": "After running the script:\n\nValidation of results - Check the diagnostic plots the toolbox delivers:\n\nCheck the raw time series for artifacts/ detachments (4A)\nCheck distribution of raw physiological data (should be left-skewed, 4B), are there any indications for ceiling effects or temporary detachment (2)?\n\nCheck period used for synchronization, does it fit? (4C)\nCheck filtered time series (are there still severe artifacts?)\nCheck regressor file (3): regressors should have a value for each slice and 18 regressors if you specified RETROICOR (i.e., 6 cardiac phase regressors, 8 respiratory phase regressors, and 4 interaction terms).\nRegressors can then be added to a GLM.\n\n\nFigure 2. Artifacts in raw respiratory signal due to issues during recording with the breathing belt.\n\nFigure 3. Final output (RETROICOR regressors)\n\nFigure 4. Steps from raw data to the final regressors.",
    "crumbs": [
      "Home",
      "MRI",
      "Physiological noise correction"
    ]
  },
  {
    "objectID": "mri/mri-lab-setup-2024/index.html",
    "href": "mri/mri-lab-setup-2024/index.html",
    "title": "Update to the MRI lab setup",
    "section": "",
    "text": "The MRI lab has had some upgrades in 2024 including:\n\nNew stimulation PC\nStimulation goggles\n\nThis document details some information about the new setup",
    "crumbs": [
      "Home",
      "MRI",
      "Update to the MRI lab setup"
    ]
  },
  {
    "objectID": "mri/mri-lab-setup-2024/index.html#general-setup-information",
    "href": "mri/mri-lab-setup-2024/index.html#general-setup-information",
    "title": "Update to the MRI lab setup",
    "section": "General setup information:",
    "text": "General setup information:\n\nStarting\n\nTurn on the stimulus presentation PC\nTurn on the eyetracking PC (if using the goggles or eyetracking)\n\n\nEven if not using the eyetracker but using the goggles it is useful to use the eyetracker to be able to ensure that the goggles are aligned properly with the participant\n\n\nLogin to the stimulus pc and/or the eyetracker pc with the credentials provided\n\n\nWhen logging into the stimulus PC it might be that you actually don’t see anything on the screen yet if there are other screens connected to the stimulus PC, therefore the sequence of keystrokes when starting the PC is:\n\n    1. Enter decryption password \n\n    2. Press enter \n    \n    3. Press enter again (this selects the top-most user)\n    \n    4. Enter the user password required from the technician (or someone else in our lab)\n\n\nPlug peripherals in\n\nIf you are using the goggles then make sure that the HDMI cables are first plugged in\n\n\n\n\n\nGoggles HDMI inputs\n\n\n\n\nThe right goggle’s hdmi cable (labelled with tape) should be in the top most position and the left in the bottom position\n\n\nThe goggles can now be plugged in\n\n\n\n\n\nGoggles plug unplugged\n\n\n\n\n\nGoggles plug plugged in\n\n\n\n\nThe goggles need to be additionally turned on inside the scanner room (double check with technician that this is the case)\nIf using the eyetracker function of the goggles plug in the cable to link the eyetracker signal to the PC\n\n\n\n\n\nThe light on the eyetracker should be illuminated red\n\n\n\n\nStart ViewPoint and you should see something other than ‘no signal’\n\n\nFor more information visit the arringtonresearch website\n\n\n\nExtend/ mirror display\n\nPopOS (the stimulation PC’s operating system) might be a bit funky when it comes to changing the display settings when doing so with the settings menu in PopOS\n\n\nWhen opening the display settings do not click apply but instead choose a different settings option e.g. power/ sound/ notficiations etc. and then close the settings menu\n\n\n\n\n\nPopOS example settings window, always make sure to close this window and not apply new display settings\n\n\n\n\nUse shortcut win + p to change whether to extend or mirror the screens, you can hit win + p twice and this will change to mirror mode\n\n\n\nKVM switcher\n\nThe KVM switcher is used to switch between devices and for the goggle + stimulation PC setup can be used to switch between either the eyetracker PC or stimulus PC\n\n\n\n\n\nKVM switcher, button 1 = eyetracker pc, 2 = stimulus pc\n\n\n\n\n\nTrigger box\n\nIf the trigger box is not powered, plug it in\n\nSometimes the cable for the trigger box is unplugged/ or the cable that is plugged in is not attached to a PC\nIf this is the case the USB light on the box will be flashing (indicating that the USB is not connected)\nLocate at the back of the box the corresponding USB input and plug it in\n\n\n\n\n\nBack of the trigger box\n\n\n\n\nThe USB light should stop flashing\nTest the button box inside the scanner\n\n\n\n\n\nTrigger box front\n\n\n\n\n8 lights on the left are blinking whenever the 8 buttons on the response boxes are being pressed\nOn the right, the top light is constantly turned on if it is connected to the computer. On the bottom, the light is blinking whenever the scanner trigger is being sent\nTrigger box settings\nThe trigger box settings for the normal button box which we typically use should look as in the picture below\n\nIf not, press the round button and select the following settings:\nManual config\n\nHHSC-2x4-L\n\nUSB\n\nHID KEY 12345\n\nPress some buttons in the scanner to make sure the trigger box works. If not, unplug it and plug it in again.\n\n\n\nExperimental Protocol\nBefore entering the scanner room always make sure you are MR safe and sign the respective forms\n\nAsk the participant to sign the forms while doing so ensure that the button box inside the scanner works and double check the view on the goggles that everything looks okay\n\n\n\nPsychopy\n\nStart psychopy by opening the terminal and typing Psy and the pressing “Tab”\nYou will see multiple PsychoPy versions available listed ad bash scripts (.sh extension)\nChoose the version you need and run it\nUsually psychopy starts by asking to open a .psyexp file so just open the one you have or if using the coder only:\n\nOpen a .psyexp file that doesn’t apply to you\nquit builder view and do not save\n\nNow you can open a .py file\n\nAlternatively start psychopy with the -c flag\n\n\nPsychtoolbox\nDifferently from PsychoPy, PsychToolbox under Linux handles multiple keyboards independently. It is therefore important that your PsychToolbox script reads scanner trigger (Key ‘5%’) and button box input from the correct keyboard (Trigger Box). To check the Keyboard ID of the Trigger Box and its name, run “response_box_check.m” prior to putting the subject into the scanner. Currently, the keybaord name is ‘Current Designs, Inc. 932’ , but the Keyboard ID can vary depending on other devices that are plugged in.\nHere are the steps:\n\nStart the script\nPress some buttons on the subject response box inside the scanner room\nIf you see the confirmation of buttons being presses in the command line, this is all fine\nIf not, exit by pressing ESCSPE, restart the script and try another keyboard from the list.\n\nIt is highly recommended to do this every time before putting a subject into the scanner, to avoid issues when the subject is inside.",
    "crumbs": [
      "Home",
      "MRI",
      "Update to the MRI lab setup"
    ]
  },
  {
    "objectID": "mri/mni-space/mni-space.html",
    "href": "mri/mni-space/mni-space.html",
    "title": "MNI space",
    "section": "",
    "text": "This is a summary of useful information about MNI spaces, collected by Carmen. This page should be extended at some point to incorporate lab-specific information.\nShort: About the MNI space(s) – Lead-DBS\nDetailed and up-to-date: About the mNI space(s) - BIDS Team\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "MRI",
      "MNI space"
    ]
  },
  {
    "objectID": "mri/analysis/fmriprep.html",
    "href": "mri/analysis/fmriprep.html",
    "title": "fMRIprep analysis",
    "section": "",
    "text": "Link to fMRIprep documentation\nLink to fMRIprep github (Useful information in the issues section about troubleshooting etc.)",
    "crumbs": [
      "Home",
      "MRI",
      "fMRIprep analysis"
    ]
  },
  {
    "objectID": "mri/analysis/fmriprep.html#reasons-to-use-fmriprep",
    "href": "mri/analysis/fmriprep.html#reasons-to-use-fmriprep",
    "title": "fMRIprep analysis",
    "section": "Reasons to use fMRIprep",
    "text": "Reasons to use fMRIprep\n\nfMRIprep is a useful tool to preprocess fMRI data in a manor that can be reproducible across research\nWhere possible fMRIprep should be used in preprocessing to limit the variability involved in preprocessing data\nfMRIprep follows the BIDS format see here on how to validate bids\n\n \n\nPermissions\n\nfMRIprep is run as a docker container meaning that you will need to be added to the docker group on the server before running it\nBefore running fMRIprep it is necessary to create the folders that fMRIprep uses\n\nScratch directory (meaning just the temporary output files for fMRIprep)\nOutput directory (The actual output of fMRIprep)\n\nTo create the folders first run the following:\n\nmkdir $niidir/../fmriprep_tmp\nmkdir $niidir/derivatives/fmriprep\n\nTo avoid the output folders of fMRIprep being owned by root you can run fMRIprep with the following flags:\ndocker run -u $(id -u):$(id -g) ...\n\nNOTE: using fitlins does not currently work using the flag -u $(id -u):$(id -g) the user must specify their user id directly e.g. -u 1087:1092 see post about fitlins",
    "crumbs": [
      "Home",
      "MRI",
      "fMRIprep analysis"
    ]
  },
  {
    "objectID": "mri/analysis/fmriprep.html#example-fmriprep-scripts",
    "href": "mri/analysis/fmriprep.html#example-fmriprep-scripts",
    "title": "fMRIprep analysis",
    "section": "Example fMRIprep scripts",
    "text": "Example fMRIprep scripts\n\nfMRIprep without prerun FreeSurfer\nAn example script of fMRIprep when there is no prerun FreeSurfer\nexport PATH=~/local/python2/:$PATH\nexport FREESURFER_HOME=/usr/local/freesurfer_7.3.2\nexport MATLAB=/usr/bin/matlab\nexport FSL_DIR=/usr/local/fsl\nexport FSF_OUTPUT_FORMAT=nii.gz\nniidir=/storage/adam/CLAUS_3T/pilots/bids\nexport SUBJECTS_DIR=$niidir/derivatives/freesurfer/\npath=($path /usr/local/matlab/R2019b/bin/)\nsource $FREESURFER_HOME/SetUpFreeSurfer.sh\n\nsubject=sub-01\n\nrsync -a -v $niidir/$subject/ses-1/anat/*T1w.nii.gz $niidir/../all_anat_faced/\n#--- deface ---#\nscansForDefacing=$(ls -1 $niidir/$subject/ses-1/anat/*.nii.gz)\necho $scansForDefacing\nfor theScan in $scansForDefacing; do\n    echo \"defacing scan $theScan\"\n    pydeface $theScan --outfile $theScan --force\ndone\n#--- mriqc (current v 23.0.1) ---#\ndocker run -it --rm -v $niidir:/data:ro -v $niidir/derivatives/mriqc:/out nipreps/mriqc:23.0.1 /data /out participant --participant_label $subject --no-sub\n\n\nmkdir $niidir/../fmriprep_tmp\nmkdir $niidir/derivatives/fmriprep\n\ndocker run -u $(id -u):$(id -g) --rm -it \\\n        -v /usr/local/freesurfer_7.3.2/license.txt:/opt/freesurfer/license.txt \\\n        -v $niidir/../fmriprep_tmp:/scratch -v $niidir:/data:ro \\\n        -v $niidir/derivatives/fmriprep:/out \\\n        -v $niidir/derivatives/freesurfer:/sub-dir nipreps/fmriprep:21.0.1 \\\n        /data /out participant --participant-label $subject \\\n        --md-only-boilerplate --skip-bids-validation --stop-on-first-crash --nthreads 4 -w /scratch --output-spaces T1w\n \n\n\nfMRIprep prerun FreeSurfer (no hires)\nAn example script with prerun FreeSurfer (without hires options and biascorrecing the anatomical scan):\nexport PATH=~/local/python2/:$PATH\nexport FREESURFER_HOME=/usr/local/freesurfer_7.3.2\nexport MATLAB=/usr/bin/matlab\nexport FSL_DIR=/usr/local/fsl\nexport FSF_OUTPUT_FORMAT=nii.gz\nniidir=/storage/adam/CLAUS_3T/pilots/bids\nexport SUBJECTS_DIR=$niidir/derivatives/freesurfer/\npath=($path /usr/local/matlab/R2019b/bin/)\nsource $FREESURFER_HOME/SetUpFreeSurfer.sh\n\nsubject=sub-01\n\nrsync -a -v $niidir/$subject/ses-1/anat/*T1w.nii.gz $niidir/../all_anat_faced/\n#--- deface ---#\nscansForDefacing=$(ls -1 $niidir/$subject/ses-1/anat/*.nii.gz)\necho $scansForDefacing\nfor theScan in $scansForDefacing; do\n    echo \"defacing scan $theScan\"\n    pydeface $theScan --outfile $theScan --force\ndone\n#--- mriqc (current v 23.0.1) ---#\ndocker run -it --rm -v $niidir:/data:ro -v $niidir/derivatives/mriqc:/out nipreps/mriqc:23.0.1 /data /out participant --participant_label $subject --no-sub\n# freesurfer\nrecon_path=$niidir/$subject/ses-1/anat/*.nii.gz\nrecon-all -all -i $recon_path -s $subject -threads 4\n\n\nmkdir $niidir/../fmriprep_tmp\nmkdir $niidir/derivatives/fmriprep\n\n# Here we add the flag --fs-output-dir to specify the directory where freesurfer output is located\ndocker run -u $(id -u):$(id -g) --rm -it \\\n        -v /usr/local/freesurfer_7.3.2/license.txt:/opt/freesurfer/license.txt \\\n        -v $niidir/../fmriprep_tmp:/scratch -v $niidir:/data:ro \\\n        -v $niidir/derivatives/fmriprep:/out \\\n        -v $niidir/derivatives/freesurfer:/sub-dir nipreps/fmriprep:21.0.1 \\\n        /data /out participant --fs-subjects-dir /sub-dir --participant-label $subject \\\n        --md-only-boilerplate --skip-bids-validation --stop-on-first-crash --nthreads 4 -w /scratch --output-spaces T1w",
    "crumbs": [
      "Home",
      "MRI",
      "fMRIprep analysis"
    ]
  },
  {
    "objectID": "mri/analysis/fmriprep.html#troubleshooting",
    "href": "mri/analysis/fmriprep.html#troubleshooting",
    "title": "fMRIprep analysis",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nI ran fMRIprep but no distortion correction happened\n\nfMRIprep might be using an outdated BIDS specification. Here this is related to the fmap json file. See Information about BIDS uri.\n\n\nIf your fmap .json file is using the BIDS uri specification in the IntendedFor field and you are using an older fMRIprep version then this will need to be changed to the older path specification e.g.\n\nBIDS uri spec:\n\"IntendedFor\": [\n    \"bids::sub-1181001/ses-1/func/sub-1181001_ses-1_task-movie_run-1_bold.nii.gz\",\n    \"bids::sub-1181001/ses-1/func/sub-1181001_ses-1_task-movie_run-2_bold.nii.gz\", \n    etc\n    ]\nOlder fmap spec:\n\"IntendedFor\": [\n    \"ses-1/func/sub-1181001_ses-1_task-movie_run-1_bold.nii.gz\",\n    \"ses-1/func/sub-1181001_ses-1_task-movie_run-2_bold.nii.gz\",\n    etc\n    ]\n\n \n\n \n\nI ran fMRIprep and the distortion correction of the data is too extreme\n\n\n\n\n\nAn example of poor distortion correction\n\n\n\n\nThis is an issue with certain issues of fMRIprep. Using fMRIprep version 20.2.7 may be needed to avoid the extreme distortion correction (newer versions of fMRIprep haven’t been tested)\n\n \n\n \n\nIf using already run FreeSurfer recon-all output and have specified an expert-opts file then fMRIprep shows multiple errors\n\nIn the directory that has the prerun recon-all there is a folder scripts that saves the expert-opts file. Removing this file allows fMRIprep to run\n\n\n\n\nIf fmriprep fails silently (i.e. gives no error) on coregistering the functional to the structural scan, the solution is (if your structural and functional scans are from the same session) to add the following flag:\n--bold2t1w-init header",
    "crumbs": [
      "Home",
      "MRI",
      "fMRIprep analysis"
    ]
  },
  {
    "objectID": "mri/analysis/7t_analysis_on_neurodesk.html#basics",
    "href": "mri/analysis/7t_analysis_on_neurodesk.html#basics",
    "title": "7T anatomical analysis on neurodesk",
    "section": "Basics",
    "text": "Basics\nAfter converting the data from DICOMS to niftis (BIDS format) you will notice that unlike 3T data, you might have collected 3 different anatomical scans:\n\nINV1 (1st inversion)\nINV2 (2nd inversion)\nUNI (MP2RAGE)\n\nThe files we really care about for the anatomical analysis are INV2 and UNI (MP2RAGE)\nWhen opening an MP2RAGE scan it is clear that there is a lot of noise that we do not get at 3T.\nThis means that we first need to get rid of this noise before using recon-all (recon-all will fail if trying to pass it these noisy images).\n\nThis means there are a few extra pieces of software that is needed to be able to run the anatomical analysis\n\n\nWhat software is needed for 7T anatomical analysis?\n\nPresurfer (uses MATLAB/ SPM12)\nCAT12\nFreeSurfer (to use recon-all)",
    "crumbs": [
      "Home",
      "MRI",
      "7T anatomical analysis on neurodesk"
    ]
  },
  {
    "objectID": "mri/analysis/7t_analysis_on_neurodesk.html#neurodesk-and-containers",
    "href": "mri/analysis/7t_analysis_on_neurodesk.html#neurodesk-and-containers",
    "title": "7T anatomical analysis on neurodesk",
    "section": "Neurodesk and containers",
    "text": "Neurodesk and containers\nContainers are explained in more detail here.\n\nPut simply, currently on neurodesk we cannot load 2 or 3 pieces of software at once and this means that the anatomical analysis of 7T data is a bit tricky.\nLuckily Denis Chaimow (link to github) has created a container in which we can run the analysis.\nThe container and scripts I edited to allow the use of processing anatomical results for a single subject with multiple sessions",
    "crumbs": [
      "Home",
      "MRI",
      "7T anatomical analysis on neurodesk"
    ]
  },
  {
    "objectID": "mri/analysis/7t_analysis_on_neurodesk.html#how-to-run-the-script",
    "href": "mri/analysis/7t_analysis_on_neurodesk.html#how-to-run-the-script",
    "title": "7T anatomical analysis on neurodesk",
    "section": "How to run the script?",
    "text": "How to run the script?\n\nThe script should be located in /shared/7T_analysis/anatomy\n\n0. Open a terminal:\n\n\n\nOpening a terminal on neurodesk\n\n\n \n1. Start the container\n\nThe container can be started like so:\n\nsingularity run --bind /path/to/output:/derivatives /shared/7T_analysis/anatomy/mp2rage_recon-all.sif\n\nHere we need to make sure that the /path/to/output is exactly where our data is stored.\n\nFor example:\n\nHere my anatomical data folder is stored in /home/jovyan/7t_auditory/bids/sub-a001/ses-1\n\n\n\n\nLocating where the data is stored\n\n\n\nThis means that I want to mount this folder to the container so my command will be like so:\n\nsingularity run --bind /home/jovyan/7t_auditory/bids/sub-a001/ses-1/:/derivatives /shared/7T_analysis/anatomy/mp2rage_recon-all.sif\n\nThis then means that once in side the container the path to my data is now mounted to a folder called /derivative\nPut simply I create a link from my local folder /home/jovyan/7t_auditory/bids/sub-a001/ses-1/ to a folder inside the container called /derivatives\n\n2. Once inside the container\n\nTo know if you are in the container or not you will see the following:\n\n\n\n\nInside the container\n\n\n\nThis means that we are in our own environment and that we can run the command necessary to start the anatomical preprocessing\n\n3. Export FreeSurfer subjects directory\n\nThis command should be the same for everyone\n\nexport $SUBJECTS_DIR=/derivatives/mprage_recon-all/freesurfer\n4. Run the script\n\nWe need to now start the preprocessing script\n\n\n\n\n\n\n\nRecall\n\n\n\n\nRemember that we have mounted a folder to this folder: /derivatives\nThis means that everything you see inside of your local directory e.g. /home/jovyan/7t_auditory/bids/sub-a001/ses-1/ is mirrored inside the container\n\n\n\n\nTherefore, we can run the script like so:\n\nmp2rage_recon-all.py --inv2 /derivatives/anat/inv2.nii --uni /derivatives/anat/uni.nii\n5. The script will now start and run everything needed to run the anatomical analysis\n\n\n\n\n\n\nmultiple sessions?\n\n\n\n\nIf you have multiple sessions you need to make sure that all anatomical scans sessions are inside just 1 folder\nYou can then run the command:\n\nmp2rage_recon-all.py --inv2 /derivatives/anat/inv2_session1.nii /derivatives/anat/inv2_session2.nii --uni /derivatives/anat/uni_session1.nii /derivatives/anat/uni_session2.nii",
    "crumbs": [
      "Home",
      "MRI",
      "7T anatomical analysis on neurodesk"
    ]
  },
  {
    "objectID": "mri/analysis/7t_analysis_on_neurodesk.html#troubleshooting",
    "href": "mri/analysis/7t_analysis_on_neurodesk.html#troubleshooting",
    "title": "7T anatomical analysis on neurodesk",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nThe files will need to be unzipped first. This is becuase the script uses SPM12 and that this is only able to read files that end in .nii and not .nii.gz\n\nRun the commands in a new terminal window:\nml freesurfer\nmri_convert /path/to/file.nii.gz /path/to/file.nii\n\n\n\n\n\n\nValueError: substring not found\n\n\n\nOSError: This docstring was not generated by Nipype!\nThis error occurs for unknown reasons. The suggested fix is export FORCE_SPMMCR as an environment variable. So run this in the terminal:\n\nexport FORCE_SPMMCR=1\nhttps://neurostars.org/t/valueerror-substring-not-found-when-running-spm-spmcommand-version-with-spm-standalone-and-mcr-installation/24082",
    "crumbs": [
      "Home",
      "MRI",
      "7T anatomical analysis on neurodesk"
    ]
  },
  {
    "objectID": "labmeeting.html",
    "href": "labmeeting.html",
    "title": "Information about our lab meeting",
    "section": "",
    "text": "Labmeeting\n\n\n\nLab\n\n\n\nInformation about our lab meeting\n\n\n\nNZ, MG\n\n\nJan 4, 2025\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "internship.html",
    "href": "internship.html",
    "title": "MRI: from DICOMs to GLM",
    "section": "",
    "text": "Internship project - Automatic Claustrum Segmentation\n\n\n\nInternship\n\n\nLab\n\n\n\nShort summary of an internship about the investigation of different methods to do an automatic segmentation of the human claustrum\n\n\n\nBK, MG\n\n\nJul 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPflichtpraktikum\n\n\n\nInternship\n\n\nLab\n\n\n\nInformation for compulsory internship for bachelor students\n\n\n\nHM, MG\n\n\nJul 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nInternship Checklist\n\n\n\nInternship\n\n\nLab\n\n\n\nInformation for starting as an intern\n\n\n\nNZ\n\n\nMay 8, 2025\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top",
    "crumbs": [
      "Home",
      "Internship"
    ]
  },
  {
    "objectID": "internship/claustrum_segmentation.html",
    "href": "internship/claustrum_segmentation.html",
    "title": "Internship project - Automatic Claustrum Segmentation",
    "section": "",
<<<<<<< HEAD
    "text": "In diesem Projekt wurden verschiedene Methoden zur automatischen Segmentierung des menschlichen Claustrum untersucht. Dabei wurde ein hochauflösender 7T TI-gewichteter MRT scan verwendet.\n\nNextBrain (Histoatlas) Segmentierung\n\n\nCasamitjana et al. (2024)\nverwendet bayesian Algorithmus und KI Methoden\n333 unterschiedliche Regions of Interest\nMethode: ~10000 serielle, histologische 3D Schnittbilder von 5 Hemispähren\nVerwendung unabhängig der Auflösung\nVerarbeitet das gesamte Gehirn, kann dementsprechend einige Tage laufen\n\nVoraussetzungen:\n\nFreeSurfer als Neurodesk-Modul geladen\nInput: z. B. sub-01_ses-01_7T_T1w_defaced.nii.gz\nOutput-Ordner: beliebig (z. B. ~/freesurfer-output)\n\nBefehl:\nmri_histo_atlas_segment \\\n/path/to/input.nii.gz \\\n/path/to/output_folder \\\nfull \\\n1 \\\n-1\nOutput:\n- Struktursegmentierung `seg_right.mgz`, `seg_left.mgz`\n- .csv mit Volumina\nVisualisierung: freeview -v input.nii.gz -v seg_right.mgz:colormap=lut:lut=path/to/AllenAtlasLUT\nTroubleshooting:\n- Stelle sicher, dass Lookup Table (AllenAtlasLUT) korrekt verlinkt ist\n- Bei Performanceproblemen: viel Zeit einplanen (v. a. bei 0.25 mm Auflösung)\n\n\nSynthSeg (Mauri et al.) Claustrum Segmentierung\n\n\nMauri et al. (2024)\nbasiert auf Deep Learning (SynthSeg)\nverwendet syntethische Daten, die aus Label-Maps generiert werden\nMethode: an 18 ultra-hochauflösenden MRT scans mit manuellem Claustrum Label trainiert\nVerwendung unabhängig vom contrast (T1, T2)\nrobust gegenüber Auflösung\n\nVoraussetzungen:\n- Python 3.8 Conda-Umgebung (z. B. synthseg_env)\n- FreeSurfer-Modul geladen (FREESURFER_HOME optional, aber hilfreich)\n- model.h5 vorhanden in claustrum_segmentation/model/\nInput: hochaufgelöste T1-Datei (z. B. T1.mgz oder .nii.gz)\nBefehl:\ncsh /home/jovyan/SynthSeg/claustrum_segmentation/mri_claustrum_seg \\\n--i /path/to/input.nii.gz \\\n--o /path/to/output_folder \\\n--model /path/to/model.h5 \\\n--threads 32 \\\n--qc \\\n--surf\nOutput:\n- `claustrum.lh.nii.gz`, `claustrum.rh.nii.gz`\n- Optional: .mgz-Konvertierung via mri_convert\nTroubleshooting:\n- ![:x:](https://a.slack-edge.com/production-standard-emoji-assets/14.0/google-medium/274c.png) model.h5 zu klein? $\\to$ Neu herunterladen\n- ![:x:](https://a.slack-edge.com/production-standard-emoji-assets/14.0/google-medium/274c.png) Kein seg.lh.mgz? $\\to$ Crash durch fehlende LUT oder unvollständige Ausführung\n- ![:weißes_häkchen:](https://a.slack-edge.com/production-standard-emoji-assets/14.0/google-medium/2705.png) Vorher export FREESURFER_HOME=/path/to/freesurfer_module setzen\n- ![:x:](https://a.slack-edge.com/production-standard-emoji-assets/14.0/google-medium/274c.png) TEMP-Ordner leer? $\\to$ Python-Fehler oder Pfadproblem\n- Optional: Denoising vor Segmentierung vermeiden (Registrierungsfehler möglich)\n\n\nMezerLab Claustrum Segmentierung\n\n\nBerman et al. (2020)\nnur dorsales Claustrum\nbasierend auf dem Human Connectome Project\nMethode: basierend auf 1068 bilateralen, dorsalen Claustrum Segmentierungen\nValidierung durch den Vergleich automatischer und manueller Claustrum Segmentierungen in 10 Datensätzen (2 Rater)\n\n\nModule laden (im LX-Terminal)\n\nmodule load matlab/2024b\\\nmodule load spm12/r7771\\\nmodule load fsl/6.0.7.16\\\nmodule load mrtrix3/3.0.4\n\nT1-Scan vorbereiten\n\nmri_convert /home/jovyan/matlab/T1.mgz /home/jovyan/matlab/T1.nii.gz \\\nbet /home/jovyan/matlab/T1.nii.gz /home/jovyan/matlab/T1_brain.nii.gz \\\nfslreorient2std /home/jovyan/matlab/T1_brain.nii.gz /home/jovyan/matlab/T1_brain_std.nii.gz\n\n5TT-Datei erzeugen (voll aufgelöst)\n\n\n5ttgen fsl /home/jovyan/matlab/T1_brain_std.nii.gz /home/jovyan/matlab/T1_5tt.nii.gz\nFalls T1_5tt.nii.gz zu niedrige Auflösung hat \\(\\to\\) Erneut ausführen mit nicht zugeschnittenem Input: 5ttgen fsl /home/jovyan/matlab/T1.nii.gz /home/jovyan/matlab/T1_5tt_fixed.nii.gz\n\n\nFSL FIRST-Segmentierung\n\nrun_first_all -i /home/jovyan/matlab/T1_brain_std.nii.gz -o /home/jovyan/matlab/first_output \\\ncp /home/jovyan/matlab/first_output_all_fast_firstseg.nii.gz /home/jovyan/matlab/first_seg.nii.gz\n\nMATLAB starten: matlab\nPfade in MATLAB setzen\n\naddpath(genpath('/home/jovyan/matlab/vistasoft')); \\\naddpath(genpath('/home/jovyan/matlab/ClaustrumSegmentation')); \\\naddpath(genpath('/home/jovyan/matlab/spm12')); \\\nspm('defaults', 'fmri'); \\\nspm_jobman('initcfg');\n\nSegmentierung ausführen\n\nt1wFile = '/home/jovyan/matlab/T1_brain_std.nii.gz'; \\\ntt5File = '/home/jovyan/matlab/T1_5tt_fixed.nii.gz'; % Verwende fixierte Datei \\\nfslSegFile = '/home/jovyan/matlab/first_seg.nii.gz'; \\\n\nsegment_claustrum_rh(t1wFile, tt5File, fslSegFile, '/home/jovyan/matlab/claustrum_output/rh_claustrum.nii.gz'); \\\nsegment_claustrum_lh(t1wFile, tt5File, fslSegFile, '/home/jovyan/matlab/claustrum_output/lh_claustrum.nii.gz');\nTroubleshooting (kurz & präzise)\n\n\n\n\n\n\n\n\nProblem\nUrsache\nLösung\n\n\n\n\nKein lh_claustrum.nii.gz\nFalsches oder leeres tt5File\nNutze T1_5tt_fixed.nii.gz\n\n\nkmeans: X must have more rows than clusters\ncandidate enthält &lt;2 Werte\nTT5 zu grob / CSF-Filter zu streng → TT5 neu erstellen\n\n\ncsfMask(find(tmp)) = 0; Fehler\nTT5-Auflösung unpassend\nMit flirt resamplen oder TT5 neu erzeugen\n\n\nFIRST-Output fehlt\nrun_first_all nicht korrekt ausgeführt\n-i und -o prüfen\n\n\nbet oder run_first_all nicht gefunden\nModul nicht geladen\nmodule load fsl/...\n\n\nAuflösungsfehler\nBildgrößen ungleich\nAlle Inputs auf 1×1×1 mm prüfen (z. B. mit mrinfo, fslhd)\n\n\n\nOptional:Visualisierung in FreeSurfer\nmri_convert /home/jovyan/matlab/claustrum_output/rh_claustrum.nii.gz /home/jovyan/matlab/claustrum_output/rh_claustrum.mgz \\\nmri_convert /home/jovyan/matlab/claustrum_output/lh_claustrum.nii.gz /home/jovyan/matlab/claustrum_output/lh_claustrum.mgz \\\n\nfreeview -v /home/jovyan/matlab/T1.mgz \\\n-v /home/jovyan/matlab/claustrum_output/rh_claustrum.mgz \\\n-v /home/jovyan/matlab/claustrum_output/lh_claustrum.mgz\n\n\nClaustrum Segmentierung nach Li et al.\n\n\nLi et al. (2021)\nmulti-view Deep Learning\nMethode: 181 Versuchspersonen mit bilateralen, manuellen Segementierungen durch Fachperson in Neuroradiologie\nbenötigt ~75 Testdatensätze um robuste Ergebnisse zu liefern\nfunktioniert nur mit einer Auflösung von 1mm\\(^3\\)\n\n\nResampling\n\nPython-Skript aus dem GitHub-Repository nutzen (resample.py)\nEingabepfad, Zielauflösung (1 mm³), und Output-Pfad anpassen\n\nSkull-Stripping\n\nMit FSL-BET\n\n\nB.: bet2 input_resampled.nii.gz output_ss.nii.gz -f 0.5 -g 0 -m\n\n\nDenoising\n\nANTs DenoiseImage verwenden\n\nB.: DenoiseImage -d 3 -i output_ss.nii.gz -o output_ss_denoised.nii.gz\n\n\nClaustrum-Segmentierung mit Multi-View-Netz\n\nGitHub Repository claustrum demo klonen (im read me)\nVerzeichnisstruktur beibehalten (z. B. data/test/subjectID/subjectID_T1w_denoised.nii.gz)\nModelle in models/ einfügen (axial und coronal)\nIn Conda-Umgebung mit Python 3.8 und TensorFlow 2.3–2.5 arbeiten (außedem mit keras, Simple Itk, scipy, numpy, PIL)\nAlle Abhängigkeiten mit pip install -r requirements.txt installieren\nAusführen mit: python test_two_views.py\n\nOutput: Segmentierungsmaske wird in output/ gespeichert.\n\n\n\n\n Back to top",
=======
    "text": "NextBrain (Histoatlas) Segmentierung\n\nVoraussetzungen:\n\nFreeSurfer als Neurodesk-Modul geladen\nInput: z. B. sub-01_ses-01_7T_T1w_defaced.nii.gz\nOutput-Ordner: beliebig (z. B. ~/freesurfer-output)\n\nBefehl:\nmri_histo_atlas_segment \\\n/path/to/input.nii.gz \\\n/path/to/output_folder \\\nfull \\\n1 \\\n-1\nOutput:\n- Struktursegmentierung `seg_right.mgz`, `seg_left.mgz`\n- .csv mit Volumina\nVisualisierung: freeview -v input.nii.gz -v seg_right.mgz:colormap=lut:lut=path/to/AllenAtlasLUT\nTroubleshooting:\n- Stelle sicher, dass Lookup Table (AllenAtlasLUT) korrekt verlinkt ist\n- Bei Performanceproblemen: viel Zeit einplanen (v. a. bei 0.25 mm Auflösung)\n\n\nSynthSeg (Mauri et al.) Claustrum Segmentierung\n\nVoraussetzungen:\n- Python 3.8 Conda-Umgebung (z. B. synthseg_env)\n- FreeSurfer-Modul geladen (FREESURFER_HOME optional, aber hilfreich)\n- model.h5 vorhanden in claustrum_segmentation/model/\nInput: hochaufgelöste T1-Datei (z. B. T1.mgz oder .nii.gz)\nBefehl:\ncsh /home/jovyan/SynthSeg/claustrum_segmentation/mri_claustrum_seg \\\n--i /path/to/input.nii.gz \\\n--o /path/to/output_folder \\\n--model /path/to/model.h5 \\\n--threads 32 \\\n--qc \\\n--surf\nOutput:\n- `claustrum.lh.nii.gz`, `claustrum.rh.nii.gz`\n- Optional: .mgz-Konvertierung via mri_convert\nTroubleshooting:\n- ![:x:](https://a.slack-edge.com/production-standard-emoji-assets/14.0/google-medium/274c.png) model.h5 zu klein? $\\to$ Neu herunterladen\n- ![:x:](https://a.slack-edge.com/production-standard-emoji-assets/14.0/google-medium/274c.png) Kein seg.lh.mgz? $\\to$ Crash durch fehlende LUT oder unvollständige Ausführung\n- ![:weißes_häkchen:](https://a.slack-edge.com/production-standard-emoji-assets/14.0/google-medium/2705.png) Vorher export FREESURFER_HOME=/path/to/freesurfer_module setzen\n- ![:x:](https://a.slack-edge.com/production-standard-emoji-assets/14.0/google-medium/274c.png) TEMP-Ordner leer? $\\to$ Python-Fehler oder Pfadproblem\n- Optional: Denoising vor Segmentierung vermeiden (Registrierungsfehler möglich)\n\n\nMezerLab Claustrum Segmentierung\n\n\nModule laden (im LX-Terminal)\n\nmodule load matlab/2024b\\\nmodule load spm12/r7771\\\nmodule load fsl/6.0.7.16\\\nmodule load mrtrix3/3.0.4\n\nT1-Scan vorbereiten\n\nmri_convert /home/jovyan/matlab/T1.mgz /home/jovyan/matlab/T1.nii.gz \\\nbet /home/jovyan/matlab/T1.nii.gz /home/jovyan/matlab/T1_brain.nii.gz \\\nfslreorient2std /home/jovyan/matlab/T1_brain.nii.gz /home/jovyan/matlab/T1_brain_std.nii.gz\n\n5TT-Datei erzeugen (voll aufgelöst)\n\n\n5ttgen fsl /home/jovyan/matlab/T1_brain_std.nii.gz /home/jovyan/matlab/T1_5tt.nii.gz\nFalls T1_5tt.nii.gz zu niedrige Auflösung hat \\(\\to\\) Erneut ausführen mit nicht zugeschnittenem Input: 5ttgen fsl /home/jovyan/matlab/T1.nii.gz /home/jovyan/matlab/T1_5tt_fixed.nii.gz\n\n\nFSL FIRST-Segmentierung\n\nrun_first_all -i /home/jovyan/matlab/T1_brain_std.nii.gz -o /home/jovyan/matlab/first_output \\\ncp /home/jovyan/matlab/first_output_all_fast_firstseg.nii.gz /home/jovyan/matlab/first_seg.nii.gz\n\nMATLAB starten: matlab\nPfade in MATLAB setzen\n\naddpath(genpath('/home/jovyan/matlab/vistasoft')); \\\naddpath(genpath('/home/jovyan/matlab/ClaustrumSegmentation')); \\\naddpath(genpath('/home/jovyan/matlab/spm12')); \\\nspm('defaults', 'fmri'); \\\nspm_jobman('initcfg');\n\nSegmentierung ausführen\n\nt1wFile = '/home/jovyan/matlab/T1_brain_std.nii.gz'; \\\ntt5File = '/home/jovyan/matlab/T1_5tt_fixed.nii.gz'; % Verwende fixierte Datei \\\nfslSegFile = '/home/jovyan/matlab/first_seg.nii.gz'; \\\n\nsegment_claustrum_rh(t1wFile, tt5File, fslSegFile, '/home/jovyan/matlab/claustrum_output/rh_claustrum.nii.gz'); \\\nsegment_claustrum_lh(t1wFile, tt5File, fslSegFile, '/home/jovyan/matlab/claustrum_output/lh_claustrum.nii.gz');\nTroubleshooting (kurz & präzise)\n\n\n\n\n\n\n\n\nProblem\nUrsache\nLösung\n\n\n\n\nKein lh_claustrum.nii.gz\nFalsches oder leeres tt5File\nNutze T1_5tt_fixed.nii.gz\n\n\nkmeans: X must have more rows than clusters\ncandidate enthält &lt;2 Werte\nTT5 zu grob / CSF-Filter zu streng → TT5 neu erstellen\n\n\ncsfMask(find(tmp)) = 0; Fehler\nTT5-Auflösung unpassend\nMit flirt resamplen oder TT5 neu erzeugen\n\n\nFIRST-Output fehlt\nrun_first_all nicht korrekt ausgeführt\n-i und -o prüfen\n\n\nbet oder run_first_all nicht gefunden\nModul nicht geladen\nmodule load fsl/...\n\n\nAuflösungsfehler\nBildgrößen ungleich\nAlle Inputs auf 1×1×1 mm prüfen (z. B. mit mrinfo, fslhd)\n\n\n\nOptional:Visualisierung in FreeSurfer\nmri_convert /home/jovyan/matlab/claustrum_output/rh_claustrum.nii.gz /home/jovyan/matlab/claustrum_output/rh_claustrum.mgz \\\nmri_convert /home/jovyan/matlab/claustrum_output/lh_claustrum.nii.gz /home/jovyan/matlab/claustrum_output/lh_claustrum.mgz \\\n\nfreeview -v /home/jovyan/matlab/T1.mgz \\\n-v /home/jovyan/matlab/claustrum_output/rh_claustrum.mgz \\\n-v /home/jovyan/matlab/claustrum_output/lh_claustrum.mgz\n\n\nClaustrum Segmentierung nach Li et al.\n\n\nResampling\n\nPython-Skript aus dem GitHub-Repository nutzen (resample.py)\nEingabepfad, Zielauflösung (1 mm³), und Output-Pfad anpassen\n\nSkull-Stripping\n\nMit FSL-BET\n\n\nB.: bet2 input_resampled.nii.gz output_ss.nii.gz -f 0.5 -g 0 -m\n\n\nDenoising\n\nANTs DenoiseImage verwenden\n\nB.: DenoiseImage -d 3 -i output_ss.nii.gz -o output_ss_denoised.nii.gz\n\n\nClaustrum-Segmentierung mit Multi-View-Netz\n\nGitHub Repository claustrum demo klonen (im read me)\nVerzeichnisstruktur beibehalten (z. B. data/test/subjectID/subjectID_T1w_denoised.nii.gz)\nModelle in models/ einfügen (axial und coronal)\nIn Conda-Umgebung mit Python 3.8 und TensorFlow 2.3–2.5 arbeiten (außedem mit keras, Simple Itk, scipy, numpy, PIL)\nAlle Abhängigkeiten mit pip install -r requirements.txt installieren\nAusführen mit: python test_two_views.py\n\nOutput: Segmentierungsmaske wird in output/ gespeichert.\n\n\n\n\n Back to top",
>>>>>>> 4f1eb82d34caf3aafd572958364906347c3d1654
    "crumbs": [
      "Home",
      "Internship",
      "Internship project - Automatic Claustrum Segmentation"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Visual Neuroscience lab",
    "section": "",
    "text": "About\n\nWe investigate how subjective experience arises from sensory input and how this is represented in the brain.\nPI: Natalia Zaretskaya\n\n\n\n\nRecent Changes\n\n\n\n\n\n\n\n\n\n\nMNI space\n\n\ninformation about various MNI spaces and templates\n\n\n\nNZ\n\n\nMar 7, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nproject funding opportunities\n\n\nOverview of programs to fund your projects\n\n\n\nCY, MG\n\n\nJul 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeurodesktop access\n\n\nHow to use Neurodesktop on the server\n\n\n\nNZ, MG\n\n\nJul 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n00 - Overview\n\n\nOverview about how to get from DICOM files from the Scanner to your GLM Analysis\n\n\n\nMG\n\n\nJul 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n09- second level GLM\n\n\nExample of second level GLM\n\n\n\nMG\n\n\nJul 28, 2025\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "dicom_to_glm/troubleshooting.html",
    "href": "dicom_to_glm/troubleshooting.html",
    "title": "Troubleshooting",
    "section": "",
    "text": "This page is dedicated for Troubleshooting. Known errors and problems will be listed here with a possible solution for solving this issue. If you solved a problem that might be relevant for others as well, feel free to contact Maximilian (,Adam or Natalia) so that it can be added to this page as well.",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "dicom_to_glm/troubleshooting.html#i-ran-fmriprep-but-no-distortion-correction-happened",
    "href": "dicom_to_glm/troubleshooting.html#i-ran-fmriprep-but-no-distortion-correction-happened",
    "title": "Troubleshooting",
    "section": "2.1 I ran fMRIprep but no distortion correction happened",
    "text": "2.1 I ran fMRIprep but no distortion correction happened\n\n2.1.1 Detection of this Problem\n\n2.1.1.1 Error Message\nThis problem might occur with a corresponding error message (and the fMRIprep command stopping)\n\n\n2.1.1.2 Silent error\nIt can be hard to realize this issue as the output might not be visible anymore, or the problem “occurs silently” (no error message are shown at all).\nIf the distortion correction was successful, the subject’s .html should include a section \\(B_0\\) field mapping. As you can see in the image below, this would be shown at the top of the file where the different sections are listed as well:\nIf the distortion correction failed silently (thus, without an explicit error that is printed somewhere), the section is missing as whole and is not shown at the top of the .html file:\n\n\n\n2.1.2 Solution\n\nyou might be using a wrong BIDS specification in the fmap’s json file. See Information about BIDS uri, or the corresponding wiki entry.\nIf the content of the fmap’s .json file uses a different naming convention than necessary for your version of fMRIprep, no distortion correction is applie\n\n\\(\\to\\) Follow this wiki entry to check and update the entries of the fmap’s .json file",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "dicom_to_glm/troubleshooting.html#valueerror-reference-passed-is-not-aligned-with-spline-grids",
    "href": "dicom_to_glm/troubleshooting.html#valueerror-reference-passed-is-not-aligned-with-spline-grids",
    "title": "Troubleshooting",
    "section": "2.2 ValueError: Reference passed is not aligned with spline grids",
    "text": "2.2 ValueError: Reference passed is not aligned with spline grids\nThis error sometimes occurs by chance, but should not appear regularly. It is caused by the coregistration algorithm of fmriprep itself. There are two options you can try:\n\nTry a different version of fMRIprep\nTransform your T1w scan\n\n\n2.2.1 Transform your T1w scan\nTransforming your T1w scan to avoid this error is not the ideal solution, but it is a good workaround for this complex problem.\nTo transform your T1w scan:\n\nLoad your structural scan in freeview (identical process as described here for a functional scan)\nIn the menu-bar in freeview (top-left), select Tools &gt; Transform Volume...\nIn the resulting window, select Transform in the top menu and change the value in the second “line” (= Y (P-A)) to -2.5 (with the slider or enter the number directly). This will shift the T1w scan 2.5mm in the posterior direction. Subsequently, select Save Volume As\nYou can simply overwrite the existing T1w file which should already be selected (you could always reverse this step with the described method). Make sure to select the option Do not resample voxel data when saving (only update header)!\n\n\\(\\to\\) when you run freesurfer with this transformed T1w scan, the “Spline Grid Error” should not occur anymore.",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "dicom_to_glm/troubleshooting.html#if-using-already-run-freesurfer-recon-all-output-and-have-specified-an-expert-opts-file-then-fmriprep-shows-multiple-errors",
    "href": "dicom_to_glm/troubleshooting.html#if-using-already-run-freesurfer-recon-all-output-and-have-specified-an-expert-opts-file-then-fmriprep-shows-multiple-errors",
    "title": "Troubleshooting",
    "section": "2.3 If using already run FreeSurfer recon-all output and have specified an expert-opts file then fMRIprep shows multiple errors",
    "text": "2.3 If using already run FreeSurfer recon-all output and have specified an expert-opts file then fMRIprep shows multiple errors\n\\(\\to\\) In the directory that has the prerun recon-all there is a folder scripts that saves the expert-opts file. Removing this file allows fMRIprep to run",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "dicom_to_glm/troubleshooting.html#i-ran-fmriprep-and-the-distortion-correction-of-the-data-is-too-extreme",
    "href": "dicom_to_glm/troubleshooting.html#i-ran-fmriprep-and-the-distortion-correction-of-the-data-is-too-extreme",
    "title": "Troubleshooting",
    "section": "2.4 I ran fMRIprep and the distortion correction of the data is too extreme",
    "text": "2.4 I ran fMRIprep and the distortion correction of the data is too extreme\n\n\n\n\nAn example of poor distortion correction\n\n\n\n\nThis is an issue with certain issues of fMRIprep. Using fMRIprep version 20.2.7 may be needed to avoid the extreme distortion correction (newer versions of fMRIprep haven’t been tested)",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "dicom_to_glm/troubleshooting.html#fmriprep-fails-silently-i.e.-gives-no-error-on-coregistering-the-functional-to-the-structural-scan",
    "href": "dicom_to_glm/troubleshooting.html#fmriprep-fails-silently-i.e.-gives-no-error-on-coregistering-the-functional-to-the-structural-scan",
    "title": "Troubleshooting",
    "section": "2.5 fMRIprep fails silently (i.e. gives no error) on coregistering the functional to the structural scan",
    "text": "2.5 fMRIprep fails silently (i.e. gives no error) on coregistering the functional to the structural scan\n\\(\\to\\) the solution is (if your structural and functional scans are from the same session) to add the following flag:\n`--bold2t1w-init header`",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "dicom_to_glm/run_fmriprep.html",
    "href": "dicom_to_glm/run_fmriprep.html",
    "title": "06 - fMRIprep",
    "section": "",
    "text": "fMRIprep is a helpful and stardardized preprocessing tool/pipeline for BIDS-valid datsets. By combining various neuroimaging software packages (FSL, ANTs, FreeSurfer,…) it allows us to run various preprocessing steps automatically:",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "06 - fMRIprep"
    ]
  },
  {
    "objectID": "dicom_to_glm/run_fmriprep.html#sec-fmriprepStart",
    "href": "dicom_to_glm/run_fmriprep.html#sec-fmriprepStart",
    "title": "06 - fMRIprep",
    "section": "2.1 Starting fMRIprep",
    "text": "2.1 Starting fMRIprep\nTo start fMRIprep, click on the bird symbol at the bottom left on neurodesk, hover neurodesk, then All applications and navigate to fmriprep. Select the version that you want to use \nAfter selecting your version, a new bash window opens and fMRIprep loads. This can take a few moments, be patient! When everything is ready, the bottom line should state fmriprep-\"Version\" (with “Version” being whatever version you selected, for example fmriprep-24.1.1)",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "06 - fMRIprep"
    ]
  },
  {
    "objectID": "dicom_to_glm/run_fmriprep.html#sec-fmriprepSetup",
    "href": "dicom_to_glm/run_fmriprep.html#sec-fmriprepSetup",
    "title": "06 - fMRIprep",
    "section": "2.2 Setting up necessary directories and defining “SUBJECTS_DIR”",
    "text": "2.2 Setting up necessary directories and defining “SUBJECTS_DIR”\nFirst, you need to create some new directories that are necessary for fMRIprep to run properly. For this, enter the following command (adapt the path(s)!)\nmkdir -p /home/jovyan/website/bids/derivatives/fmriprep/sourcedata/freesurfer/\nBy confirming your command with Enter, you´ll find the new directory structure within you derivatives directory\nThen, we need to specify where fMRIprep should store the resulting files with this command (again, adapt the path(s) if necessary):\nSUBJECTS_DIR=/home/jovyan/website/bids/derivatives/fmriprep/sourcedata/freesurfer/\n\n\n\nSet SUBJECTS_DIR\n\n\nHint: you can copy/paste both commands from above and execute them by pressing enter simultaneously, it doesn´t matter",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "06 - fMRIprep"
    ]
  },
  {
    "objectID": "dicom_to_glm/run_fmriprep.html#sec-fmriprepCommand",
    "href": "dicom_to_glm/run_fmriprep.html#sec-fmriprepCommand",
    "title": "06 - fMRIprep",
    "section": "2.3 fmriprep command",
    "text": "2.3 fmriprep command\nWith the setup done, you can now run the command (for more information about the commands/flag, please refer to the fMRIprep documentation:\nfmriprep /home/jovyan/website/bids /home/jovyan/website/bids/derivatives/fmriprep participant --bold2t1w-init header --force-bbr -w /home/jovyan/website/fmriprep_tmp --output-spaces T1w fsaverage fsnative --participant-label sub-999 --nprocs 8 --mem 10000 --skip_bids_validation --stop-on-first-crash -v --fs-license-file /home/jovyan/website/freesurferlicense/license.txt\nImportant: adapt the flag --participant-label sub-999 accordingly, so that you state the subject you want to preprocess\nHint: same as before, you could run this command with the two commands above for the setup as well. The only important thing is the order: first the command for making the directories, then defining SUBJECTS_DIRand finally the fMRIprep command.",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "06 - fMRIprep"
    ]
  },
  {
    "objectID": "dicom_to_glm/run_fmriprep.html#waiting",
    "href": "dicom_to_glm/run_fmriprep.html#waiting",
    "title": "06 - fMRIprep",
    "section": "2.4 Waiting",
    "text": "2.4 Waiting\nAfter fMRIprep started, it´s probably time to get a coffee and do something else. This process will take quite some time (hours), you don´t have to stare at the output the whole time. You can also close the Tab where you opened neurodesk! Even if you close it, the program continues its work as your server keeps running!",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "06 - fMRIprep"
    ]
  },
  {
    "objectID": "dicom_to_glm/RemoveNoiseScan.html",
    "href": "dicom_to_glm/RemoveNoiseScan.html",
    "title": "05 - Removing Noise Scan",
    "section": "",
    "text": "For this step, you need to already have nifti files! If you currently only have DICOM files, please convert them to nifti’s first!\nDepending on the sequence used in the scanner session, you might have data that were acquired with a denoising sequence. This is sometimes done, since it doesn´t take much time in the scanner, but you have the ability to denoise the data if it turns out to be necessary.\nBut by default, we don´t use this denoising information. Additionally, some applications (like fmriprep) cannot process data with this additional information!\nThus, before running our default pipeline with fmriprep, we have to remove this information from our scans!",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "05 - Removing Noise Scan"
    ]
  },
  {
    "objectID": "dicom_to_glm/RemoveNoiseScan.html#sec-OpenFreeview",
    "href": "dicom_to_glm/RemoveNoiseScan.html#sec-OpenFreeview",
    "title": "05 - Removing Noise Scan",
    "section": "1.1 Open freeview",
    "text": "1.1 Open freeview\nYou can check if the denoising sequence was used in for your data, by checking the functional scans in freeview. For this, you first have to source freesurfer and then open freeview:\n\nOpen a terminal (Monitor Icon at the bottom left on neurodesk) \nType ml freesurfer and press enter\nType freeview and press enter (from time to time, it can take a while until freeview opens, be patient! It is also possible, that some messages about runtime are printed in the terminal, but that is nothing to worry about!) \n\n\\(\\to\\) When freeview opened successfully, it should look like this:",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "05 - Removing Noise Scan"
    ]
  },
  {
    "objectID": "dicom_to_glm/RemoveNoiseScan.html#sec-loadVolume",
    "href": "dicom_to_glm/RemoveNoiseScan.html#sec-loadVolume",
    "title": "05 - Removing Noise Scan",
    "section": "1.2 Load a functional volume",
    "text": "1.2 Load a functional volume\nNow you have to look at a functional scan to see if there is a denoising scan at the end.\nFor this, select File &gt; Load Volume (top left corner in freeview): \nA new window opens, asking you to Select volume file. Either type in the path(s) to your/a functional scan manually, or start by clicking on the folder symbol \nNavigate to your func directory in your BIDS, select one of the nifti files of one of the runs and press Open afterwards \nNow, with the field Select volume file specified with the path(s) to the nifti file, you can click Open\nWhen the volume loaded, it should see some kind of data and something that looks like a brain:",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "05 - Removing Noise Scan"
    ]
  },
  {
    "objectID": "dicom_to_glm/RemoveNoiseScan.html#sec-CheckDenoising",
    "href": "dicom_to_glm/RemoveNoiseScan.html#sec-CheckDenoising",
    "title": "05 - Removing Noise Scan",
    "section": "1.3 Check for noise Scan",
    "text": "1.3 Check for noise Scan\nTo check if a noise scan is included in you data, use the slider next to Frames (left side, roughly in the middle), to navigate to the last frame. If a denoising sequence was used during data acquisition in the scanner, this frame looks “empty”. Toggle between the last and second to last frame, to visualize the difference even more \nIf you confirmed a noise-scan in one functional run, they will probably be there for all your runs. It might be good to check every run anyway.",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "05 - Removing Noise Scan"
    ]
  },
  {
    "objectID": "dicom_to_glm/Overview_fromDICOMtoGLM.html",
    "href": "dicom_to_glm/Overview_fromDICOMtoGLM.html",
    "title": "00 - Overview",
    "section": "",
    "text": "This page gives an overview and brief information about the necessary steps to get from the DICOM files to a GLM Analysis. You can find more detailed information by navigating to the sub-pages by clicking on the caption of the respective point.\n\n\n\nRegular scanning time:\n\nyou get the data as nifti files\na few days later via mri-dropoff in the J drive\nstep 1 and 4 generally are already done for you then (but check the defacing anyway, sometimes this doesn´t work!)\n\nScanning with Natalia\n\nwe have the raw DICOM files\nconversion to niftis has to be done (see 1)\ndefacing has to be done (see 4)\nIMPORTANT: Load the DICOM files to the server and delete them from any external/local device (USB-Stick, local storage of your computer, etc.) as soon as possible!!!",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "00 - Overview"
    ]
  },
  {
    "objectID": "dicom_to_glm/Overview_fromDICOMtoGLM.html#two-ways-to-your-data",
    "href": "dicom_to_glm/Overview_fromDICOMtoGLM.html#two-ways-to-your-data",
    "title": "00 - Overview",
    "section": "",
    "text": "Regular scanning time:\n\nyou get the data as nifti files\na few days later via mri-dropoff in the J drive\nstep 1 and 4 generally are already done for you then (but check the defacing anyway, sometimes this doesn´t work!)\n\nScanning with Natalia\n\nwe have the raw DICOM files\nconversion to niftis has to be done (see 1)\ndefacing has to be done (see 4)\nIMPORTANT: Load the DICOM files to the server and delete them from any external/local device (USB-Stick, local storage of your computer, etc.) as soon as possible!!!",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "00 - Overview"
    ]
  },
  {
    "objectID": "dicom_to_glm/DICOMtoBIDS.html",
    "href": "dicom_to_glm/DICOMtoBIDS.html",
    "title": "01 - DICOM to BIDS",
    "section": "",
    "text": "the raw files from the scanner are DICOM-Files\nnot really useful as we need .nifti files for our analysis \n\n\\(\\qquad\\to\\) first step is to transform the DICOM- to nifti-files",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "01 - DICOM to BIDS"
    ]
  },
  {
    "objectID": "dicom_to_glm/DICOMtoBIDS.html#about-bids",
    "href": "dicom_to_glm/DICOMtoBIDS.html#about-bids",
    "title": "01 - DICOM to BIDS",
    "section": "2.1 About BIDS",
    "text": "2.1 About BIDS\nApart from a clear structure that is understandable (for other researchers as well) and standardized, the BIDS format offers further advantages. For example, various BIDS-Apps can perform different steps automatically (e.g. fmriprep), but they require the data to be BIDS-valid).",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "01 - DICOM to BIDS"
    ]
  },
  {
    "objectID": "dicom_to_glm/DICOMtoBIDS.html#bidscoin",
    "href": "dicom_to_glm/DICOMtoBIDS.html#bidscoin",
    "title": "01 - DICOM to BIDS",
    "section": "3.1 bidscoin",
    "text": "3.1 bidscoin\nFirst, you need to start through the bird-icon on the bottom left of neurodesk, navigate to Neurodesk &gt; All Applications &gt; bidscoin and select a version (the process has been tested with v4.6.0 & v4.6.1)",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "01 - DICOM to BIDS"
    ]
  },
  {
    "objectID": "dicom_to_glm/DICOMtoBIDS.html#shell-commands",
    "href": "dicom_to_glm/DICOMtoBIDS.html#shell-commands",
    "title": "01 - DICOM to BIDS",
    "section": "3.2 Shell commands",
    "text": "3.2 Shell commands\nAfter some time to load, it should look like this:\n\nThen you can follow this script (running the lines separately can help you find/fix potential problems! For potential problems also see the subsections below!):\n#!/bin/bash\n\n# you can copypaste the following commands into the terminal window that is opened by BIDSCOINER\n\n# define directories\nsourceFolder=/home/jovyan/completion2/dcm\nsortedFolder=/home/jovyan/completion2/dcm_sorted\n\n# check the output of the source folder\nls -l $sourceFolder\n\n# make the target directory with subdirectories\nmkdir -p $sortedFolder\n\n# change to the script folder\ncd /home/jovyan/dicomsort/\n\n# run the script\n./DICOMsort.sh $sourceFolder $sortedFolder\n\n# check the output of the target folder\nls -l $sortedFolder\n\n# run bidsmapper\nbidsmapper $sortedFolder /home/jovyan/completion2/bids\n# note: in the intendedFor of the fieldmap, add func/sub-001_ses-1_xxx_bold.*\n\n# once you are happy with the mapping, run bidscoiner\nbidscoiner $sortedFolder /home/jovyan/completion2/bids",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "01 - DICOM to BIDS"
    ]
  },
  {
    "objectID": "dicom_to_glm/DICOMtoBIDS.html#bismapper",
    "href": "dicom_to_glm/DICOMtoBIDS.html#bismapper",
    "title": "01 - DICOM to BIDS",
    "section": "3.3 bismapper",
    "text": "3.3 bismapper\n\nCurrently, to discover the data we need to delete the content of the “subprefix” field and then save the bids map in the default location. After this, run the same bidsmapper command again.\n\n3.3.1 Subject and session\n\nNow the bidsmapper discovered everything, but the files are not named correctly.\nEdit subject and session first.\nNow we need to rename the files by clicking “Edit” next to every file.\n\n\n3.3.2 Editing file names: anatomical\n\nanatmprage -&gt; mprage in the “acq” filed\n\n\n3.3.3 Editing file names: functional\n\nEdit the task name and acq field\nDo this for every functional file\n\n\n3.3.4 Editing file names: fieldmap\n\nKeep in mind, that we often need the option “epi” in the selection field of “suffix” (and not the “fieldmap” option)!\nIn the section “Metadata” you have to add which files should be selected for the “IntendedFor” field of the fieldmap. You can use a wildcard (*) to select all respective functional runs: “func/sub-001*_bold.*”. This includes all files in the directory “func” that start with “sub-001” and somewhere in the filename include “_bold.”. Watch out if you have more than one session! Then you might need to specify the respective session as well, e.g. “func/sub-001_ses-3*_bold.*”",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "01 - DICOM to BIDS"
    ]
  },
  {
    "objectID": "dicom_to_glm/DICOMtoBIDS.html#bidscoiner-after-renaming-all-files",
    "href": "dicom_to_glm/DICOMtoBIDS.html#bidscoiner-after-renaming-all-files",
    "title": "01 - DICOM to BIDS",
    "section": "3.4 bidscoiner after renaming all files",
    "text": "3.4 bidscoiner after renaming all files\nOnce you are happy with the naming of the files, run bidscoiner-command to do the actual conversion",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "01 - DICOM to BIDS"
    ]
  },
  {
    "objectID": "dicom_to_glm/DICOMtoBIDS.html#view-files",
    "href": "dicom_to_glm/DICOMtoBIDS.html#view-files",
    "title": "01 - DICOM to BIDS",
    "section": "3.5 View files",
    "text": "3.5 View files",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "01 - DICOM to BIDS"
    ]
  },
  {
    "objectID": "dicom_to_glm/DICOMtoBIDS.html#mricron",
    "href": "dicom_to_glm/DICOMtoBIDS.html#mricron",
    "title": "01 - DICOM to BIDS",
    "section": "3.6 MRICron",
    "text": "3.6 MRICron\n\nNow you can view the nifti files with any suitable software (Hint: In the image above, you see a T1w scan that is already defaced! If you convert your DICOMs to niftis, the T1w-scan will probably look differently and you have to deface before continuing!)",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "01 - DICOM to BIDS"
    ]
  },
  {
    "objectID": "dicom_to_glm/DICOMtoBIDS.html#preparations",
    "href": "dicom_to_glm/DICOMtoBIDS.html#preparations",
    "title": "01 - DICOM to BIDS",
    "section": "4.1 Preparations",
    "text": "4.1 Preparations\n\n4.1.1 Setting up the directory structure\nFirst, you need to set up the correct structure for the script to run properly later. For this, you:\n\nCreate an empty directory named bids (right-click, Create New... &gt; Folder)\nCreate a directory (name can be anything, for example dicoms) and copy/paste your folder with the DICOM-Files in this directory\n\nIt should look like this:\nThese two directories (bids & dicoms) need to be there for the script to work!\nThe directory scrips is not necessary for the script to run, but it may be helpful to gather and safe the scripts and notes that you need during the process from your raw files to the “final” analysis.\n\n\n4.1.2 Naming of the nifti files\nIt is curcial for the BIDS format, that the resulting nifti-files are named properly (see here for the naming convention(s)).\nTo ensure that later, it might be helpful to write down the proper names for the files before you start the process. This could be done in a simple .txt file, where you specify all the file-names. You can then just open this .txt file and copy/paste the file-names whenever you need them. To have everything at one place, you could save this file in your scripts folder as well.\n\n4.1.2.1 Create a naming.txt\n\nnavigate to your scripts folder, right-click, select Create New &gt; Empty File\nassign a name (in this case naming.txt)\nfill this text file with the proper, BIDS-conform names for the nifti files and saveNow, you can just open this file when you need to change the name of a nifti.",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "01 - DICOM to BIDS"
    ]
  },
  {
    "objectID": "dicom_to_glm/DICOMtoBIDS.html#sec-start",
    "href": "dicom_to_glm/DICOMtoBIDS.html#sec-start",
    "title": "01 - DICOM to BIDS",
    "section": "4.2 Starting the script",
    "text": "4.2 Starting the script\n\nTo start the script, you first need to know where the script dcm2bids.sh is. In this case it is here: /home/jovyan/Website/scripts/dcm2bids.sh (you can copy/paste this file to your directory from /shared/website/dcm2bids.sh, or directly access) \nNavigate to the location of the script, right-click it, and select Copy Path(s) \nOpen a Terminal (icon of a monitor at the bottom left of your screen):\nType ml freesurfer and press enter\nType bash, add a space, then right-click and select Paste to fill in the path to the script (or type in the path manually)Press enter to start the script.",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "01 - DICOM to BIDS"
    ]
  },
  {
    "objectID": "dicom_to_glm/DICOMtoBIDS.html#installing-all-necessary-packages",
    "href": "dicom_to_glm/DICOMtoBIDS.html#installing-all-necessary-packages",
    "title": "01 - DICOM to BIDS",
    "section": "4.3 (Installing all necessary packages)",
    "text": "4.3 (Installing all necessary packages)\nIt is possible (especially if you start the script for the first time), that some necessary packages are not installed. If this is the case, the script will tell you! If something is missing you´ll be asked, if you want to install these packages. By typing in y and pressing enter, the package will get installed (you might have to do this for more than one package):\nIf there is still a missing package after all “automatic” suggestions of packages to install, it is probably related to dcm2bids_scaffold (but again, the script will “show” you that there is something missing). In this case, type pip install dcm2bids and press enter\nNow “restart” the script (see 2.d + 2.e):",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "01 - DICOM to BIDS"
    ]
  },
  {
    "objectID": "dicom_to_glm/DICOMtoBIDS.html#sec-preparation",
    "href": "dicom_to_glm/DICOMtoBIDS.html#sec-preparation",
    "title": "01 - DICOM to BIDS",
    "section": "4.4 Preparation: Provide Input and Output Directory",
    "text": "4.4 Preparation: Provide Input and Output Directory\nWith all necessary packages installed, you are asked to provide the path to the nifti directory. It should look like this:\nNifti directory means the directory/folder, where the nifti-files should be saved to. This is our (currently empty) bids folder that we created in the beginning.\n\nTo indicate the nifti directory, we either type in the path manually, or get the path by navigating to the directory, right-clicking the bids directory and selecting Copy Path(s) (same procedure as when we needed the path of the script)\ngo back to the terminal window where dcm2bids is running\nright-click and select Paste to enter the path\npress enter\n\nSubsequently, you are asked to indicate the dicom directory.\n\\(\\to\\) Repeat the steps a)-d) for the dicom directory where your dicoms are stored (that we also created in the beginning)\nWhen both paths are set, you are asked to choose a dicom name (basically the files of a participant; in this case, there is only one scanning-session, but if you have more than one subject, you can select the one that you want do process) and it should look like this:\nAfter selecting a subject/dicom name (Press enter), the script automatically suggest a BIDS “subject id” and asks if it is correct (Watch out! Generally the suggested ID is not BIDS-conform!)\nFor your data to be BIDS-conform at the end, the “subject id” has to have a structure like this: sub-001, sub-002,… (All necessary information about BIDS can be found here)\nIf the suggested name is correct, type in y and press enter.\nIf it is not correct, type in n, press enter and type in a/the correct subject id Once again, press enter\nAs a last preparative step, you are asked, if you would like to change the session folder from ses-1 to another session. If you only have one session, type n and press enter\nIf you want to change the session, type y and provide the name of the sessions (Important: to have/get a BIDS-conform structure at the end, subdirectories for sessions have to be named ses-x with x being a number)\nSubsequently, you get a short reminder in which directories your files should be stored.\nBy pressing enter, the script starts to convert your dicom files to nifti’s. This can take a few seconds - be patient!",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "01 - DICOM to BIDS"
    ]
  },
  {
    "objectID": "dicom_to_glm/DICOMtoBIDS.html#assigning-niftis-to-directories-and-adjusting-file-names",
    "href": "dicom_to_glm/DICOMtoBIDS.html#assigning-niftis-to-directories-and-adjusting-file-names",
    "title": "01 - DICOM to BIDS",
    "section": "4.5 Assigning nifti’s to directories and adjusting file names",
    "text": "4.5 Assigning nifti’s to directories and adjusting file names\nAfter the script converts the dicom’s to nifti’s, we have to potentially correct automatically assigned names and assign a directory. At the top of the terminal, you always see how many dicoms are left to categorize.\n\n4.5.1 Possible Actions to perform for a dicom\n\n\n\nActions for a dicom\n\n\nYou can select one of three options for any dicom file (use up-/down arrow key to switch between the options and press enter to confirm your choice):\n\nKeep: By selecting this option, you tell the script that the suggested name is correct and that you want to assign this file with the shown name to a directory (Since BIDS uses a specific naming convention, you´ll probably need this option only for “additional files”, like files related to denoising, that will be placed in a directory that is more or less independent of the BIDS-Format; see Section 4.5.3*)\nModify Name: This option tells the script that you want to change the suggested name. This is needed, when the suggested name is not BIDS-conform. After selecting this option (by pressing enter), you are asked to edit the name. Change the name (first delete the suggested one) to a BIDS-conform filename (Hint: the initially suggested name implies what type of scan it was, so you should be able to easily determine if it was an anatomical scan, which run of a functional scan, or the fieldmap). Using a previously created file with bids-conform names can help avoiding errors and typos here, by just copy/pasting the prepared filenames! After you changed the name, press enter to confirm\nDelete (Do not convert): As the name suggests, selecting this option deletes the mentioned file (thus, there will be no corresponding nifti-file!). One case when this is generally used is for the anat-scout scan that is irrelevant for the preprocessing and analysis, since it is a short scan at the beginning of a scanning session to ensure a proper position of the participant and to adjust the parameters for the actual scans.\n\n\n\n4.5.2 Assigning a file to a directory\nAfter either keeping or modifying the name, you are asked to assign the file to a directory.\nYou can either:\n\nKeep folder name: By selecting this option, you accept the suggested directory. In general, the correct directory is already suggested for most files (anatomical \\(\\to\\) anat, functional \\(\\to\\) func, fieldmap \\(\\to\\) fmap)\nModify folder name: This option is only needed for files that were acquired “additionally”, for example, when a denoising sequence was used (files with .ph at the end, see here). As for the file name, you can edit the name of the directory after selecting this option. Entering a new/unique name for the directory here will result in an additional sub-directory in the subsequent BIDS-directory (if you enter the same name for all “additional” files, they will all be stored in this specified directory)\n\n\n\n4.5.3 The “extra” directory\nSometimes we acquire files that are not part of the BIDS-Format (and, more importantly, are not used by BIDS-Apps), for example when running a denoising sequence during acquisition of the data. In the example of a denoising sequence, corresponding files are named with _ph at the end. When asked about the filename, we can just keep it as it is, since we use these files separately anyway.\nImportant!!\nThose files have to be stored in a different directory! Thus, when asked about where to assign the file, we select Modify folder name. All “additional” files can be saved in a directory named extra (\\(\\to\\) edit the name of the directory to extra and press enter)\nAfter assigning the last file to its directory, you´ll see the information, that an additional file has been created, which we will use later. The script has finished:",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "01 - DICOM to BIDS"
    ]
  },
  {
    "objectID": "dicom_to_glm/check_IntendedFor_field.html",
    "href": "dicom_to_glm/check_IntendedFor_field.html",
    "title": "02 - Check IntendedFor field of the fmap’s JSON",
    "section": "",
    "text": "With changing Versions of BIDS and BIDS-Apps, slight changes occur in how and where you have to include specific information. One example for this is the IntendedFor information in the fmap’s .json file. It tells fmriPrep for which (functional) data the distortion correction should be applied to.\nDepending on the conversion method you used to convert your DICOM to nifti files, the contents will look differently.\nAdditionally, depending on the version of fMRIprep you plan to use for preprocessing, you need different naming conventions of the entries.\nIn general, there are two naming conventions that might be used in the IntendedFor field:\nBIDS uri spec necessary for fmriprep 24.0.0 or newer:\n\n``` json\n\"IntendedFor\": [\n    \"bids::sub-1181001/ses-1/func/sub-1181001_ses-1_task-movie_run-1_bold.nii.gz\",\n    \"bids::sub-1181001/ses-1/func/sub-1181001_ses-1_task-movie_run-2_bold.nii.gz\", \n    etc\n    ]\n```\n\nOlder fmap spec necessary for fmriprep older than 24.0.0:\n\n``` json\n\"IntendedFor\": [\n    \"ses-1/func/sub-1181001_ses-1_task-movie_run-1_bold.nii.gz\",\n    \"ses-1/func/sub-1181001_ses-1_task-movie_run-2_bold.nii.gz\",\n    etc\n    ]\n```\nDue to these differences and potential resulting problems (e.g. no distortion correction is applied), you should always check the content of the fmap’s “IntendedFor” field before running fMRIprep.\n\n1 Check the content of the JSON file\nNavigate to the fmap directory of a/each subject (*/bids/sub-999/ses-1/fmap/) and open the .json file\nScroll down to the very end of it. There should be a section that starts with IntendedFor and looks like this:\nIn General, all entries should follow the same naming convention (either the new or the old one, see above). If the naming convention corresponds to the fMRIprep version you want to use, you don´t have to change anything. If it does not correspond to the naming convention necessary for your version, you can follow the steps below to update the file.\nAdditionally, it may occur, that additional files are included in this field (e.g. files named “*part-phase*”) where no distortion correction should be applied to. Even if the naming convention is correct for your planned version of fmriprep, your should remove the entries of these files (see below)\n\n\n2 Update “IntendedFor” field of the fmap’s JSON file\nThere is a dedicated script to update the contents of the fmap’s .json file. It can be found at /shared/website/update_json.py. Of course, you can do the changes manually as well.\nTo start and use the script:\n\nOpen a terminal \nType python /shared/update_json.py\nIndicate if you want to change the naming convention (independent of the current version, the old naming convention is changed to the new one and vice versa if you select 1 (=Yes) here)\nIndicate if you want to remove entries from the field\nIf you want to remove entries, indicate the keywords that identify an entry to be removed\nProvide the path to your bids directory\n\nAfter providing all the inputs, the indicated changes will be performed and saved in the file automatically. The original .json file will be saved as filename_orig.json (if the original entries are needed later again). Additionally, the changes are only applied, if a fmap-directory with a .json file doesn´t already have a _orig.json.\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "02 - Check IntendedFor field of the fmap's JSON"
    ]
  },
  {
    "objectID": "behavioral/chinrests/chinrests.html",
    "href": "behavioral/chinrests/chinrests.html",
    "title": "Custom-built chin rests",
    "section": "",
    "text": "If you need a solid chinrest for your behavioral, EEG or brain stimulation experiment, we can create one ourselves using 3D printing, some standard parts and this tutorial\nhttps://www.thingiverse.com/thing:2968729#google_vignette\nwith some adaptations/improvements. If you need a chinrest, talk to Lukas. Note that it may take a few weeks since some parts need to be orders and or/printed.\nThe result will look something like this:\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Behavioral",
      "Custom-built chin rests"
    ]
  },
  {
    "objectID": "behavioral/2024-05-22/index.html",
    "href": "behavioral/2024-05-22/index.html",
    "title": "Vision tests",
    "section": "",
    "text": "Before starting any experiment with a subject, confirm that they have “normal vision” since it is a critical exclusion criteria for most of the studies in vision science.\n\n\nAccording to the World Heath Organization (WHO), “visual acuity is … a simple, non-invasive measure and critical to determine the presence of vision impairment” [1]. The vision impairment is an eye condition which causes imperfections in the function of visual system [2]. We use visual acuity tests to quantify how acurately the visual system perceives the outer world.\nIn our lab, we perform FrACT10 to measure the visual acuity. The subjects having mild impairment or worse are excluded based on the scores of FrACT10. The threshold for exclusion is 0.40 logMAR or 0.4 decVA [3].\n\n\n\nStereopsis is also an important phenomenon for binocular rivalry since it is based on binocular disparity [4]. Therefore, the subjects who has a stereoacuity less than 30arcsec are considered as having poor stereovision and mostly excluded from binocular rivalry experiments [5]. We use Random Dot V test with red cyan anaglyph glasses to quantify the stereopsis.\nThe difficulty of the test increases while the number on the test window decreases. Considering the distance from the screen, our threshold is level 3. So, the subject should perceive 3D shape on Random Dot V test at least level 3 and above.\n\n\n\nFrACT10: Test when both eyes are open and then, when only one eye open (left and right seperately). You may train the subject before testing because it is not so straight-forward for some people.\n\nacuity_task                  4-step Landolt-Cs\nunit                          VAdec (1.0+) & LogMAR(&lt;0)\nlengthOfCalibrationRuler   173.0\ndistanceInCm               230.0\nmaxDecimalAcuity         2.41\ncontrastWeber             100.0\nnTrials                       24\nrangeLimitStatus           rangeOK\ncrowding                   0\nthreshold                   logMAR =&lt; 0.4\n\nRandom Dot V:\nThe first number at the right bottom corner of test window indicates the level of difficulty. Test starting from level 6 - right-click increases and left-click decreases the level. When you move the cursor outside of the test window, the stimulus is regenereted.\nThe second number at the right bottom corner of test window indicates the orientation of the letter V. The opening of V is up for 1, right for 2, down for 3, and left for 4.\n\ndistanceInCm    230.0\nthreshold        hardest level with correct response =&lt; 3",
    "crumbs": [
      "Home",
      "Behavioral",
      "Vision tests"
    ]
  },
  {
    "objectID": "behavioral/2024-05-22/index.html#visual-acuity",
    "href": "behavioral/2024-05-22/index.html#visual-acuity",
    "title": "Vision tests",
    "section": "",
    "text": "According to the World Heath Organization (WHO), “visual acuity is … a simple, non-invasive measure and critical to determine the presence of vision impairment” [1]. The vision impairment is an eye condition which causes imperfections in the function of visual system [2]. We use visual acuity tests to quantify how acurately the visual system perceives the outer world.\nIn our lab, we perform FrACT10 to measure the visual acuity. The subjects having mild impairment or worse are excluded based on the scores of FrACT10. The threshold for exclusion is 0.40 logMAR or 0.4 decVA [3].",
    "crumbs": [
      "Home",
      "Behavioral",
      "Vision tests"
    ]
  },
  {
    "objectID": "behavioral/2024-05-22/index.html#stereoacuity",
    "href": "behavioral/2024-05-22/index.html#stereoacuity",
    "title": "Vision tests",
    "section": "",
    "text": "Stereopsis is also an important phenomenon for binocular rivalry since it is based on binocular disparity [4]. Therefore, the subjects who has a stereoacuity less than 30arcsec are considered as having poor stereovision and mostly excluded from binocular rivalry experiments [5]. We use Random Dot V test with red cyan anaglyph glasses to quantify the stereopsis.\nThe difficulty of the test increases while the number on the test window decreases. Considering the distance from the screen, our threshold is level 3. So, the subject should perceive 3D shape on Random Dot V test at least level 3 and above.",
    "crumbs": [
      "Home",
      "Behavioral",
      "Vision tests"
    ]
  },
  {
    "objectID": "behavioral/2024-05-22/index.html#parameters-that-we-use",
    "href": "behavioral/2024-05-22/index.html#parameters-that-we-use",
    "title": "Vision tests",
    "section": "",
    "text": "FrACT10: Test when both eyes are open and then, when only one eye open (left and right seperately). You may train the subject before testing because it is not so straight-forward for some people.\n\nacuity_task                  4-step Landolt-Cs\nunit                          VAdec (1.0+) & LogMAR(&lt;0)\nlengthOfCalibrationRuler   173.0\ndistanceInCm               230.0\nmaxDecimalAcuity         2.41\ncontrastWeber             100.0\nnTrials                       24\nrangeLimitStatus           rangeOK\ncrowding                   0\nthreshold                   logMAR =&lt; 0.4\n\nRandom Dot V:\nThe first number at the right bottom corner of test window indicates the level of difficulty. Test starting from level 6 - right-click increases and left-click decreases the level. When you move the cursor outside of the test window, the stimulus is regenereted.\nThe second number at the right bottom corner of test window indicates the orientation of the letter V. The opening of V is up for 1, right for 2, down for 3, and left for 4.\n\ndistanceInCm    230.0\nthreshold        hardest level with correct response =&lt; 3",
    "crumbs": [
      "Home",
      "Behavioral",
      "Vision tests"
    ]
  },
  {
    "objectID": "archive/freesurfer FSFAST.html#reasons-to-use-freesurfers-fsfast",
    "href": "archive/freesurfer FSFAST.html#reasons-to-use-freesurfers-fsfast",
    "title": "FreeSurfer FSFAST",
    "section": "Reasons to use FreeSurfers FSFAST",
    "text": "Reasons to use FreeSurfers FSFAST\n\nIf fitlins does not have a particular option for you that you would like to use i.e. different\nPotentially easier when working with surface space data (FSFAST was originally designed for surface space data)"
  },
  {
    "objectID": "archive/freesurfer FSFAST.html#neurodesk",
    "href": "archive/freesurfer FSFAST.html#neurodesk",
    "title": "FreeSurfer FSFAST",
    "section": "Neurodesk",
    "text": "Neurodesk\n\nWhat does a container mean\n\nNeurodesk has a whole host of software that is available for us to use\nNeurodesk works in which each piece of software has its own container\nSimply put for us this means that we use 1 piece of software that is contained inside its own environment\n\n\n\n\nExample of what containers mean\n\n\n\nSo for the example above if you were inside of the FreeSurfer container the only available software to you would be FreeSurfer\nMATLAB, fMRIprep and SPM would not be available to you\n\n \n\n\n\n\n\n\nNote: MATLAB runtime environment\n\n\n\nHowever, FreeSurfer uses something called a MATLAB runtime environment\n\nMATLAB runtime environment is smaller piece of software that is inside the FreeSurfer container\nFreeSurfer commands can be run without the whole of MATLAB directly and instead use this “MATLAB runtime environment”"
  },
  {
    "objectID": "archive/freesurfer FSFAST.html#freesurfer-commands-that-do-rely-on-the-matlab-runtime-environment",
    "href": "archive/freesurfer FSFAST.html#freesurfer-commands-that-do-rely-on-the-matlab-runtime-environment",
    "title": "FreeSurfer FSFAST",
    "section": "FreeSurfer commands that do rely on the MATLAB runtime environment",
    "text": "FreeSurfer commands that do rely on the MATLAB runtime environment\n\nSometimes when running FreeSurfer commands on neurodesk you might run into an error like so:\n\n\n\n\n\n\n\nWhat this means is the following:\n\nFreeSurfer by default is looking for a whole MATLAB installation\nFreeSurfer does not know whether there is a MATLAB runtime environment\n\nThis is because the MATLAB runtime environment is installed in a folder somewhere"
  },
  {
    "objectID": "archive/freesurfer FSFAST.html#steps-to-run-freesurfer-commands",
    "href": "archive/freesurfer FSFAST.html#steps-to-run-freesurfer-commands",
    "title": "FreeSurfer FSFAST",
    "section": "Steps to run FreeSurfer commands",
    "text": "Steps to run FreeSurfer commands\n1. Start a FreeSurfer container:\n\n\n\n\n\n \n2. Set up your environment variables:\n\nWhen using FreeSurfer it’s important to tell FreeSurfer where to find the subjects directory (SUBJECTS_DIR)\n\n\n(see FreeSurfers FSFAST tutorial for more information)\n\nFor example:\nexport SUBJECTS_DIR=$TUTORIAL_DATA/buckner_data/tutorial_subjs\n \n3. Change directory to where your data is:\nFor example:\ncd data\n \n4. Try and run the command first:\nFor example:\nFreeview ← this doesn’t rely on MATLAB so the freeview window should open\nrecon-all ← this doesn’t rely on MATLAB either so should work\nmri_vol2vol ← this doesn’t rely on MATLAB so should also work\nmkcontrast-sess ← this does rely on MATLAB runtime environment\n \n5. What to do if your command doesn’t work?\nIf you run a command for example:\nmkcontrast-sess -analysis auditory_v_baseline.sm0   -contrast a_vs_base -a 2 -c 1\nAnd then see an error like so:\n\n\n\n\n\n\nFigure 1\n\n\n\n\nThis means that FreeSurfer doesn’t know where the MATLAB runtime environment is therefore run the following:\n\nexport FS_MCRROOT=/opt/MCR2019b/v97/\nThe command above is important to tell FreeSurfer where the MATLAB runtime environment is\n \n6. Re-run the command again\n\nIf you fall into the error as above Figure 1 : then run the command using -mcr 1\n\nFor example:\nmkcontrast-sess -analysis auditory_v_baseline.sm0   -contrast a_vs_base -a 2 -c 1 -mcr 1"
  },
  {
    "objectID": "archive/dicom_to_bids.html",
    "href": "archive/dicom_to_bids.html",
    "title": "Dicom to Bids conversion",
    "section": "",
    "text": "Documentation related to the conversion of dicoms to BIDS format\n\ndcm2niix github\npydicom\ndcmunpack\n\n\nSee also BIDS specificiation\n\nOther programs that can be used to convert dicoms to BIDS\n\nGoing from DICOMS to BIDS datasets (with folder structure):\nhttps://github.com/nipy/heudiconv https://github.com/cbedetti/Dcm2Bids https://github.com/jmtyszka/bidskit https://github.com/dangom/dac2bids\nGoing from DICOMS to NIFTI + BIDS JSON sidecar:\nhttps://github.com/rordenlab/dcm2niix\nmatlab dicom to nifti tool converter"
  },
  {
    "objectID": "archive/dicom_to_bids.html#script",
    "href": "archive/dicom_to_bids.html#script",
    "title": "Dicom to Bids conversion",
    "section": "Script",
    "text": "Script\n\nI created a script found here that carries out the transformation from dicom to BIDS format\nThis is useful because often times we want to convert to BIDS and there are lots of different ways to do this (some more complicated than others)\nHere I wanted the script to be as easy as possible for a number of reasons but primarily so that anyone can run it (even without using .json files)"
  },
  {
    "objectID": "archive/dicom_to_bids.html#how-to-use-the-script",
    "href": "archive/dicom_to_bids.html#how-to-use-the-script",
    "title": "Dicom to Bids conversion",
    "section": "How to use the script",
    "text": "How to use the script\n\nThe script will guide you through the process of converting from dicoms to BIDS\nWhat’s important to know is that the script is not always accurate and will require some input to correct some things\n\n \n\n1. Choose the name of the dicom folder in which to convert\n\n \n\n\n2. Optional: If there are subdirectories in the dicom folder (e.g. ses-1 or ses-2) pick one\n\n \n\n\n3. Get the name of BIDS subject\n\n\nWhat is often the case for us is that the dicoms folder for each participant is named differently to how we want out subject/ BIDS name to be.\n\n \n\n\n4. DICOM search. This will either use pydicom or dcmunpack\n\nFor some reason pydicom runs a lot faster!!! dcmunpack can take hours but using pydicom can be finished in a few minutes\n\n\nHere the script roughly unpacks all the dicoms ready for the step which renames files and folders and puts the niftis in these new folders\n\n\n \n\n\n5. Information about the renaming and BIDS conventions\n\npydicom and dcmunpack don’t get everything right 100% of the time and the folders/ names of the niftis might be incorrect. This is important as you might want to rename your dicom files into a new name that fits the BIDS format\n\n\nThe option to delete dicoms is also important, the anat-scout is not crucial for an analysis so theres no need to try to convert it to BIDS\n\n\n \n\n\n6. Keep/ modify/ delete\n\nChoose to keep the dicom and the name of the dicom, modify it or delete it (ie don’t convert)\n\n\nAny action here will apply to the .nii.gz and the .json\n\n\n\nIf you want to modify the name of the BIDS file then choose modify and rename according to the BIDS naming convention\n\n\n \n\n\nIf you want to keep or modify the folder the BIDS file belong to then select either keep or modify\n\n\n\nHere I want to keep the folder func because it is correct\n\n\n\nHere I want to choose the folder extra (this is not a BIDS folder but I want to convert the dicom to a nifti, I’d remove this before fMRIprep)\n\n\n\nOften times the anat folder name is wrong. Here the name should just be anat\n\n \n\n\n7. Adding IntendedFor field to fmap json\n\nThe conversion does not always add the IntenderFor field to the .json in the fmap folder\nFor [distortion correction] to take place in fMRIprep it is necessary to add the IntendedFor field to the .json\nThe script uses jq to provide the insertion into the fmap .json file\nThe format that is added is not BIDS URI compliant but the script will output a text file that can be used to change the intendedfor field with BIDS uri compliant paths."
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "FreeSurfer FSFAST\n\n\n\nMRI\n\n\nAnalysis\n\n\narchived\n\n\n\nInformation about running an FSFAST analysis on Neurodesk\n\n\n\nAC\n\n\nJan 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDicom to Bids conversion\n\n\n\nMRI\n\n\nBIDS\n\n\nDICOM\n\n\narchived\n\n\n\nhow to convert from dicom’s to bids\n\n\n\nAC\n\n\nAug 12, 2024\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top",
    "crumbs": [
      "Home",
      "Archive"
    ]
  },
  {
    "objectID": "behavioral/2024-05-23/index.html",
    "href": "behavioral/2024-05-23/index.html",
    "title": "BIDS for behavioral studies",
    "section": "",
    "text": "Behavioral experiments (with no neural recordings) - Brain Imaging Data Structure v1.9.0",
    "crumbs": [
      "Home",
      "Behavioral",
      "BIDS for behavioral studies"
    ]
  },
  {
    "objectID": "behavioral/2024-05-23/index.html#resources",
    "href": "behavioral/2024-05-23/index.html#resources",
    "title": "BIDS for behavioral studies",
    "section": "",
    "text": "Behavioral experiments (with no neural recordings) - Brain Imaging Data Structure v1.9.0",
    "crumbs": [
      "Home",
      "Behavioral",
      "BIDS for behavioral studies"
    ]
  },
  {
    "objectID": "behavioral/2024-05-23/index.html#software",
    "href": "behavioral/2024-05-23/index.html#software",
    "title": "BIDS for behavioral studies",
    "section": "Software:",
    "text": "Software:\nbids-matlab |\nbids-starter-kit on GitHub |\nJSONio on GitHub",
    "crumbs": [
      "Home",
      "Behavioral",
      "BIDS for behavioral studies"
    ]
  },
  {
    "objectID": "behavioral/2024-05-23/index.html#how-to-implement",
    "href": "behavioral/2024-05-23/index.html#how-to-implement",
    "title": "BIDS for behavioral studies",
    "section": "How to implement",
    "text": "How to implement\nYou may use the template in the BIDS starter kit (bids-starter-kit/general). Simply copy the corresponding mat file and modify the variables for your project.\n\nAfter preprocessing:\ncreate_events_tsv_json_full.m\n\n\nBefore finalizing everything:\ncreateBIDS_participants_tsv.m\ncreateBIDS_dataset_description_json.m\n\n\nPipeline for transriv project as an example\n\nAdd ‘transriv-bids’ to matlab path\nGo to the root_dir or use the full path of root_dir as input to the functions\nPreprocess data from a subject:\n\ncreateBIDS_beh_after_preprocessing(root_dir, ‘transriv’, ‘S001’, ‘Rivalry’, ‘GG’)\n\nCreate dataset files by running transriv_finalizeBIDS.m (when data collection is finished)\n\nThe folder structure based on BIDS:\nroot_dir\n  └─ ‘transriv’\n        ├─ participants.xlsx\n        ├─ participants.tsv\n        ├─ participants.json\n        ├─ sourcedata\n        │     └─ sub-S001\n        │         └─ ses-Rivalry\n        │               └─ beh\n        │                   └─ sub-S001_ses-Rivalry_run-GG_beh.mat\n        └─ rawdata\n             └─ sub-S001\n                 └─ ses-Rivalry\n                     └─ beh\n                         ├─ sub-S001_ses-Rivalry_run-GG_beh.json\n                         └─ sub-S001_ses-Rivalry_run-GG_beh.tsv\nLink to our GitHub repo containing relevant scripts",
    "crumbs": [
      "Home",
      "Behavioral",
      "BIDS for behavioral studies"
    ]
  },
  {
    "objectID": "behavioral.html",
    "href": "behavioral.html",
    "title": "Information about collecting behavioural data",
    "section": "",
    "text": "Custom-built chin rests\n\n\n\n3D-Print\n\n\n\nInformation about producing a chinrest for behavioral experiments\n\n\n\nNZ\n\n\nAug 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBIDS for behavioral studies\n\n\n\nBIDS\n\n\nBehavioral\n\n\n\nHow to store your data in bids structure if you are using psychtoolbox\n\n\n\nCY\n\n\nMay 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nVision tests\n\n\n\nVision-Prescreening\n\n\n\nHow to conduct vision tests\n\n\n\nCY\n\n\nMay 22, 2024\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top",
    "crumbs": [
      "Home",
      "Behavioral"
    ]
  },
  {
    "objectID": "dicom_to_glm/defacing.html",
    "href": "dicom_to_glm/defacing.html",
    "title": "04 - Defacing",
    "section": "",
    "text": "Ensure Defacing!\n\n\n\nDefacing your T1w scans is an important preprocessing step that has to be done before running fmriprep as you´ll probably run into errors otherwise!\nBUT: don´t run the defacing by default! You should always check first if the T1w-file is already defaced. Running the defacing algorithm multiple times(even though not necessary) can potentially harm the quality of your data!",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "04 - Defacing"
    ]
  },
  {
    "objectID": "dicom_to_glm/defacing.html#sec-startPydeface",
    "href": "dicom_to_glm/defacing.html#sec-startPydeface",
    "title": "04 - Defacing",
    "section": "2.1 Start Pydeface (v.2.0.2)",
    "text": "2.1 Start Pydeface (v.2.0.2)\nTo deface your t1w, you first have to open pydeface:\n\nClick on the Birds-Symbol at the bottom left\nHover Neurodesk\nHover All Applications\nScroll down until you find pydeface\nClick on pydeface 2.0.2 \n\nAfter it opens, it may need some time to load everything - be patient!\nFinally, it should look like this:",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "04 - Defacing"
    ]
  },
  {
    "objectID": "dicom_to_glm/defacing.html#get-the-path-to-your-original-t1w-scan",
    "href": "dicom_to_glm/defacing.html#get-the-path-to-your-original-t1w-scan",
    "title": "04 - Defacing",
    "section": "2.2 Get the path to your original T1w scan",
    "text": "2.2 Get the path to your original T1w scan\n\nNavigate to the anatomical scan of the participant within your BIDS directory\nRight-click on the anatomical nifti file\nSelect `Copy Path(s)",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "04 - Defacing"
    ]
  },
  {
    "objectID": "dicom_to_glm/defacing.html#the-pydeface-command",
    "href": "dicom_to_glm/defacing.html#the-pydeface-command",
    "title": "04 - Defacing",
    "section": "2.3 The pydeface command",
    "text": "2.3 The pydeface command\n\nGo back to the terminal window, where pydeface is loaded (or start it again)\nType pydeface\nEnter the path to your T1w scan (right-click + paste or type in the path)\nType --outfile\nRepeat Step 3 (with this approach, you “overwrite” the initial T1w with the then defaced T1w file)\nType --force\n\nIt should look like this: \npydeface /home/jovyan/Website/bids/sub-999/ses-1/anat/sub-999_ses-1_T1w.nii.gz --outfile /home/jovyan/Website/bids/sub-999/ses-1/anat/sub-999_ses-1_T1w.nii.gz --force\nThen, press enter. Again, it might take a few moments, until pydeface finishes, be patient! At the end, it should look like this:",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "04 - Defacing"
    ]
  },
  {
    "objectID": "dicom_to_glm/defacing.html#the-next-steps",
    "href": "dicom_to_glm/defacing.html#the-next-steps",
    "title": "04 - Defacing",
    "section": "2.4 The next steps",
    "text": "2.4 The next steps\nCongratulations, you successfully defaced a T1w-scan and you are one step closer to analyzing your data!\nTo continue in your journey to your Analysis, please return to the Overview-Page to check what to do next.\nAlternatively, here is a list of potential next steps to continue with\n\nProvide Participants their T1w file\nRemove Noise Scans from your data?\nCheck the IntendedFor field of the fmap\nrun fmriprep",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "04 - Defacing"
    ]
  },
  {
    "objectID": "dicom_to_glm/first_level_glm_nilearn.html",
    "href": "dicom_to_glm/first_level_glm_nilearn.html",
    "title": "08 - first level GLM",
    "section": "",
    "text": "After preprocessing your data, it´s time to do your first level GLM (= general linear model).\nIn this step, you will compute mean contrast-estimates for predefined contrasts for each subject individually. These estimations will be used later in the second level Analysis, where you extract relevant (for your research question/hypothesis) values.\n\n1 Surface-based\nThis page describes the first level GLM for a previous master thesis. It included 1 localizer run and 8 experimental runs. The experimental runs differed in the used attention task (“L”etter vs. “C”olor) and the flicker-condition (“F”ull vs. “N”othing; = flickering against full circles or against the background). Every “attention task” x “flicker condition” combination was presented two times. For every task (Loc, LF, LN, CF, CN), a separate GLM had to be computed. To analyze this data, a surface-based approach was chosen. Important: This study only included one session! If you did multiple sessions, you have to adapt the script accordingly!\nYou can find the example jupyter notebook here: /shared/website/GLM_surface_Main.ipynb\nKeep in mind, that some processes might take some time!\nShort explanation of the process (enumeration corresponds to the cell number in the notebook):\n\nLoad necessary packages/modules\nHelper function to get SurfaceImages (can be ignored, hopefully not necessary in the future)\nDefinition of paths, parameters and constants\n\nif you only have one task or one subject, enter the corresponding name/id, but make sure that it is within the squared brackets! (Subsequent processes loop over these lists! If there is only one entry, it just stops after one iteration)\n\nExtract the first level GLM from your BIDS structure (if everything is defined correctly in the cell before, you can simply run this without any changes)\nOptimal: Uncomment (= remove the #) and run to confirm for example the number of FirstLevelModel objects that were extracted\nGet the surface data with the workaround (if everything is/was defined correctly, you can simply run this without any changes)\nIn this cell, the models are acutally fitted to the data. Again, if everything was defined correctly you can simply run this cell (Even though the processes are parallelized, this probably takes some time!)\nNow you define the contrasts.\n\nIn this case, it loops through the tasks (definetely necessary) and subjects (could potentially be omitted if everything is the same for all participants) and defines the corresponding contrasts\nfirst a directory is set up to store the design matrix (plot_design_matrix immediately afterwards) and the contrast visualizations (plot_contrast_matrix at the end)\nYou have to think through your task & subject structure (influences how and how many objects you have for example in fitted_models) to ensure that the logic holds! (in this the GLM for the first task is stored for the 5 subjects, then for the second task, etc.; depending on your setup, you might need to change the for loops)\nbasic_contrasts is a dictionary with “column-name” as key and the column as value for each column of the design matrix (\\(\\to\\) has as many entries/key-value-pairs as the design matrix has columns)\ncontrasts is the actual definition of your contrasts and is implemented as a dictionary as well. The key is the name of your contrast (can be chosen freely, but it is advised to use a “descriptive” name, such that you later immediately know what the contrast is about). The value is the actual contrast, e.g. one condition vs. another. Make sure that the value in the squared brackets behind basic_contrasts corresponds to the actual name of the column in the design matrix (as basic_contrasts is a dictionary and with basic_contrasts[key_name], it searches a key named key_name and returns its stored value)\n\nNow you actually run the contrasts (i.e. you compute the values for the subjects/tasks)\n\nchange the name of the lists within the for loops. You need as many empty lists as you defined contrasts in the previous step\nfor every contrast (and thus list) you need the .compute_contrast section with the corresponding name that you defined in the previous step\n\nSaves the activation maps (should run without any changes)\nSaves the activation maps (should run without any changes). These are the files with which you later work in the second level GLM\n\n\nThe notebook /shared/website/GLM_surface_Loc.ipynb was used for the GLM of the localizer data. The structure and logic is the same is for the main experiment\n\n\n\n2 Volume-based Analysis\nIf you do a volume-based Analysis, you can also do this with nilearn and a jupyter notebook. You can find an example here: /shared/website/GLM_volume.ipynb. The structure and logic again is similar, but some stepts have to be done differently compared to a surface-based analysis.\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "08 - first level GLM"
    ]
  },
  {
    "objectID": "dicom_to_glm/ProvidingParticipantsWithTheirT1wFile.html",
    "href": "dicom_to_glm/ProvidingParticipantsWithTheirT1wFile.html",
    "title": "03 - T1w for PBn",
    "section": "",
    "text": "It can be a good incentive to promote your study if you offer participants to give them their T1w files (since everyone knows how cool images of brains are!) If you promise to give out the T1w-scans to participants, make sure, that you really do it!\nThere are only some things to remember:\n\nParticipants always get the “faced” T1w scan. If you get the data from the MRI lab directly, there should be a faced directory with the files that you can give to the participant. If you started with raw DICOM files, make sure to send the T1w file before defacing or save defaced image separately!\nIf you provide participants with their T1w scans, it might be a good idea to provide them with the software on how to look at their data! For example, you could point them to MRIcroGL as a free tool that they can install themselves to look at their data: Link to MRIcroGL Download (there is also a ready-to-distribute PDF for the installation of MRIcroGL here: /shared/website/How_to_look_at_your_brain.pdf)\nWe do not keep the “non-defaced” T1w scans of participants! Thus, delete the non-defaced T1w scan of a participant either immediately after defacing, or at the latest after your study finished!\n\n\n1 The next Steps\nTo continue in your journey to your Analysis, please return to the Overview-Page to check what to do next.\nAlternatively, here is a list of potential next steps to continue with\n\nDefacing\nCheck JSON file of the fmap\nRemove noise scans\nfmriprep\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "03 - T1w for PBn"
    ]
  },
  {
    "objectID": "dicom_to_glm/retinotopic_mapping.html",
    "href": "dicom_to_glm/retinotopic_mapping.html",
    "title": "07 - Retinotopic Mapping",
    "section": "",
    "text": "Retinotopic mapping is used to individually identify subregions in the visual cortex (i.e V1, V2, V3 and sometimes higher areas as well). This is then used to better understand how visual information is processed in the brain by different regions.\nNormally, this is achieved by implementing dedicated “retinotopy-runs” in the experimental design. However, this is not always possible.\nThe Benson2014 atlas allows to get a retinotopic mapping even without dedicated “retinotopy-runs” by using structural features and probabilistic maps. Generally, it gives you information (in dedicated files) about specific regions (i.e. discrete labels for V1, V2, etc.; varea), the eccentricity (i.e. the distance from fixation in visual degrees; eccen) or the polar angle (angle)\n\n\n\n\n\n\nImportant\n\n\n\nTo do the retinotopic mapping, you need preprocessed data!\n\n\n\nThe script\nThe script to run the probabilistic retinotopic mapping can be found on the server: /shared/website/occatlas.sh\n(If you do surface based analysis, you can adapt the script /shared/website/occatlas_surface_multipleSubs.sh to run multiple subjects after each other.)\nAlternatively, you can copy/paste the code (after adapting it for your data! See below):\n#!/usr/bin/env bash\n\n# This script demonstrates how to run Benson's neuropythy atlas on neurodesk\n# and map early visual areas onto the subject's functional volume space\n# NZ 2025.03.21\n\n# define variables and make dirs\nml freesurfer/7.4.1 # tested with fs 7.4.1\nsid=999\nniidir=/home/jovyan/completion/4_conditions_main/bids\nexport SUBJECTS_DIR=$niidir/derivatives/fmriprep/sourcedata/freesurfer\nexport SINGULARITYENV_SUBJECTS_DIR=$SUBJECTS_DIR # this is container-specific definition of SUBJECTS_DIR\nexport APPTAINERENV_SUBJECTS_DIR=$SUBJECTS_DIR   # this is container-specific definition of SUBJECTS_DIR\nmkdir -p /home/jovyan/completion/4_conditions_main/bids/neuropythy_tmp\nshow=yes # show images: yes or no\ndo_volumetric=no # run volumetric (surf2vol) part: yes or no\n\n# run benson's occipital atlas docker as singularity container\ntmpdir=/home/jovyan/completion/4_conditions_main/bids/neuropythy_tmp\nsingularity run -B $SUBJECTS_DIR:/subjects \\\n  -B $tmpdir:/data docker://nben/neuropythy \\\n  atlas --verbose sub-$sid\n\nif [ \"$show\" = \"yes\" ]; then\n  # take a look at the data\n  freeview \\\n    -f $SUBJECTS_DIR/sub-$sid/surf/lh.inflated:overlay=$SUBJECTS_DIR/sub-$sid/surf/lh.benson14_varea.mgz:overlay_color=colorwheel \\\n    -f $SUBJECTS_DIR/sub-$sid/surf/rh.inflated:overlay=$SUBJECTS_DIR/sub-$sid/surf/rh.benson14_varea.mgz:overlay_color=colorwheel\nfi\n\n# run volumetric projection if enabled\nif [ \"$do_volumetric\" = \"yes\" ]; then\n\n  # surf2vol\n  mkdir -p $niidir/derivatives/rois/sub-$sid\n\n  # loop over hemis\n  for hemi in lh rh; do\n\n    # convert to t1 space first\n    mri_surf2vol \\\n      --surfval $SUBJECTS_DIR/sub-$sid/surf/$hemi.benson14_varea.mgz \\\n      --temp $SUBJECTS_DIR/sub-$sid/mri/orig.mgz \\\n      --identity sub-$sid \\\n      --o $niidir/derivatives/rois/sub-$sid/$hemi.sub-${sid}_benson14_varea_space-T1.nii.gz \\\n      --fill-projfrac -0.5 1.5 0.05 \\\n      --subject \"sub-$sid\" \\\n      --hemi $hemi\n\n    if [ \"$show\" = \"yes\" ]; then\n      freeview \\\n        $SUBJECTS_DIR/sub-$sid/mri/orig.mgz \\\n        $niidir/derivatives/rois/sub-$sid/$hemi.sub-${sid}_benson14_varea_space-T1.nii.gz:colormap=LUT\n    fi\n\n    # now convert from T1 to BOLD space\n    mri_vol2vol \\\n      --mov $niidir/derivatives/rois/sub-$sid/$hemi.sub-${sid}_benson14_varea_space-T1.nii.gz \\\n      --targ $niidir/derivatives/fmriprep/sub-$sid/ses-1/func/*task-loc*space-T1w_boldref.nii.gz \\\n      --o $niidir/derivatives/rois/sub-$sid/$hemi.sub-${sid}_benson14_varea_space-bold.nii.gz \\\n      --regheader \\\n      --nearest\n\n    if [ \"$show\" = \"yes\" ]; then\n      freeview \\\n        $niidir/derivatives/fmriprep/sub-$sid/ses-1/func/*task-loc*space-T1w_boldref.nii.gz \\\n        $niidir/derivatives/rois/sub-$sid/$hemi.sub-${sid}_benson14_varea_space-bold.nii.gz:colormap=LUT\n    fi\n\n    # binarize the labels\n    mri_binarize \\\n      --i $niidir/derivatives/rois/sub-$sid/$hemi.sub-${sid}_benson14_varea_space-bold.nii.gz \\\n      --match 1 2 3 --o \"$niidir/derivatives/rois/sub-$sid/$hemi.sub-${sid}_evc_space-bold.nii.gz\"\n\n  done\n\n  # unite hemispheres and binarize again\n  mri_concat \\\n    --i $niidir/derivatives/rois/sub-$sid/lh.sub-${sid}_evc_space-bold.nii.gz \\\n    --i $niidir/derivatives/rois/sub-$sid/rh.sub-${sid}_evc_space-bold.nii.gz \\\n    --sum \\\n    --o $niidir/derivatives/rois/sub-$sid/sub-${sid}_evc_space-bold.nii.gz\n\nfi\n\n# the rest should probably be done in python, thresholding values on the fly\n\n\nNecessary adaptiation steps\n\nChange the variable sid to match the subject you want to process (only the number is necessary! In the script it will be implemented as sub-sid). It has to match the directory name of your subjects!\nChange the variable niidir to match the path to your BIDS-directory\nOptional (you can look at the output later anyway): Change the variable show to see the intermediate steps (\\(\\to\\) show=yes vs. show=no)\nIndicate if you use volumentric (do_volumetric=yes) oder surface data (do_volumentric=no) in your later analysis\nSave your changes\nRun the script \\(\\to\\) open a terminal and execute:\n\nbash /Path/to/occatlas.sh\nchange the working directory to where the script is using cd /Path/to/directory/with/script and then bash ./occatlas.sh\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "07 - Retinotopic Mapping"
    ]
  },
  {
    "objectID": "dicom_to_glm/second_level_glm_nilearn.html",
    "href": "dicom_to_glm/second_level_glm_nilearn.html",
    "title": "09- second level GLM",
    "section": "",
    "text": "Similar to the first level GLM, an example is provided and described as a starting point to adapt your own work.\nIn the second level GLM you want to take the necessary steps to make inferences beyond individual subjects (as out first level GLM considered the subjects individually and tried to predict their activation). This is step is what leads you to the “actual” answers of your research questions and hypothesis.\nWe continue with the example described in the first level GLM. The goal now is, to extract contrast estimates for specific regions of interest. These regions of interest will be infered from the localizer data and retinotopic mapping (We look for areas in the primary visual cortex that are specifically active for visual stimuli in a specific region of the visual field). The inferred mask will then be overlayed on the data of the functional runs, to get the contrast estimates within these regions of interest (these contrast estimates are the values that go into our statistical analysis at the end).\nHint about the localizer: In different conditions, a flickering checkerboard was visible in specific areas of the screen. A simple example is to show a flickering checkerboard either in the left or the right half of the screen. By looking at the activation data a defining correct contrasts in the first level GLM, this allows you to infer areas that show specific activation only for the left (or right) visual field.\n(e.g. when a flickering checkerboard is presented in the top right quadrant of the screen, we want to find areas that correspond only to this condition)\nYou can find the example script here: /shared/website/GLM_secondLevel.ipynb\nThe general logic/structure is as follows:\n\nImport necessary libraries and modules\nDefine important variables:\n\nsubjects and data directory\nsubregions (= Regions of Interes): corresponding to your naming convention of the files from the first level GLM for the localizer data\ncontrast_names: corresponding to your naming convention of the files from the first level GLM for the experimental data\ntasks: you don´t need to include this, but you might need to change subsequent code then\n\nMake a mask based on the data from the retinotopic mapping for every subjects\n\nget and load the data with the information about the specific regions (= *_varea.mgz) and about the eccentricity (= *_eccen.mgz)\ndefine a mask (for each hemisphere) that is “True” (=boolean value) for all vertices of the surface, where the *_varea.mgz file is 1 (\\(\\to\\) meaning that this vertex corresponds to V1), and “False” for all other vertices\ndefine a mask (for each hemisphere) that is “True” for all vertices, that have an eccentricity of &gt; 0 and &lt; 5 (\\(\\to\\) only the central visual field) and “False” for all other vertices (\\(\\to\\) that cover the periphery)\ncombine both masks and save the resulting mask containing areas that are both in V1 AND have an eccentricity &lt; 5 (if you compare two boolean values with AND/&, the result is only “True”, if both inputs that are compared are “True”)\n\nMake a mask based on the data from the localizer data for every subject\n\\(\\to\\) We need a mask for every region of interest that we want to consider later\n\\(\\to\\) “For-loop” over all subregions that we defined in step 2. Thus, for every of these regions (that also have a corresponding file from the first-level GML)\n\nGet and load the maps containing the z-scores for the specific region of interest for both hemispheres\nDefine and save a mask that is “True” for all vertices with a z-score &lt; 1.96 and “False” for all other vertices (\\(\\to\\) we want the vertices, that are more active when the respective area of the visual field was stimulated in the localizer)\n\nMake a combined mask for every subject\n\\(\\to\\) our goal for the mask is to find vertices that\n\nAre located in V1 and cover the central visual field (achieved by steo 3)\nAre more active in the specific condition of the localizer/visual field\n\n\\(\\to\\) by combining these two masks (again with a AND/& combination of two boolean martices), we get a resulting mask, that has the value “True” for all vertices that satify both condition a) & b) and “False” for all other vertices (\\(\\to\\) that satify none of only one of the two requirements)\nCalculate and save the contrast estimates for every subject and every subregion\n\nfirst we initialize an empty list to later store the results\nFor every subject (first loop) we want to get estimates for every region of interest (second loop). Within every region of interest, we want the estimates for every task (third loop, defined in step 2 in the variable tasks; here you have to adapt the code if you chose to not use the variable!). Finally, we want the contrast estimate for every (relevant) contrast (fourth loop; also defined in step 2)\nWithin each combination, we 1) get the statmap from the first level GLM containing the contrast estimates for each vertex 2) apply the combined mask from the previous steps to this data (\\(\\to\\) We apply the boolean values True/False of the mask to the numerical data of the statmap. Like this, we only consider the contrast estimates of vertices that also satisy the conditions that we defined in step 5!)\nwe take the mean from the resulting values (so only those values corresponding to vertices that satisfy the conditions 5a and 5b) to get the “Mean Contrast Estimate” of a specific subject, for a specific contrast in a specific region of interest\nwe save the estimate and the information about the condition\n\nAfter calculating the mean contrast estimate for all “Subject” x “Contrast” x “Region of Interst” combination, we save this information in a .csv file\nOptimal: We can already define some plots/visualizations here\n\n\nThis example of a second level GLM already includes a “control analysis”/sanity check by looking at the contrast estimates of the localizer data is well. It can be done easier, or in two notebooks!\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis",
      "09- second level GLM"
    ]
  },
  {
    "objectID": "dicom_to_glm.html",
    "href": "dicom_to_glm.html",
    "title": "MRI: from DICOMs to GLM",
    "section": "",
    "text": "00 - Overview\n\n\nOverview about how to get from DICOM files from the Scanner to your GLM Analysis\n\n\n\nMG\n\n\nJul 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n09- second level GLM\n\n\n\nMRI\n\n\nAnalysis\n\n\nfmriPrep\n\n\nNeurodesk\n\n\nnilearn\n\n\n\nExample of second level GLM\n\n\n\nMG\n\n\nJul 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n02 - Check IntendedFor field of the fmap’s JSON\n\n\n\nMRI\n\n\nAnalysis\n\n\nfmriPrep\n\n\nPreprocessing\n\n\nNeurodesk\n\n\n\nDescription of how to check and potentially update the fmap’s JSON file\n\n\n\nMG\n\n\nJul 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n08 - first level GLM\n\n\n\nMRI\n\n\nAnalysis\n\n\nfmriPrep\n\n\nNeurodesk\n\n\nnilearn\n\n\n\nSeveral approaches to run a first level GLM using nilearn\n\n\n\nMG\n\n\nMay 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n07 - Retinotopic Mapping\n\n\n\nMRI\n\n\nRetinotopy\n\n\n\nProcedure to apply the probabilistic retinotopic mapping using the Benson Atlas\n\n\n\nMG\n\n\nMay 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n01 - DICOM to BIDS\n\n\n\nMRI\n\n\nDICOM\n\n\nBIDS\n\n\nPreprocessing\n\n\nNeurodesk\n\n\n\nStep by step description of how to get from DICOM files to necessary nifti-files in BIDS format\n\n\n\nMG\n\n\nApr 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n03 - T1w for PBn\n\n\n\nMRI\n\n\n\nInformation about how to provide participants with their T1w image\n\n\n\nMG\n\n\nApr 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n04 - Defacing\n\n\n\nMRI\n\n\nPreprocessing\n\n\nNeurodesk\n\n\n\nStep by step description of how to deface your data\n\n\n\nMG\n\n\nApr 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTroubleshooting\n\n\n\nMRI\n\n\nAnalysis\n\n\nfmriPrep\n\n\nPreprocessing\n\n\nNeurodesk\n\n\n\nCollection of known potential problems and their solutions\n\n\n\nMG\n\n\nMar 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n05 - Removing Noise Scan\n\n\n\nMRI\n\n\nPreprocessing\n\n\nNeurodesk\n\n\n\nStep by step description of how to remove the denoising scan from your data\n\n\n\nMG\n\n\nMar 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n06 - fMRIprep\n\n\n\nMRI\n\n\nAnalysis\n\n\nfMRIprep\n\n\nNeurodesk\n\n\n\nStep by step description of how to run fMRIprep\n\n\n\nMG\n\n\nMar 8, 2025\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top",
    "crumbs": [
      "Home",
      "MRI: from DICOM to GLM Analysis"
    ]
  },
  {
    "objectID": "internship/checklist.html",
    "href": "internship/checklist.html",
    "title": "Internship Checklist",
    "section": "",
    "text": "Interns are an integral part of our team and are treated as equal colleagues. However, there are specific rules related to internships that all of us must abide by. Below you will find an internship checklist, which should help you navigate the process.",
    "crumbs": [
      "Home",
      "Internship",
      "Internship Checklist"
    ]
  },
  {
    "objectID": "internship/checklist.html#before-starting",
    "href": "internship/checklist.html#before-starting",
    "title": "Internship Checklist",
    "section": "Before starting",
    "text": "Before starting\n\nDiscuss the content of your project with the PI\nDo the necessary paperwork and make sure you got an o.k. from the secretary’s office, the HR department of the university or the international office (for international students).\n\nIf you are a bachelor student doing a “Pflichtpraktikum” at the University of Graz, you can find the relevant information in the Pflichtpraktikum Section.",
    "crumbs": [
      "Home",
      "Internship",
      "Internship Checklist"
    ]
  },
  {
    "objectID": "internship/checklist.html#on-the-first-day",
    "href": "internship/checklist.html#on-the-first-day",
    "title": "Internship Checklist",
    "section": "On the first day",
    "text": "On the first day\n\nGet an office key from the secretary’s office\nHave someone show you your desk\nDo the safety briefing with a PTA or PI (form can be found here)\nJoin our lab’s slack (ask someone to add you if you did not receive an invitation yet)\n\nDo not feel obliged to keep slack on all the time and silence your notifications for times when you don’t want to be disturbed. Everyone in the lab has the right to have their own working hours and nobody expects anyone else to have the same ones.\n\nMake sure you know the date and time of the lab meetings if you would like to attend them\nCreate a spreadsheet in any format where you note the dates and times you worked on the project. This is not meant to control you, but rather to make sure we do not overload you with work.\nIf you would like to appear as intern on our lab’s official website, please provide a square photograph (at least 700x700 pixels) of yourself (currently to MG)",
    "crumbs": [
      "Home",
      "Internship",
      "Internship Checklist"
    ]
  },
  {
    "objectID": "internship/checklist.html#during-the-internship",
    "href": "internship/checklist.html#during-the-internship",
    "title": "Internship Checklist",
    "section": "During the internship",
    "text": "During the internship\n\nNotify the PI when you reach your last 20 hours of your internship time, because we would love to hear about your project and its outcome in the lab meeting presentation.\nIf you prefer to use the office outside of the institute’s typical opening hours (Sundays, holidays and late evenings), you can ask the secretary’s office to activate your student card for entering the building.",
    "crumbs": [
      "Home",
      "Internship",
      "Internship Checklist"
    ]
  },
  {
    "objectID": "internship/checklist.html#before-leaving",
    "href": "internship/checklist.html#before-leaving",
    "title": "Internship Checklist",
    "section": "Before leaving",
    "text": "Before leaving\n\nMake sure you check out the leaving section. It is made primarily for doctoral students, but some things in there will apply to you as well.\nHave the PI sign your documents.\nOptional: Provide a short summary of what you did and what you found out that can be uploaded to the website",
    "crumbs": [
      "Home",
      "Internship",
      "Internship Checklist"
    ]
  },
  {
    "objectID": "internship/pflichtpraktikum.html",
    "href": "internship/pflichtpraktikum.html",
    "title": "Pflichtpraktikum",
    "section": "",
    "text": "&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD Um das Pflichtpraktikum im Rahmen des Bachelor- oder Masterstudiums in unserer Arbeitsgruppe absolvieren zu können, muss der gleiche Prozess wie für jede andere Praktikumsstelle für das Pflichtpraktium durchlaufen werden. Die relevanten Informationen hierzu sind auf der Institutsseite und im entsprechenden Moodle-Kurs zu finden.\nWichtig: Es sollten IMMER die Informationen des Moodle-Kurses beachtet werden! Die folgenden Schritte beschreiben die Schritte, wie sie in der Vergangenheit notwendig warem, um das Praktikum bei uns beginnen zu können:\n\nAnmeldung zur VU “Einführung in die Praxisfelder der Psychologie” (Bachelor; Kursnummer im WS: 602.060, Kursnummer im SS: 602.067) oder “Supervision der facheinschlägigen Praxis” (Master; Kursnummer immer “PSY.940”) in Uni Graz Online, oder auf Moodle mit dem Passwort “pp2011” selbst einschreiben.\nIm Kurs den ausgefüllten Antrag auf Genehmigung der Pflichtpraxis einreichen und auf das “OK” der Kursleitung warten.\n(Möglich während auf das “OK” für den Antrag gewartet wird:) Kursleitung (zur Zeit PJ) kontaktieren, dass diese die Bestätigung für die Praxisstelle unterschreibt. Das entsprechende Formular ist (wie alle anderen Formulare auch) im Moodle-Kurs zu finden.\nSobald das “OK” für den Antrag im Moodle-Kurs sowie die unterschriebene Bestätigung für die Praxisstelle vorliegt, muss eine Mail an das Sekretariat der allgemeinen Psychologie geschickt werden, um dann endlich mit dem Praktikum beginnen zu können. In der Mail sollten folgende Dokumente enthalten sein:\nUm das Pflichtpraktikum im Rahmen des Bachelor- oder Masterstudiums in unserer Arbeitsgruppe absolvieren zu können, muss der gleiche Prozess wie für jede andere Praktikumsstelle durchlaufen werden. Die relevanten Informationen hierzu sind auf der Institutsseite und im entsprechenden Moodle-Kurs zu finden.\n\nEs sollten IMMER die Informationen des Moodle-Kurses beachtet werden! Aus Erfahrungen in der Vergangenheit sollten die folgenden Schritte notwendig sein, um das Praktikum bei uns beginnen zu können:\n\nAnmeldung zur VU “Einführung in die Praxisfelder der Psychologie” (Bachelor; Kursnummer im WS: 602.060, Kursnummer im SS: 602.067) oder “Supervision der facheinschlägigen Praxis” (Master; Kursnummer immer “PSY.940”) in Uni Graz Online, oder auf Moodle mit dem Passwort “pp2011” selbst einschreiben.\nIm Kurs den ausgefüllten Antrag auf Genehmigung der Pflichtpraxis einreichen und auf das “OK” der Kursleitung warten.\n(Möglich während auf das “OK” für den Antrag gewartet wird:) Kursleitung (zur Zeit PJ) kontaktieren, dass diese die Bestätigung für die Praxisstelle unterschreibt. Das entsprechende Formular ist (wie alle anderen Formulare auch) im Moodle-Kurs zu finden.\nSobald das “OK” für den Antrag im Moodle-Kurs sowie die unterschriebene Bestätigung für die Praxisstelle vorliegt, muss eine Mail an das Sekretariat der allgemeinen Psychologie geschickt werden, um dann endlich mit dem Praktikum beginnen zu können. In der Mail sollten folgende Dokumente enthalten sein: &gt;&gt;&gt;&gt;&gt;&gt;&gt; db02cdb6286ee5509bd35c0b3e56aa84b159f17e\n\n\nScan deines Reisepasses\nScreenshot des „OK“ aus Moodle\nDie unterschriebene Bestätigung für die Praxisstelle\nDer ausgefüllte Antrag auf Genehmigung der Praxisstelle\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD 5. Nach Bestätigung durch das Sekretariat geht es auch schon los!\n\nInsgesamt sollte der gesamte Prozess, von Schritt 1 bis zum tatsächlichen Beginn des Praktikums, circa 2-4 Wochen dauern.\n\nNach Bestätigung durch das Sekretariat geht es auch schon los!\n\nInsgesamt sollte der gesamte Prozess, von Schritt 1 bis zum tatsächlichen Beginn des Praktikums, circa 2-4 Wochen dauern. &gt;&gt;&gt;&gt;&gt;&gt;&gt; db02cdb6286ee5509bd35c0b3e56aa84b159f17e\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Internship",
      "Pflichtpraktikum"
    ]
  },
  {
    "objectID": "labmeeting/index.html",
    "href": "labmeeting/index.html",
    "title": "Labmeeting",
    "section": "",
    "text": "We meet every week in the corner office, alternating between Labmeetings and “Lunch & Science”. You can see the schedule on the calendar.\nIn labmeetings, we discuss organisatory issues and everyone can bring up potential topics or problems to discuss. Additionally, we do presentations of our work, discuss papers,…\nFor “Lunch & Science”, everyone gathers in the corner office with something to eat and we watch a talk related to our research.",
    "crumbs": [
      "Home",
      "Labmeeting"
    ]
  },
  {
    "objectID": "labmeeting/index.html#fmri-methods",
    "href": "labmeeting/index.html#fmri-methods",
    "title": "Labmeeting",
    "section": "1.1 fMRI methods",
    "text": "1.1 fMRI methods\n\nMargulies et al., 2016: Hierarchy of areas based on the resting-state data: https://www.pnas.org/doi/10.1073/pnas.1608282113\nGlasser parcellation of the cortex, which everyone is using now: https://www.nature.com/articles/nature18933\nMultiple comparison correction methods for fMRI (no specific paper yet)\nHCP processing pipeline and CIFTI format (alternative to fmriprep and state-of-the-art for 7 T preprocessing): https://www.sciencedirect.com/science/article/pii/S1053811913005053?via%3Dihub\nResting-state connectivity of subcortical structures (including the claustrum) https://www.jneurosci.org/content/43/39/6609\nNeuroquery https://elifesciences.org/articles/53385\nNeurodesk https://www.nature.com/articles/s41592-023-02145-x\nWhite matter activations https://www.pnas.org/doi/10.1073/pnas.2219666120",
    "crumbs": [
      "Home",
      "Labmeeting"
    ]
  },
  {
    "objectID": "labmeeting/index.html#subjective-experience",
    "href": "labmeeting/index.html#subjective-experience",
    "title": "Labmeeting",
    "section": "1.2 Subjective experience",
    "text": "1.2 Subjective experience\n\nSelf et al., 2019: ephys signatures of figure-ground segmentation in macaque V1: https://www.sciencedirect.com/science/article/pii/S0960982219301551\nRecent findings from Tsao lab: https://www.pnas.org/doi/10.1073/pnas.2221122120\nDMN and awareness: https://www.nature.com/articles/s41467-022-34410-6\nPerceptography: https://www.biorxiv.org/content/10.1101/2022.10.24.513337v2\nPerceptography in humans using visual illusions: https://www.science.org/doi/full/10.1126/sciadv.adj3906",
    "crumbs": [
      "Home",
      "Labmeeting"
    ]
  },
  {
    "objectID": "labmeeting/index.html#general-methods",
    "href": "labmeeting/index.html#general-methods",
    "title": "Labmeeting",
    "section": "1.3 General methods",
    "text": "1.3 General methods\n\nStatistical power (small sample sizes): https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6267527/\nStatistical power: https://elifesciences.org/articles/62461\nExperimental methods: https://www.frontiersin.org/articles/10.3389/fnhum.2018.00421/full\nScientific writing: https://stevenpinker.com/files/pinker/files/descioli_pinker_2021_piled_modifiers.pdf\nRandomization techniques for statistical inference: permutation, bootstrapping, Monte Carlo simulations (no specific paper yet)",
    "crumbs": [
      "Home",
      "Labmeeting"
    ]
  },
  {
    "objectID": "labmeeting/index.html#ai",
    "href": "labmeeting/index.html#ai",
    "title": "Labmeeting",
    "section": "1.4 AI",
    "text": "1.4 AI\n\nhttps://www.sciencedirect.com/science/article/pii/S2589004222011853?via%3Dihub",
    "crumbs": [
      "Home",
      "Labmeeting"
    ]
  },
  {
    "objectID": "mri/3D-prints/3d-prints.html",
    "href": "mri/3D-prints/3d-prints.html",
    "title": "3D printing a brain",
    "section": "",
    "text": "There are many guidelines and tutorials on how to 3D print a brain. The currently recommended procedure is to use this docker image:\nhttps://github.com/nimaid/mri2stl\nThe advantage is that the brain is printed with the cerebellum, and the number of things you manually need to do is minimal.\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "MRI",
      "3D printing a brain"
    ]
  },
  {
    "objectID": "mri/analysis/bids.html",
    "href": "mri/analysis/bids.html",
    "title": "MRI BIDS and OSF",
    "section": "",
    "text": "Brain Imaging Data Structure (more information here).\n\n\nWith docker, you can run this line in the terminal:\n$ docker run -ti --rm -v [path_to_your_toplevel_bids_directory]:/data:ro bids/validator /data\nCommon issues:\n\nfiles don’t follow BIDS naming conventions (more info about filenames here)\nevents.tsv files are missing\nparticipants.tsv is missing\nparticipants.json file is missing or does not describe all variables from the tsv file\ndatasset_description is missing or there is no name of the dataset or author names in it.\nreadme file is missing or too short\n…",
    "crumbs": [
      "Home",
      "MRI",
      "MRI BIDS and OSF"
    ]
  },
  {
    "objectID": "mri/analysis/bids.html#validating-bids-dataset",
    "href": "mri/analysis/bids.html#validating-bids-dataset",
    "title": "MRI BIDS and OSF",
    "section": "",
    "text": "With docker, you can run this line in the terminal:\n$ docker run -ti --rm -v [path_to_your_toplevel_bids_directory]:/data:ro bids/validator /data\nCommon issues:\n\nfiles don’t follow BIDS naming conventions (more info about filenames here)\nevents.tsv files are missing\nparticipants.tsv is missing\nparticipants.json file is missing or does not describe all variables from the tsv file\ndatasset_description is missing or there is no name of the dataset or author names in it.\nreadme file is missing or too short\n…",
    "crumbs": [
      "Home",
      "MRI",
      "MRI BIDS and OSF"
    ]
  },
  {
    "objectID": "mri/analysis/freesurfer FSFAST.html#reasons-to-use-freesurfers-fsfast",
    "href": "mri/analysis/freesurfer FSFAST.html#reasons-to-use-freesurfers-fsfast",
    "title": "FreeSurfer FSFAST",
    "section": "Reasons to use FreeSurfers FSFAST",
    "text": "Reasons to use FreeSurfers FSFAST\n\nIf fitlins does not have a particular option for you that you would like to use i.e. different\nPotentially easier when working with surface space data (FSFAST was originally designed for surface space data)"
  },
  {
    "objectID": "mri/analysis/freesurfer FSFAST.html#neurodesk",
    "href": "mri/analysis/freesurfer FSFAST.html#neurodesk",
    "title": "FreeSurfer FSFAST",
    "section": "Neurodesk",
    "text": "Neurodesk\n\nWhat does a container mean\n\nNeurodesk has a whole host of software that is available for us to use\nNeurodesk works in which each piece of software has its own container\nSimply put for us this means that we use 1 piece of software that is contained inside its own environment\n\n\n\n\nExample of what containers mean\n\n\n\nSo for the example above if you were inside of the FreeSurfer container the only available software to you would be FreeSurfer\nMATLAB, fMRIprep and SPM would not be available to you\n\n \n\n\n\n\n\n\nNote: MATLAB runtime environment\n\n\n\nHowever, FreeSurfer uses something called a MATLAB runtime environment\n\nMATLAB runtime environment is smaller piece of software that is inside the FreeSurfer container\nFreeSurfer commands can be run without the whole of MATLAB directly and instead use this “MATLAB runtime environment”"
  },
  {
    "objectID": "mri/analysis/freesurfer FSFAST.html#freesurfer-commands-that-do-rely-on-the-matlab-runtime-environment",
    "href": "mri/analysis/freesurfer FSFAST.html#freesurfer-commands-that-do-rely-on-the-matlab-runtime-environment",
    "title": "FreeSurfer FSFAST",
    "section": "FreeSurfer commands that do rely on the MATLAB runtime environment",
    "text": "FreeSurfer commands that do rely on the MATLAB runtime environment\n\nSometimes when running FreeSurfer commands on neurodesk you might run into an error like so:\n\n\n\n\n\n\n\nWhat this means is the following:\n\nFreeSurfer by default is looking for a whole MATLAB installation\nFreeSurfer does not know whether there is a MATLAB runtime environment\n\nThis is because the MATLAB runtime environment is installed in a folder somewhere"
  },
  {
    "objectID": "mri/analysis/freesurfer FSFAST.html#steps-to-run-freesurfer-commands",
    "href": "mri/analysis/freesurfer FSFAST.html#steps-to-run-freesurfer-commands",
    "title": "FreeSurfer FSFAST",
    "section": "Steps to run FreeSurfer commands",
    "text": "Steps to run FreeSurfer commands\n1. Start a FreeSurfer container:\n\n\n\n\n\n \n2. Set up your environment variables:\n\nWhen using FreeSurfer it’s important to tell FreeSurfer where to find the subjects directory (SUBJECTS_DIR)\n\n\n(see FreeSurfers FSFAST tutorial for more information)\n\nFor example:\nexport SUBJECTS_DIR=$TUTORIAL_DATA/buckner_data/tutorial_subjs\n \n3. Change directory to where your data is:\nFor example:\ncd data\n \n4. Try and run the command first:\nFor example:\nFreeview ← this doesn’t rely on MATLAB so the freeview window should open\nrecon-all ← this doesn’t rely on MATLAB either so should work\nmri_vol2vol ← this doesn’t rely on MATLAB so should also work\nmkcontrast-sess ← this does rely on MATLAB runtime environment\n \n5. What to do if your command doesn’t work?\nIf you run a command for example:\nmkcontrast-sess -analysis auditory_v_baseline.sm0   -contrast a_vs_base -a 2 -c 1\nAnd then see an error like so:\n\n\n\n\n\n\nFigure 1\n\n\n\n\nThis means that FreeSurfer doesn’t know where the MATLAB runtime environment is therefore run the following:\n\nexport FS_MCRROOT=/opt/MCR2019b/v97/\nThe command above is important to tell FreeSurfer where the MATLAB runtime environment is\n \n6. Re-run the command again\n\nIf you fall into the error as above Figure 1 : then run the command using -mcr 1\n\nFor example:\nmkcontrast-sess -analysis auditory_v_baseline.sm0   -contrast a_vs_base -a 2 -c 1 -mcr 1"
  },
  {
    "objectID": "mri/mri-lab-setup-2023/index.html",
    "href": "mri/mri-lab-setup-2023/index.html",
    "title": "Old MRI setup",
    "section": "",
    "text": "Make sure to switch to the Linux computer (the button number 2 under the monitor)\nLogin with credentials you received\n\n\n\n\nimage\n\n\n\nOnce logged in:\nIf you plan to use MATLAB, you’re going to need internet connection: Einstellungen -&gt; Netzwerk -&gt; Kabel -&gt; Here you can choose between Internet or Eyelink (it should default to the internet)\nCheck if the trigger box is connected to the computer (and at the correct USB Port is used – bottom right):\n\n\n\n\nimage\n\n\n\nStart MATLAB / Octave and test the trigger box with the script response_box_check.m\n\n\n\n\nimage\n\n\n\n8 lights on the left are blinking whenever the 8 buttons on the response boxes are being pressed\nOn the right, the top light is constantly turned on if it is connected to the computer. On the bottom, the light is blinking whenever the scanner trigger is being sent.",
    "crumbs": [
      "Home",
      "MRI",
      "Old MRI setup"
    ]
  },
  {
    "objectID": "mri/mri-lab-setup-2023/index.html#general-setup-information",
    "href": "mri/mri-lab-setup-2023/index.html#general-setup-information",
    "title": "Old MRI setup",
    "section": "",
    "text": "Make sure to switch to the Linux computer (the button number 2 under the monitor)\nLogin with credentials you received\n\n\n\n\nimage\n\n\n\nOnce logged in:\nIf you plan to use MATLAB, you’re going to need internet connection: Einstellungen -&gt; Netzwerk -&gt; Kabel -&gt; Here you can choose between Internet or Eyelink (it should default to the internet)\nCheck if the trigger box is connected to the computer (and at the correct USB Port is used – bottom right):\n\n\n\n\nimage\n\n\n\nStart MATLAB / Octave and test the trigger box with the script response_box_check.m\n\n\n\n\nimage\n\n\n\n8 lights on the left are blinking whenever the 8 buttons on the response boxes are being pressed\nOn the right, the top light is constantly turned on if it is connected to the computer. On the bottom, the light is blinking whenever the scanner trigger is being sent.",
    "crumbs": [
      "Home",
      "MRI",
      "Old MRI setup"
    ]
  },
  {
    "objectID": "mri/mri-lab-setup-2023/index.html#experimental-protocol",
    "href": "mri/mri-lab-setup-2023/index.html#experimental-protocol",
    "title": "Old MRI setup",
    "section": "Experimental Protocol",
    "text": "Experimental Protocol\n\nParticipants can fill out the forms in the lobby. You can also instruct them there.\nPaperwork: …\nSnellen chart is available behind one of the changing rooms. There, an MRI-compatible googles are available as well, with lenses of different diopters in the suitcase. The goggles and lenses should be disinfected before and after each use!",
    "crumbs": [
      "Home",
      "MRI",
      "Old MRI setup"
    ]
  },
  {
    "objectID": "mri/MRI-Studies_general/mri-studies_general-information.html",
    "href": "mri/MRI-Studies_general/mri-studies_general-information.html",
    "title": "MRI-Studies: General Information",
    "section": "",
    "text": "This page is a constant work-in-progress file. The goal is to gather and provide necessary/useful/helpful information regarding (f)MRI studies. It can serve as a helpful guide when starting to plan an Experiment and while doing it. If you already acquired your data, you can refer to this page for information about the processing and analysis of your data.\nIf you realize something that is missing, please let us know, so we can add it here!",
    "crumbs": [
      "Home",
      "MRI",
      "MRI-Studies: General Information"
    ]
  },
  {
    "objectID": "mri/MRI-Studies_general/mri-studies_general-information.html#events.tsv-files",
    "href": "mri/MRI-Studies_general/mri-studies_general-information.html#events.tsv-files",
    "title": "MRI-Studies: General Information",
    "section": "1.1 events.tsv files",
    "text": "1.1 events.tsv files\n\nto save time later, keep in mind that you often need *_events.tsv files for your analysis! see here\nIn our fMRI Studies, the bare minimum generally is that these files should include is the Onset, duration and trial_type (e.g. the condition that was shown). This is later used to build your contrasts, e.g. in the first level GLM\n*_events.tsv files have to be named exactly like the functional run they are referring to (except the ending). E.g. for the functional scan sub-140001_ses-1_task-loc_run-01_bold.nii.gz, the corresponding file has to be named sub-140001_ses-1_task-loc_run-01_events.tsv\n\nExample of a rudimentary .tsv file:\nonset   duration    trial_type\n0.5     1.0         Condition A\n2.0     1.0         Condition B\n4.0     1.5         Condition A",
    "crumbs": [
      "Home",
      "MRI",
      "MRI-Studies: General Information"
    ]
  },
  {
    "objectID": "mri.html",
    "href": "mri.html",
    "title": "MRI",
    "section": "",
    "text": "3D printing a brain\n\n\n\nMRI\n\n\n3D-Print\n\n\n\ncurrent procedure for 3d printing brains\n\n\n\nNZ\n\n\nOct 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7T anatomical analysis on neurodesk\n\n\n\nMRI\n\n\n7T\n\n\nAnalysis\n\n\nNeurodesk\n\n\n\nHow to run 7 Tesla anatomy analysis on neurodesk\n\n\n\nAC\n\n\nJan 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMRI BIDS and OSF\n\n\n\nBIDS\n\n\nOSF\n\n\n\nInformation about BIDS and OSF\n\n\n\nAA\n\n\nMay 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfMRIprep analysis\n\n\n\nMRI\n\n\nAnalysis\n\n\nNeurodesk\n\n\nfmriPrep\n\n\n\nInformation about using fMRIprep to preprocess fMRI data\n\n\n\nAC\n\n\nJul 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFreeSurfer FSFAST\n\n\n\nMRI\n\n\nAnalysis\n\n\nNeurodesk\n\n\n\nInformation about running an FSFAST analysis on Neurodesk\n\n\n\nAC\n\n\nJan 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMNI space\n\n\n\nMRI\n\n\n\ninformation about various MNI spaces and templates\n\n\n\nNZ\n\n\nMar 7, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOld MRI setup\n\n\n\nMRI\n\n\n\nInformation about running experiments in the MRI lab (before the new computer arrived!)\n\n\n\nAA\n\n\nJan 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nUpdate to the MRI lab setup\n\n\n\nMRI\n\n\nNNL-Goggles\n\n\n\n\n\n\n\nAC\n\n\nOct 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMRI-Studies: General Information\n\n\n\nMRI\n\n\n\nUseful information to consider before and while doing a (f)MRI Study\n\n\n\nMG\n\n\nJul 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPhysiological noise correction\n\n\n\nMRI\n\n\nPhysiology\n\n\n\nCreating physiology regressors using the PhysIO toolbox\n\n\n\nMW\n\n\nOct 26, 2023\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top",
    "crumbs": [
      "Home",
      "MRI"
    ]
  },
  {
    "objectID": "neurodesk.html",
    "href": "neurodesk.html",
    "title": "Neurodesktop",
    "section": "",
    "text": "Neurodesktop access\n\n\n\nNeurodesk\n\n\n\nHow to use Neurodesktop on the server\n\n\n\nNZ, MG\n\n\nJul 28, 2025\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "OtherTopics/ContributeWithR.html",
    "href": "OtherTopics/ContributeWithR.html",
    "title": "Contribute to the website",
    "section": "",
    "text": "(tested on RStudio 2024.04.2 Build 764 for OSX)\nNote: You need to have git installed and you need to generate a personal authentication token in your github account to be able to push your changes to the remote repo.\nThis has an advantage that you can edit .qmd files in visual mode and paste screenshots without having to explicitly save and link images.\n\nGo to File -&gt; New Project\nSelect “Version control”\n\n\n\nPaste the git repo http path:\nhttps://github.com/visualneuroscience/visualneuroscience.github.io.git\nEdit the local version as you wish\nWhen finished editing, go to “Build” tab and press “Render Website”. This will render htmls and other aspects of the website structure, and open the current page in the browser.\n\nIf you are happy switch to the “Git” tab, select either all or some files, stage and commit. After this, press “Push” to send the new content to github repo\n\nEvery time you start editing the R project, first press “Pull” to make sure your local version contains all the most recent changes, which may have been made by others.\n\n\n\nThe difficulty on windows PC in the office has been the extreme slowness of the Git Gui window. The solution is to use the terminal (not the R console!) to commit changes and push:\ngit add *  \n\ngit commit -m \"yourmessage\"  \n\ngit push",
    "crumbs": [
      "Home",
      "Other Topics",
      "Contribute to the website"
    ]
  },
  {
    "objectID": "OtherTopics/ContributeWithR.html#note-on-windows",
    "href": "OtherTopics/ContributeWithR.html#note-on-windows",
    "title": "Contribute to the website",
    "section": "",
    "text": "The difficulty on windows PC in the office has been the extreme slowness of the Git Gui window. The solution is to use the terminal (not the R console!) to commit changes and push:\ngit add *  \n\ngit commit -m \"yourmessage\"  \n\ngit push",
    "crumbs": [
      "Home",
      "Other Topics",
      "Contribute to the website"
    ]
  },
  {
    "objectID": "OtherTopics/ContributeWithR.html#recent-changes-section",
    "href": "OtherTopics/ContributeWithR.html#recent-changes-section",
    "title": "Contribute to the website",
    "section": "2.1 Recent Changes Section",
    "text": "2.1 Recent Changes Section\nAt the starting page of the website, there is a “Recent Changes” section at the bottom. For making this section useful, it is important to add the date option in the meta-data of a page (format: MM-DD-YYYY)! Additionally, when you make meaningful and important changes to an existing page, you also need to change the data in the meta-data of the respective page! Otherwise, the change will not be shown at the “Recent Changes” section!",
    "crumbs": [
      "Home",
      "Other Topics",
      "Contribute to the website"
    ]
  },
  {
    "objectID": "OtherTopics/ContributeWithR.html#adding-new-content",
    "href": "OtherTopics/ContributeWithR.html#adding-new-content",
    "title": "Contribute to the website",
    "section": "2.2 Adding new content",
    "text": "2.2 Adding new content\nAfter getting access, you´ll see the directory structure of the website.\nAt the top of the hierarchy, you see the directories representing the “major topics” of the website (Labmeeting, People, Archive,…). If you want to add a page that is related to an existing topic, make sure to add it in the respective directory. If you´ll add something “new and unrelated to the rest”, you can add a new directory (+ corresponding .qmd file).",
    "crumbs": [
      "Home",
      "Other Topics",
      "Contribute to the website"
    ]
  },
  {
    "objectID": "OtherTopics/ContributeWithR.html#making-changes-visible",
    "href": "OtherTopics/ContributeWithR.html#making-changes-visible",
    "title": "Contribute to the website",
    "section": "2.3 “Making changes visible”",
    "text": "2.3 “Making changes visible”\nAfter adding new content, you might have to take an additional step to actually make it visible on the website! If you created a new directory, you also have to add it in the _quarto.yml file! Here you have the option to add it to the menu bar at the top and/or on the side.\nIf you added a new page within an existing directory, it should be visible automatically.",
    "crumbs": [
      "Home",
      "Other Topics",
      "Contribute to the website"
    ]
  },
  {
    "objectID": "OtherTopics/LabMembersOnly/LabMembersOnly.html",
    "href": "OtherTopics/LabMembersOnly/LabMembersOnly.html",
    "title": "Lab Members Only!!!",
    "section": "",
    "text": "https://cloud.uni-graz.at/index.php/f/220686861\n\n\n\nWe have activated server access via neurodesktop for students and those lab members who would like to use it. Please see the dedicated page.",
    "crumbs": [
      "Home",
      "Other Topics",
      "Lab Members Only!!!"
    ]
  },
  {
    "objectID": "OtherTopics/LabMembersOnly/LabMembersOnly.html#x2go",
    "href": "OtherTopics/LabMembersOnly/LabMembersOnly.html#x2go",
    "title": "Lab Members Only!!!",
    "section": "",
    "text": "https://cloud.uni-graz.at/index.php/f/220686861",
    "crumbs": [
      "Home",
      "Other Topics",
      "Lab Members Only!!!"
    ]
  },
  {
    "objectID": "OtherTopics/LabMembersOnly/LabMembersOnly.html#neurodesktop",
    "href": "OtherTopics/LabMembersOnly/LabMembersOnly.html#neurodesktop",
    "title": "Lab Members Only!!!",
    "section": "",
    "text": "We have activated server access via neurodesktop for students and those lab members who would like to use it. Please see the dedicated page.",
    "crumbs": [
      "Home",
      "Other Topics",
      "Lab Members Only!!!"
    ]
  },
  {
    "objectID": "people/coffee/index.html",
    "href": "people/coffee/index.html",
    "title": "Coffee time",
    "section": "",
    "text": "Coffee machine was acquired by the first generation of doctoral students #ana and #marilena . They kindly passed it on to future generations when leaving the lab.",
    "crumbs": [
      "Home",
      "People",
      "Coffee time"
    ]
  },
  {
    "objectID": "people/coffee/index.html#history",
    "href": "people/coffee/index.html#history",
    "title": "Coffee time",
    "section": "",
    "text": "Coffee machine was acquired by the first generation of doctoral students #ana and #marilena . They kindly passed it on to future generations when leaving the lab.",
    "crumbs": [
      "Home",
      "People",
      "Coffee time"
    ]
  },
  {
    "objectID": "people/coffee/index.html#general",
    "href": "people/coffee/index.html#general",
    "title": "Coffee time",
    "section": "General",
    "text": "General\nThere is only one thing to do: Empty the water container at the end of the day.",
    "crumbs": [
      "Home",
      "People",
      "Coffee time"
    ]
  },
  {
    "objectID": "people/coffee/index.html#decalcify",
    "href": "people/coffee/index.html#decalcify",
    "title": "Coffee time",
    "section": "Decalcify",
    "text": "Decalcify\nWhen the red light is blinking, it is time to decalcify. Do it right away, do not wait until the coffee machine is completely blocked by calcium to create a coffee flood in the office. Here are the steps:\n1. Turn on the coffee machine and wait until it is ready (if it was on before, turn it off and on, to get rid of coffee rests in the pipes)\n2. Empty the water tank, and add fill it with decalk fluid (located in the cupboard) until section “A” (alternatively, you can add the amount equivalent to one section of the botttle). Add normal water until section “B”. Insert the water tank back into the coffee machine.\n\n3. Insert the grey plastic bowl (located in the same cupboard) at a place where you usually put your cup. You may need something additional to support it, like a cookie box.\n\n\nPress the blinking red light for 5 seconds (until it stops blinking)\nTurn the small handle at the top left form 0 to I and wait for 10-20 minutes\nOnce the water indicator is blinking red, return the handle from I to 0\nEmpty the grey bowl and place it back\nFill the water container with water until section “MAX”\nTurn the small handle to I\nOnce the water indictor is blinking red, return the handle from I to 0\nRepeat from step 8 onwards\nAfter the second loop the red light that was originally blinking should extinguish\nEnjoy your coffee :-)\n\nP.S. Don’t forget to put a note that there is a decalcification in progress, to avoid any interference from your coffee-hungry colleagues",
    "crumbs": [
      "Home",
      "People",
      "Coffee time"
    ]
  },
  {
    "objectID": "people/funding/funding.html",
    "href": "people/funding/funding.html",
    "title": "project funding opportunities",
    "section": "",
    "text": "Overview\n\n\n\nList\n\nDOC (ÖAW)\n\n\nmax. 2 years after completion of diplom/Master\nduration: 24-36 months\n\n\nL’Oreal (ÖAW)\n\n\nfor women\nto finish your PhD or to write a Post-Doc proposal\nduration: 6-12 months\n\n\nAPART-MINT (ÖAW)\n\n\nmax. 3 years after completion of PhD\nduration: 12 months\n\n\ndoc.funds (FWF)\n\n\nwith 5 researchers\nduration: 4 years#\n\n\ndoc.funds.connect (FWF)\n\n\nwith 5 researchers from different institutions\nduration: 4 years\n\n\nESPRIT (FWF)\n\n\nmax. 5 years after completion of PhD\nduration: 36 months\n\n\nMax Kade (ÖAW)\n\n\nto visit the USA\nmax. 5 years after completion of PhD\nduration: 12 months\n\n\nErwin Schrödinger (FWF)\n\n\nearly stage post-docs\nduration: 36 months (10-24 months abroad)\n\n\nASTRA Awards (FWF)\n\n\n2-9 years after completion of PhD\nduration: 5 years\n\n\nScience Communication\n\n\nongoing FWF project of max. 3 years after completion of project\nduration: 24 months\n\n\nTop Citizen Science\n\n\nongoing FWF project\nonly by the project leader\nnot for salary\nduration: 24 months\n\n\n\nOther Links/Ressources\n\nUni Graz Research Management\nFWF calendar for workshops and webinars\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "People",
      "project funding opportunities"
    ]
  },
  {
    "objectID": "people/stipends/index.html",
    "href": "people/stipends/index.html",
    "title": "Stipends and prizes for master students",
    "section": "",
    "text": "Stipend of the Natural Science Faculty (we have mixed experiences with it)\nhttps://nawi.uni-graz.at/de/studieren/informationen-und-formulare-fuer-studierende/stipendien/leistungsstipendium/#c125989\n\n\n\nStipend of the Natural Science Faculty\nhttps://nawi.uni-graz.at/de/studieren/informationen-und-formulare-fuer-studierende/stipendien/foerderungsstipendium/\n\n\n\nNo experience so far\nhttps://universitaetsbund.uni-graz.at/de/gub-foerderungen/\n\n\n\nThis is suitable if you would like to go to another lab and learn a new method to potentially check out PhD options elswhere\nhttps://biotechmedgraz.at/de/programme/lab-rotation-program/\n\n\n\nThere is a possibility to apply to the Austrian Academy of Sciences, but it has to be done well in advance:\nhttps://stipendien.oeaw.ac.at/stipendien/doc\n\n\n\n\n\n\nIngeSt Prise in the category “Masterarbeit/Diplom”\nhttps://gehirnforschung.at/nachwuchsforderung/forschungspreis/\n\n\n\nAdvertised x1 per year via Email, look out for an email from Natalia\nExperience: We have been shortlisted several times, but without a final success\nhttps://gnpoe.at/",
    "crumbs": [
      "Home",
      "People",
      "Stipends and prizes for master students"
    ]
  },
  {
    "objectID": "people/stipends/index.html#before-or-during-your-masters-thesis",
    "href": "people/stipends/index.html#before-or-during-your-masters-thesis",
    "title": "Stipends and prizes for master students",
    "section": "",
    "text": "Stipend of the Natural Science Faculty (we have mixed experiences with it)\nhttps://nawi.uni-graz.at/de/studieren/informationen-und-formulare-fuer-studierende/stipendien/leistungsstipendium/#c125989\n\n\n\nStipend of the Natural Science Faculty\nhttps://nawi.uni-graz.at/de/studieren/informationen-und-formulare-fuer-studierende/stipendien/foerderungsstipendium/\n\n\n\nNo experience so far\nhttps://universitaetsbund.uni-graz.at/de/gub-foerderungen/\n\n\n\nThis is suitable if you would like to go to another lab and learn a new method to potentially check out PhD options elswhere\nhttps://biotechmedgraz.at/de/programme/lab-rotation-program/\n\n\n\nThere is a possibility to apply to the Austrian Academy of Sciences, but it has to be done well in advance:\nhttps://stipendien.oeaw.ac.at/stipendien/doc",
    "crumbs": [
      "Home",
      "People",
      "Stipends and prizes for master students"
    ]
  },
  {
    "objectID": "people/stipends/index.html#after-finishing-the-thesis",
    "href": "people/stipends/index.html#after-finishing-the-thesis",
    "title": "Stipends and prizes for master students",
    "section": "",
    "text": "IngeSt Prise in the category “Masterarbeit/Diplom”\nhttps://gehirnforschung.at/nachwuchsforderung/forschungspreis/\n\n\n\nAdvertised x1 per year via Email, look out for an email from Natalia\nExperience: We have been shortlisted several times, but without a final success\nhttps://gnpoe.at/",
    "crumbs": [
      "Home",
      "People",
      "Stipends and prizes for master students"
    ]
  },
  {
    "objectID": "people/stipends/index.html#josef-krainer-förderungspreis",
    "href": "people/stipends/index.html#josef-krainer-förderungspreis",
    "title": "Stipends and prizes for master students",
    "section": "Josef-Krainer-Förderungspreis",
    "text": "Josef-Krainer-Förderungspreis\ndeadline typically in September\nExperience: awarded to one former doctoral student\nhttp://www.steirisches-gedenkwerk.at/wissenschaftspreis/",
    "crumbs": [
      "Home",
      "People",
      "Stipends and prizes for master students"
    ]
  },
  {
    "objectID": "people/stipends/index.html#initiative-gehirnforschung-steiermark-1",
    "href": "people/stipends/index.html#initiative-gehirnforschung-steiermark-1",
    "title": "Stipends and prizes for master students",
    "section": "Initiative Gehirnforschung Steiermark",
    "text": "Initiative Gehirnforschung Steiermark\nIngeSt Prise in the category “Dissertation”\nhttps://gehirnforschung.at/nachwuchsforderung/forschungspreis/",
    "crumbs": [
      "Home",
      "People",
      "Stipends and prizes for master students"
    ]
  },
  {
    "objectID": "people/stipends/index.html#some-stipends-and-fellowships-are-listed-here",
    "href": "people/stipends/index.html#some-stipends-and-fellowships-are-listed-here",
    "title": "Stipends and prizes for master students",
    "section": "Some stipends and fellowships are listed here:",
    "text": "Some stipends and fellowships are listed here:\nhttps://stipendien.oeaw.ac.at/en/fellowships",
    "crumbs": [
      "Home",
      "People",
      "Stipends and prizes for master students"
    ]
  },
  {
    "objectID": "people/writing/writing.html",
    "href": "people/writing/writing.html",
    "title": "Scientific writing in practice",
    "section": "",
    "text": "Software for making artwork for figures: PowerPoint (simple), Inkscape (advanced functionality, especially relevant for article manuscripts)\nGraphs: R and python packages (especially seaborn)\nPasting graphs into the text\n\nas graphics (bitmap, png, jpeg, eps), not as an Microsoft graphical object\nwith “In line with text” option (to prevent figures/graphs from moving around when text is being edited)",
    "crumbs": [
      "Home",
      "People",
      "Scientific writing in practice"
    ]
  },
  {
    "objectID": "people/writing/writing.html#software",
    "href": "people/writing/writing.html#software",
    "title": "Scientific writing in practice",
    "section": "Software",
    "text": "Software\nMicrosoft word or similar. Please do not use latex or quarto pdf rendering. These are nice tools, but unfortunately they lack track-changes option which we need.",
    "crumbs": [
      "Home",
      "People",
      "Scientific writing in practice"
    ]
  },
  {
    "objectID": "people/writing/writing.html#introduction",
    "href": "people/writing/writing.html#introduction",
    "title": "Scientific writing in practice",
    "section": "Introduction",
    "text": "Introduction\nIntroduction is structured as an ice cream cone: &lt;3 It goes form more general to more specific. The cone should be “pointing” exactly at your research question. Typically the introduction covers the following topics (in this order):\n\nGeneral relevance of the topic\nPrevious research research on the topic and its findings\nGaps in knowledge, methodological problems or any reasons to study this topic further\nYour specific research question/approach",
    "crumbs": [
      "Home",
      "People",
      "Scientific writing in practice"
    ]
  },
  {
    "objectID": "people/writing/writing.html#discussion",
    "href": "people/writing/writing.html#discussion",
    "title": "Scientific writing in practice",
    "section": "Discussion",
    "text": "Discussion\nDiscussion has the following sections.\n\nIt starts with a single-paragraph brief summary of the main (not all!) results without interpretation.\nThe main part consists of the discussion of individual results, starting from more important and going to less important. For each result, one can compare the findings with the previous studies and speculate on the reasons if the results diverge. One can also speculate on the reasons for particularly interesting/unexpected results\nAn outlook for future studies is not always feasible as a separate section. The outlook on what future studies should study or improve can be provided separately for each aspect of the results that is being discussed.\nA conclusion at the end should again briefly state the results and main implications.",
    "crumbs": [
      "Home",
      "People",
      "Scientific writing in practice"
    ]
  },
  {
    "objectID": "people/writing/writing.html#general-notes-on-text",
    "href": "people/writing/writing.html#general-notes-on-text",
    "title": "Scientific writing in practice",
    "section": "General notes on text",
    "text": "General notes on text\n\nThroughout the text: use “working” headings for each paragraph that will be deleted later. They are useful for structuring the thoughts and the text. The heading should answer the question “what is this paragraph about?”\nText structure resembles a computer program\n\nThere are specific building blocks that repeat. Within the paragraph, it may be “first, second, third”. Across the paragraphs it may be “The first/second/third phase of the experiment consisted of…”.\nListings of different things should have identical and parallel grammatical structure.\n\n\n\n\n\n\n\nOriginal\nImproved\n\n\n\n\n“The location of the illusory surface was increased when the inducers formed an illusory shape, compared to when the inducers did not form an illusory shape.  The brain activity at the location of inducers partaking in the illusory shape perception was inhibited compared to the same inducers not forming the illusory shape. The background region was not modulated by the presence of the illusory shape”\n“Brain activity at the location of the illusory surface was increased when the inducers formed an illusory shape compared to when the inducers did not form an illusory shape.Brain activity at the location of the inducers forming an illusory shape was inhibited compared to when the inducers did not form an illusory shape. Finally, brain activity at the location of the background was not modulated by the presence of an illusory shape”.",
    "crumbs": [
      "Home",
      "People",
      "Scientific writing in practice"
    ]
  }
]